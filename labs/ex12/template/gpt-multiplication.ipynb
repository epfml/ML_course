{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Bk6znTnboD5e"
   },
   "source": [
    "# Training GPT to do Multiplication\n",
    "The goal of this exercise is for you to get more familiar with transformers and GPT. We will focus on a much simpler task than language modeling that can be trained in a few minutes. Specifically, we will train a small GPT model from scratch to perform multiplications. We will use individual characters as tokens. Doing multiplications directly for example in the form: \\\\\n",
    "\"12\\*34=\" -> \"408\" \\\\\n",
    "can be challenging. Even large language models are often not able to do this accurately for moderately large numbers, say 5 digits (although it is possible using [special tricks](https://arxiv.org/abs/2311.14737)).\n",
    "\n",
    "For large numbers predicting the first digit directly without any \"intermediate\" steps becomes hard. We will therefore consider training it do perform these intermediate steps explicitly, for example: \\\\\n",
    "\"12\\*34\" -> \"68\\[68\\]+340\\[408\\]=408\" \\\\\n",
    "where we compute the products of digits in the first number with the second number and then sum them up in the brackets, i.e. 68=2\\*34, 340=10\\*34, 0+68=68 and 68+340=408.\n",
    "\n",
    "This only requires a much simpler transformation at each step (corresponding to the generation of one output token) making it easier to learn and perform accurately. A similar idea is used in [\"chain-of-thought\" prompting](https://arxiv.org/abs/2201.11903) in large language models. They often perform much better if you ask them to \"think step by step\".\n",
    "\n",
    "One final thing we can do to make the task even easier is to write the numbers backwards during the intermediate steps. This is because comptuting the digits from left-to-right is significantly harder than computing them right-to-left for additions particulary but also multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "2fsQ-wq1xZFE"
   },
   "source": [
    "# Part 1: Data Creation\n",
    "This part contains the functions involved in the dataset creation. The pipline is as follows:\n",
    "* We generate random number pairs and split them into a train and validation set (generate_dataset).\n",
    "* For each pair of numbers we create a string showing the multiplication of the two numbers, potentially involving intermediate steps. For example (12,34) could get mapped to \"12\\*34=408\" or \"12\\*34=68\\[68\\]+340\\[408\\]=408\".\n",
    "* The strings are padded with spaces to all have a given length. Note that this is typically not done in standard next token prediction on language, but simplifies the training proceedure in this case.\n",
    "* The strings are tokenized, mapping them to arrays of indices (integers). In this case we use one token for each character so the main difference is that all the token indices are in an interval going from zero to the vocabulary size.\n",
    "\n",
    "You need to fill in missing details in two fuctions:\n",
    "* generate_mul_sequence\n",
    "* tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olCP4HQLkG_Y"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_mul_seq(a, b, max_digits=3, sum_cot=False, reverse_cot=False):\n",
    "    \"\"\"\n",
    "    This function takes in two integers and returns a string representing their\n",
    "    multiplication and result, optionally with intermediate steps.\n",
    "\n",
    "    >>> generate_mul_seq(867, 821, max_digits=3, sum_cot=False, reverse_cot=False)\n",
    "    '867*821=711807'\n",
    "    >>> generate_mul_seq(386, 273, max_digits=3, sum_cot=False, reverse_cot=True)\n",
    "    '386*273=873501=105378'\n",
    "    >>> generate_mul_seq(507, 779, max_digits=3, sum_cot=True, reverse_cot=False)\n",
    "    '507*779=5453[5453]+0[5453]+389500[394953]=394953'\n",
    "    >>> generate_mul_seq(807, 214, max_digits=3, sum_cot=True, reverse_cot=True)\n",
    "    '807*214=8941[8941]+0[8941]+002171[896271]=172698'\n",
    "    \"\"\"\n",
    "    prompt = f\"{a}*{b}=\"\n",
    "    prompt = \" \" * (2 * max_digits + 2 - len(prompt)) + prompt\n",
    "    if not sum_cot:\n",
    "        if not reverse_cot:\n",
    "            # No COT e.g. \"12*34=408\"\n",
    "            return prompt + f\"{a*b}\"\n",
    "        else:\n",
    "            # Reversed intermediate result e.g. \"12*34=804=408\"\n",
    "            return prompt + f\"{str(a*b)[::-1]}=\" + f\"{a*b}\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: Return a string of the type \"12*34=68[68]+340[408]=408\" if\n",
    "    # reverse_cot==False or 12*34=86[86]+043[804]=408 otherwise.\n",
    "    # You should use the prompt created above (which has a fixed length\n",
    "    # which is something we rely on later)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "token_table = {\n",
    "    **{f\"{d}\": d for d in range(10)},\n",
    "    \"*\": 10,\n",
    "    \"=\": 11,\n",
    "    \"+\": 12,\n",
    "    \"[\": 13,\n",
    "    \"]\": 14,\n",
    "    \" \": 15,  # Hacky padding\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(dataset):\n",
    "    \"\"\"\n",
    "    This function takes in a list of strings and converts each one to a uint8\n",
    "    numpy array of tokens corresponding to the characters in the string (see\n",
    "    the token_table above for the mapping).\n",
    "\n",
    "    >>> print(tokenize([\"867*821=711807\"]))\n",
    "    [array([8, 6, 7, 10, 8, 2, 1, 11, 7, 1, 1, 8, 0, 7], dtype=uint8)]\n",
    "    >>> print(tokenize([\"0\", \"123456789*=+[]\"]))\n",
    "    [array([0], dtype=uint8), array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], dtype=uint8)]\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: Map each string in the the dataset [list] to a numpy array of\n",
    "    # token indices (integers) according to the token_table defined above.\n",
    "    # Return a list of numpy arrays corresponding to the input strings.\n",
    "    # The datatype used for the numpy array should be np.uint8.\n",
    "    # For example: \"2*3=6\" should be mapped to np.array([2, 10, 3, 11, 6], dtype=np.uint8)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    return out\n",
    "\n",
    "\n",
    "# Other helper functions\n",
    "def generate_dataset(\n",
    "    train_samples=100000,\n",
    "    val_samples=1000,\n",
    "    max_digits=3,\n",
    "    sum_cot=False,\n",
    "    reverse_cot=False,\n",
    "):\n",
    "    assert train_samples + val_samples < 0.75 * 10 ** (\n",
    "        max_digits * 2\n",
    "    ), \"Too many requested data samples\"\n",
    "\n",
    "    def generate_pairs(n_samples, existing_pairs=None):\n",
    "        if existing_pairs is None:\n",
    "            existing_pairs = set()\n",
    "        max_num = 10**max_digits - 1\n",
    "        pairs = set()\n",
    "        while len(pairs) < n_samples:\n",
    "            num1 = random.randint(0, max_num)\n",
    "            num2 = random.randint(0, max_num)\n",
    "            pair = (num1, num2)\n",
    "            if pair not in existing_pairs:\n",
    "                pairs.add(pair)\n",
    "        return pairs\n",
    "\n",
    "    train_set = generate_pairs(train_samples)\n",
    "    val_set = generate_pairs(val_samples, train_set)\n",
    "\n",
    "    train_set = [\n",
    "        generate_mul_seq(a, b, max_digits, sum_cot, reverse_cot) for (a, b) in train_set\n",
    "    ]\n",
    "    val_set = [\n",
    "        generate_mul_seq(a, b, max_digits, sum_cot, reverse_cot) for (a, b) in val_set\n",
    "    ]\n",
    "\n",
    "    return list(train_set), list(val_set)\n",
    "\n",
    "\n",
    "def pad_datasets(train, val):\n",
    "    max_len = max(len(seq) for seq in train + val)\n",
    "    return [seq + \" \" * (max_len - len(seq)) for seq in train], [\n",
    "        seq + \" \" * (max_len - len(seq)) for seq in val\n",
    "    ]\n",
    "\n",
    "\n",
    "inverse_table = {val: key for (key, val) in token_table.items()}\n",
    "\n",
    "\n",
    "def detokenize(data):\n",
    "    out = [\"\".join([inverse_table[idx] for idx in seq]) for seq in data]\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test function we use to test your implementations\n",
    "import doctest\n",
    "import io\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(\n",
    "    threshold=np.inf, linewidth=np.inf, formatter={\"int\": lambda x: f\"{x:d}\"}\n",
    ")\n",
    "\n",
    "\n",
    "def test(f):\n",
    "    # The `globs` defines the variables, functions and packages allowed in the docstring.\n",
    "    tests = doctest.DocTestFinder().find(f)\n",
    "    assert len(tests) <= 1\n",
    "    for test in tests:\n",
    "        # We redirect stdout to a string, so we can tell if the tests worked out or not\n",
    "        orig_stdout = sys.stdout\n",
    "        sys.stdout = io.StringIO()\n",
    "\n",
    "        try:\n",
    "            results: doctest.TestResults = doctest.DocTestRunner().run(test)\n",
    "            output = sys.stdout.getvalue()\n",
    "        finally:\n",
    "            sys.stdout = orig_stdout\n",
    "\n",
    "        if results.failed > 0:\n",
    "            print(f\"❌ The are some issues with your implementation of `{f.__name__}`:\")\n",
    "            print(output, end=\"\")\n",
    "            print(\n",
    "                \"**********************************************************************\"\n",
    "            )\n",
    "        elif results.attempted > 0:\n",
    "            print(f\"✅ Your `{f.__name__}` passed {results.attempted} tests.\")\n",
    "        else:\n",
    "            print(f\"Could not find any tests for {f.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T6kUO7QmUzF",
    "outputId": "e88d8b5b-4b23-4509-8984-97ddd0a3510b"
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "random.seed(0)\n",
    "print(\"No COT:\")\n",
    "for seq in generate_dataset(3, 0, sum_cot=False, reverse_cot=False)[0]:\n",
    "    print(repr(seq))\n",
    "\n",
    "print(\"\\nReverse intermediate COT:\")\n",
    "for seq in generate_dataset(3, 0, sum_cot=False, reverse_cot=True)[0]:\n",
    "    print(repr(seq))\n",
    "\n",
    "print(\"\\nSum COT:\")\n",
    "for seq in generate_dataset(3, 0, sum_cot=True, reverse_cot=False)[0]:\n",
    "    print(repr(seq))\n",
    "\n",
    "print(\"\\nReverse sum COT:\")\n",
    "for seq in generate_dataset(3, 0, sum_cot=True, reverse_cot=True)[0]:\n",
    "    print(repr(seq))\n",
    "\n",
    "print()\n",
    "test(generate_mul_seq)\n",
    "test(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "W79lvFXnxqVh"
   },
   "source": [
    "# Part 2: Defining the model\n",
    "Below you will find a slightly simplified model definition from [NanoGPT](https://github.com/karpathy/nanoGPT), a lean codebase for training real GPT models. We won't ask you to implement anything but encourage you to read through the code. We do not expect you to understand everything or to be able to implement code like this. However, see if you can answer the following questions:\n",
    "* Where is Layer Normalization applied relative to the attention and MLP subblocks?\n",
    "* What activation function do we use and where?\n",
    "* How do we ensure that the model doesn't cheat during training by looking at future tokens?\n",
    "* Where do we convert the initial input tokens (integers) to vector embeddings?\n",
    "* How do we ensure that attention is aware of the sequence order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvcTNfJZb1ks"
   },
   "outputs": [],
   "source": [
    "# NanoGPT model.py\n",
    "# Slightly simplified to remove things unrelated to this exercise\n",
    "\n",
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
    "        if not self.flash:\n",
    "            print(\n",
    "                \"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\"\n",
    "            )\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                    1, 1, config.block_size, config.block_size\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q,\n",
    "                k,\n",
    "                v,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0,\n",
    "                is_causal=True,\n",
    "            )\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = (\n",
    "        50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    )\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = (\n",
    "        True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    )\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                drop=nn.Dropout(config.dropout),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=LayerNorm(config.n_embd, bias=config.bias),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = (\n",
    "            self.lm_head.weight\n",
    "        )  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.config.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
    "            )\n",
    "\n",
    "            mask = targets != -1\n",
    "            correct = (torch.argmax(logits, dim=-1) == targets) & mask\n",
    "            acc = torch.sum(1.0 * (correct)) / torch.sum(mask)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(\n",
    "                x[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "            acc = None\n",
    "\n",
    "        return logits, loss, acc\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(\n",
    "            f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "        )\n",
    "        print(\n",
    "            f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "        )\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, lr=learning_rate, betas=betas, **extra_args\n",
    "        )\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = (\n",
    "                idx\n",
    "                if idx.size(1) <= self.config.block_size\n",
    "                else idx[:, -self.config.block_size :]\n",
    "            )\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "c-yueXBcxtRR"
   },
   "source": [
    "# Part 3: Training Script\n",
    "Below you will find a simplified version of the training code used in [NanoGPT](https://github.com/karpathy/nanoGPT). We will also not ask you to implement anything here but encourage you to read through the code. This code uses some concepts you are probably not familiar with such as low precision training in float16 (for faster GPU execution). Do not worry about understanding everything but see if you can answer the following question:\n",
    "* Since the numbers in the prompt are random they can not be accurately predicted. Here we opt to ignore the prompt in our loss and accuracy computation. How do we do this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdCqb_lvf2K1"
   },
   "outputs": [],
   "source": [
    "# NanoGPT train.py (modified)\n",
    "# Slightly simplified to remove features not needed here\n",
    "\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Some configuration hyperparameters that we keep constant in this notebook\n",
    "\n",
    "# I/O\n",
    "eval_interval = 1000\n",
    "log_interval = 500\n",
    "eval_iters = 8\n",
    "\n",
    "# data\n",
    "batch_size = 128\n",
    "\n",
    "# model\n",
    "n_layer = 6\n",
    "n_head = 4\n",
    "n_embd = 128\n",
    "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3  # max learning rate\n",
    "max_iters = 5000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 1000  # how many steps to warm up for\n",
    "lr_decay_iters = max_iters  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 0  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = (\n",
    "    \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    ")\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = False  # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "def train_model(train_data, val_data, prompt_length, block_size):\n",
    "    # Usually we would take a lot more of the hyperparameters as some sort of\n",
    "    # arguments but here we only change the train_data and val_data\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "    torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "    device_type = (\n",
    "        \"cuda\" if \"cuda\" in device else \"cpu\"\n",
    "    )  # for later use in torch.autocast\n",
    "    # note: float16 data type will automatically use a GradScaler\n",
    "    ptdtype = {\n",
    "        \"float32\": torch.float32,\n",
    "        \"bfloat16\": torch.bfloat16,\n",
    "        \"float16\": torch.float16,\n",
    "    }[dtype]\n",
    "    ctx = (\n",
    "        nullcontext()\n",
    "        if device_type == \"cpu\"\n",
    "        else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "    )\n",
    "\n",
    "    # poor man's data loader\n",
    "    def get_batch(split, mask_first=prompt_length - 1):\n",
    "        data = train_data if split == \"train\" else val_data\n",
    "        ix = torch.randint(len(data), (batch_size,))\n",
    "        x = torch.stack([torch.from_numpy((data[i][:-1]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i][1:]).astype(np.int64)) for i in ix])\n",
    "        if mask_first:\n",
    "            y[:, :mask_first] = -1\n",
    "        if device_type == \"cuda\":\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(\n",
    "                device, non_blocking=True\n",
    "            )\n",
    "        else:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    print(f\"{block_size=}\")\n",
    "    tokens_per_iter = batch_size * block_size\n",
    "    print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "    # model init\n",
    "    model_args = dict(\n",
    "        n_layer=n_layer,\n",
    "        n_head=n_head,\n",
    "        n_embd=n_embd,\n",
    "        block_size=block_size,\n",
    "        bias=bias,\n",
    "        vocab_size=None,\n",
    "        dropout=dropout,\n",
    "    )  # start with model_args from command line\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    model_args[\"vocab_size\"] = 16\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    model.to(device)\n",
    "\n",
    "    # initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = model.configure_optimizers(\n",
    "        weight_decay, learning_rate, (beta1, beta2), device_type\n",
    "    )\n",
    "\n",
    "    # compile the model\n",
    "    if compile:\n",
    "        print(\"compiling the model... (takes a ~minute)\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)  # requires PyTorch 2.0\n",
    "\n",
    "    # helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss():\n",
    "        out_losses = {}\n",
    "        out_accs = {}\n",
    "        model.eval()\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            accs = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss, acc = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "                accs[k] = acc.item()\n",
    "            out_losses[split] = losses.mean()\n",
    "            out_accs[split] = accs.mean()\n",
    "        model.train()\n",
    "        return out_losses, out_accs\n",
    "\n",
    "    # training loop\n",
    "    iter_num = 0\n",
    "    best_val_loss = 1e9\n",
    "    X, Y = get_batch(\"train\")  # fetch the very first batch\n",
    "    t0 = time.time()\n",
    "\n",
    "    while True:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % eval_interval == 0:\n",
    "            losses, accs = estimate_loss()\n",
    "            print(\n",
    "                f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, train acc {accs['train']:0.4f}, val acc {accs['val']:0.4f}\"\n",
    "            )\n",
    "\n",
    "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "        with ctx:\n",
    "            logits, loss, accuracy = model(X, Y)\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch(\"train\")\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        # step the optimizer and scaler if training in fp16\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # flush the gradients as soon as we can, no need for this memory anymore\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # timing and logging\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0:\n",
    "            # get loss as float. note: this is a CPU-GPU sync point\n",
    "            lossf = loss.item()\n",
    "            print(\n",
    "                f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, {accuracy=:0.3f}\"\n",
    "            )\n",
    "        iter_num += 1\n",
    "\n",
    "        # termination conditions\n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "5o6OpMkFx2Jp"
   },
   "source": [
    "# Part 4: Evaluating models\n",
    "In this part you will train models on different types of data and evaluate how well they perform during inference.\n",
    "\n",
    "Fill out the missing details to train and compare the different models below. How do the different types of data change the difficulty of the task? Note that here we don't significantly tune the training much, it is probably possible to receive somewhat better performance on the direct task with longer training and better tuning (but we want to keep things short / managable here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgsvSd1sq7Ww"
   },
   "outputs": [],
   "source": [
    "# Helper functions for evaluation (no action needed)\n",
    "\n",
    "from IPython.display import HTML\n",
    "import difflib\n",
    "\n",
    "\n",
    "def compare_strings_html(pred_str, true_str):\n",
    "    # Initialize an empty string for the HTML output\n",
    "    diff_str = \"\"\n",
    "\n",
    "    # Iterate over the characters based on the length of the shorter string\n",
    "    for i in range(min(len(pred_str), len(true_str))):\n",
    "        if pred_str[i] != true_str[i]:\n",
    "            # If characters are different, color them red\n",
    "            diff_str += '<span style=\"color: #FF0000; text-decoration: line-through;\">{}</span>'.format(\n",
    "                pred_str[i]\n",
    "            )\n",
    "        else:\n",
    "            # If characters are the same, keep them as they are\n",
    "            diff_str += pred_str[i]\n",
    "\n",
    "    # Add the remaining characters of pred_str in red if they are extra\n",
    "    if len(pred_str) > len(true_str):\n",
    "        for extra_char in pred_str[len(true_str) :]:\n",
    "            diff_str += '<span style=\"color: #FF0000;\">{}</span>'.format(extra_char)\n",
    "\n",
    "    return diff_str\n",
    "\n",
    "\n",
    "def calculate_accuracies(Y_true, Y_pred, prompt_length):\n",
    "    total_strings = len(Y_true)\n",
    "    exact_match_count = 0\n",
    "    total_chars = 0\n",
    "    char_match_count = 0\n",
    "\n",
    "    for true_str, pred_str in zip(Y_true, Y_pred):\n",
    "        # Count exact matches\n",
    "        if true_str == pred_str:\n",
    "            exact_match_count += 1\n",
    "\n",
    "        # Count character matches, ignoring prompt_length\n",
    "        for t_char, p_char in zip(true_str[prompt_length:], pred_str[prompt_length:]):\n",
    "            if t_char != \" \":\n",
    "                total_chars += 1\n",
    "                if t_char == p_char:\n",
    "                    char_match_count += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    string_accuracy = exact_match_count / total_strings\n",
    "    char_accuracy = char_match_count / total_chars\n",
    "\n",
    "    return string_accuracy, char_accuracy\n",
    "\n",
    "\n",
    "def evaluate_model_generation(model, val_data, num_sequences=1024, batch_size=128):\n",
    "    # Generate sequences based on validation prompts and compare / compute accuracy\n",
    "    num_sequences = min(num_sequences, len(val_data))\n",
    "    model.eval()\n",
    "\n",
    "    true_seq = []\n",
    "    pred_seq = []\n",
    "\n",
    "    for batch_idx in range((num_sequences + batch_size - 1) // batch_size):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_sequences)\n",
    "        sequences = [(val_data[i]) for i in range(start_idx, end_idx)]\n",
    "\n",
    "        X_val = torch.stack(\n",
    "            [torch.from_numpy(seq.astype(np.int64)) for seq in sequences]\n",
    "        ).to(device)\n",
    "        X_prompt = X_val[:, :prompt_length]\n",
    "        Y_pred = model.generate(X_prompt, block_size - prompt_length + 1, top_k=1)\n",
    "\n",
    "        # Convert to strings\n",
    "        true_seq.extend(detokenize(X_val.cpu().numpy()))\n",
    "        pred_seq.extend(detokenize(Y_pred.cpu().numpy()))\n",
    "\n",
    "    # Compute accuracy\n",
    "    string_accuracy, char_accuracy = calculate_accuracies(\n",
    "        true_seq, pred_seq, prompt_length=prompt_length\n",
    "    )\n",
    "    print(\"Auto-regressive Generation\")\n",
    "    print(f\"Sequence-Level Accuracy: {string_accuracy:0.4f}\")\n",
    "    print(f\"Character-Level Accuracy: {char_accuracy:0.4f}\")\n",
    "\n",
    "    print(\"\\nExamples of correct/incorrect sequences:\")\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for tseq, pseq in zip(true_seq, pred_seq):\n",
    "        if tseq == pseq and correct < 5:\n",
    "            correct += 1\n",
    "            print(f\"Correct:   {tseq}\")\n",
    "        if tseq != pseq and incorrect < 5:\n",
    "            incorrect += 1\n",
    "            print(f\"Incorrect: {tseq}\")\n",
    "            display(\n",
    "                HTML(f'<div style=\"margin-left: 20px;\"><pre>Target: {tseq}</pre></div>')\n",
    "            )\n",
    "            display(\n",
    "                HTML(\n",
    "                    f'<div style=\"margin-left: 20px;\"><pre>Output: {compare_strings_html(pseq, tseq)}</pre></div>'\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XJ2Cng10vc8W",
    "outputId": "0c8fd33d-debd-4720-c4ae-cc1e7c3540ca"
   },
   "outputs": [],
   "source": [
    "for max_digits in [3, 6]:\n",
    "    for sum_cot in [False, True]:\n",
    "        for reverse_cot in [False, True]:\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"{max_digits=}, {sum_cot=}, {reverse_cot=}\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            random.seed(42)\n",
    "            prompt_length = 2 * max_digits + 2\n",
    "\n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: Generate the training and validation datasets\n",
    "            # TODO: Pad the sequences in the resulting datasets\n",
    "            # TODO: Tokanize the results to obtain the final train/val data\n",
    "            # ***************************************************\n",
    "            raise NotImplementedError\n",
    "\n",
    "            print(\"===== Training =====\")\n",
    "            block_size = len(train_data[0]) - 1\n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: Train a model, saving the resulting model\n",
    "            # ***************************************************\n",
    "            raise NotImplementedError\n",
    "\n",
    "            print(\"\\n\\n===== Evaluation =====\")\n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: Evaluate the model generation\n",
    "            # ***************************************************\n",
    "            raise NotImplementedError\n",
    "\n",
    "            print(\"=\" * 80)\n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "4aWdsrIpHl-W"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
