{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "zLuUPFZPTfQm"
   },
   "source": [
    "# [CS-433] Practical Session on Generative Adversarial Networks\n",
    "This is the practical session to the [CS-433 Machine Learning](https://edu.epfl.ch/coursebook/en/machine-learning-CS-433) Course at EPFL.\n",
    "\n",
    "You can find the accompanying slides in the course materials, week **12**.\n",
    "\n",
    "In this session, we will implement:\n",
    "- **2-D Swiss Roll GAN.** a two-dimensional GAN - trained to generate samples from a Swiss-Roll manifold;\n",
    "- Optional: **DCGAN on MNIST.** Implement a reduced model of [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) (to generate $1\\times 28 \\times 28$ images), and train it on the MNIST dataset.\n",
    "\n",
    "---\n",
    "We recommend running this notebook in [Colab](https://colab.research.google.com/). Training will be faster if using a GPU. \n",
    "To run this colab on GPU, change the runtime type: `Runtime > Change runtime type`, and select GPU from the dropdown. We can check which GPU is available by running the following command:\n",
    "\n",
    "On Colab, some cells contain 'hidden' code. Double-click those cells to see/close the source code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "pTpoJhMNFCba"
   },
   "source": [
    "**Note:** *if experimenting with hyper-parameters, ensure you re-intialize the networks, by re-running the cells they are defined in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "roXsLkRm4cPU"
   },
   "source": [
    "# Swiss-Roll toy experiment\n",
    "\n",
    "https://theloopywhisk.com/wp-content/uploads/2019/01/Chocolate-Swiss-Roll_730px_featured.jpg\n",
    "\n",
    "<center>\n",
    "<img src=\"https://theloopywhisk.com/wp-content/uploads/2019/01/Chocolate-Swiss-Roll_730px_featured.jpg\" width=\"380\" height=\"287\" alt=\"Swiss Roll\"><br>\n",
    "<i>\n",
    "Chocolate Swiss Roll CC BY Image by <a href=\"https://theloopywhisk.com/wp-content/uploads/2019/01/Chocolate-Swiss-Roll_730px_featured.jpg\">The Loopy Whisk</a></i>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup\n",
    "# Learning rate of the generator:\n",
    "g_lr = 1e-4  # @param {type:\"number\",  min:0.000001}\n",
    "# Learning rate of the discriminator:\n",
    "d_lr = 5e-4  # @param {type:\"number\",  min:0.000001}\n",
    "# batch size:\n",
    "batch_size = 64  # @param {type:\"integer\",  min:1}\n",
    "# dimension of tha latent vector\n",
    "noise_dim = 100  # @param {type:\"integer\",  min:1}\n",
    "# number of training iterations\n",
    "total_iterations = 20000  # @param {type:\"slider\", min:1, max:50000, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dataset\n",
    "import torch\n",
    "import sklearn.datasets  # we'll use it to create the Swiss Roll dataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def inf_data_gen(dataset=None, batch_size=None):\n",
    "    \"\"\"Python infinite iterator (called python-generator) of samples\n",
    "    following certain distribution.\n",
    "\n",
    "    Example Usage:\n",
    "    data_generator = inf_data_gen(dataset='swissroll', batch_size=64)\n",
    "    sample = next(data_generator)\n",
    "    \"\"\"\n",
    "    if dataset.lower() == \"swissroll\":\n",
    "        while True:\n",
    "            data = sklearn.datasets.make_swiss_roll(n_samples=batch_size, noise=0.25)[0]\n",
    "            data = data.astype(\"float32\")[:, [0, 2]]\n",
    "            data /= 7.5  # stdev plus a little\n",
    "            yield data  # torch.from_numpy(data)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only supported dataset is SwissRoll\")\n",
    "\n",
    "\n",
    "# - Test -\n",
    "data_generator = inf_data_gen(dataset=\"swissroll\", batch_size=300)\n",
    "sample = next(data_generator)\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(sample[:, 0], sample[:, 1], c=\"blue\", marker=\".\", alpha=0.5)\n",
    "plt.title(\"Real data samples (2D)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Loss & GAN architecture: define MLPs for th Generator & the Discriminator\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary cross entropy\n",
    "\n",
    "\n",
    "class GeneratorMLP(nn.Module):\n",
    "    def __init__(self, dim_hidden=512, dim_out=2, noise_dim=100):\n",
    "        super(GeneratorMLP, self).__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.noise_dim = noise_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(noise_dim, dim_hidden),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(dim_hidden, dim_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DiscriminatorMLP(nn.Module):\n",
    "    def __init__(self, dim_hidden=512, dim_gen_out=2):\n",
    "        super(DiscriminatorMLP, self).__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_gen_out = dim_gen_out\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim_gen_out, dim_hidden),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(dim_hidden, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "g_model = GeneratorMLP(noise_dim=noise_dim)\n",
    "d_model = DiscriminatorMLP()\n",
    "print(g_model)\n",
    "print(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title training functions\n",
    "def train_generator(gen, disc, loss=None, batch_size=64, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Updates the params of disc['model'] (once).\n",
    "\n",
    "    :param gen: dictionary with key 'model' [torch.nn.Sequential] and\n",
    "                'optim' [torch.optim]\n",
    "    :param disc: dictionary with key 'model' [torch.nn.Sequential]\n",
    "    :param loss: [torch.nn.<loss>]\n",
    "    :param batch_size: [int]\n",
    "    :param device: torch.device('cuda') or torch.device('cpu')\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    loss = loss or nn.BCELoss()  # Binary cross entropy\n",
    "    labels = torch.ones(batch_size, 1, device=device)\n",
    "    noise = torch.randn(batch_size, gen[\"model\"].noise_dim, device=device)\n",
    "\n",
    "    gen[\"model\"].zero_grad()\n",
    "    loss(disc[\"model\"](gen[\"model\"](noise)), labels).backward()\n",
    "    gen[\"optim\"].step()  # update params of the generator\n",
    "    return\n",
    "\n",
    "\n",
    "def train_discriminator(\n",
    "    gen,\n",
    "    disc,\n",
    "    data_sampler,\n",
    "    n_steps=1,\n",
    "    loss=None,\n",
    "    batch_size=64,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "    \"\"\"Updates the params of disc['model'] n_steps times.\n",
    "\n",
    "    :param gen: dictionary with key 'model' [torch.nn.Sequential]\n",
    "    :param disc: dictionary with key 'model' [torch.nn.Sequential] and\n",
    "                 'optim' [torch.optim]\n",
    "    :param data_sampler: [python generator (https://wiki.python.org/moin/Generators)]\n",
    "    :param n_steps: [int]\n",
    "    :param loss: [torch.nn.<loss>]\n",
    "    :param batch_size: [int]\n",
    "    :param device: torch.device('cuda') or torch.device('cpu')\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    real_labels = torch.ones(batch_size, 1, device=device)\n",
    "    fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "    loss = loss or nn.BCELoss()  # Binary cross entropy\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        disc[\"model\"].zero_grad()\n",
    "        #  1. Backprop - D on real: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        real_samples = torch.tensor(next(data_sampler), device=device)\n",
    "        loss(disc[\"model\"](real_samples), real_labels).backward()\n",
    "\n",
    "        #  2. Backprop - D on fake:\n",
    "        noise = torch.randn(batch_size, gen[\"model\"].noise_dim, device=device)\n",
    "        loss(disc[\"model\"](gen[\"model\"](noise)), fake_labels).backward()\n",
    "\n",
    "        #  3. Update the parameters  of the generator\n",
    "        disc[\"optim\"].step()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title train & store fake samples throughout training\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    g_model = g_model.cuda()\n",
    "    d_model = d_model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "g_optim = torch.optim.Adam(g_model.parameters(), lr=g_lr, betas=(0.5, 0.999))\n",
    "d_optim = torch.optim.Adam(d_model.parameters(), lr=d_lr, betas=(0.5, 0.999))\n",
    "\n",
    "generator = {\"model\": g_model, \"optim\": g_optim}\n",
    "discriminator = {\"model\": d_model, \"optim\": d_optim}\n",
    "\n",
    "fixed_noise = torch.randn(300, noise_dim, device=device)\n",
    "data_generator = inf_data_gen(dataset=\"swissroll\", batch_size=batch_size)\n",
    "\n",
    "plot_frequency = total_iterations // 100\n",
    "fake_samples = []\n",
    "for i in range(total_iterations):\n",
    "    train_discriminator(\n",
    "        generator,\n",
    "        discriminator,\n",
    "        data_sampler=data_generator,\n",
    "        loss=criterion,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    train_generator(\n",
    "        generator, discriminator, loss=criterion, batch_size=batch_size, device=device\n",
    "    )\n",
    "\n",
    "    if i % plot_frequency == 0 or (i + 1) == total_iterations:\n",
    "        fake_samples.append(generator[\"model\"](fixed_noise).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot fake samples: select the iteration (0 and 100 are at begining and end of training, resp.)\n",
    "iteration_plot_generator_samples = 39  # @param {type:\"slider\", min:0, max:100, step:1}\n",
    "\n",
    "fake_sample = fake_samples[iteration_plot_generator_samples]\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(fake_sample[:, 0], fake_sample[:, 1], c=\"red\", marker=\"*\", alpha=0.5)\n",
    "plt.title(\n",
    "    f\"Fake data samples at {iteration_plot_generator_samples*plot_frequency} iteration\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "FDtcmEU04vNB"
   },
   "source": [
    "# Optional: DCGAN on MNIST\n",
    "\n",
    "Examples of DCGAN generated images, trained on MNIST (the animation is over training steps):\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1UNCENHn736j6R1YUnRSbGfDGdfKAt7DJ\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title DCGAN architectures\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vision_utils\n",
    "import torchvision.datasets as _datasets\n",
    "import torchvision.transforms as _transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "_NOISE_DIM = 128\n",
    "_H_FILTERS = 64\n",
    "\n",
    "\n",
    "class DiscriminatorCNN28(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_channels=1,\n",
    "        h_filters=_H_FILTERS,\n",
    "        spectral_norm=False,\n",
    "        img_size=None,\n",
    "        n_outputs=1,\n",
    "    ):\n",
    "        \"\"\"Defines the architecture of the Discriminator.\n",
    "\n",
    "        :param img_channels: [int] e.g. 1 and 3 for greyscale and RGB, resp.\n",
    "        :param h_filters: [int] depth of the convolution kernels\n",
    "        :param spectral_norm: [bool] True activates Spectral Norm\n",
    "        :param img_size: [int]\n",
    "        :param n_outputs: [int]\n",
    "        \"\"\"\n",
    "        if any(\n",
    "            not isinstance(_arg, int) for _arg in [img_channels, h_filters, n_outputs]\n",
    "        ):\n",
    "            raise TypeError(\"Unsupported operand type. Expected integer.\")\n",
    "        if not isinstance(spectral_norm, bool):\n",
    "            raise TypeError(\n",
    "                f\"Unsupported operand type: {type(spectral_norm)}.\" \" Expected bool.\"\n",
    "            )\n",
    "        if min([img_channels, h_filters, n_outputs]) <= 0:\n",
    "            raise ValueError(\n",
    "                \"Expected nonzero positive input arguments for: the \"\n",
    "                \"number of output channels, the depth of the convolution \"\n",
    "                \"kernels, as well as .\"\n",
    "            )\n",
    "        super(DiscriminatorCNN28, self).__init__()\n",
    "        _apply_sn = lambda x: nn.utils.spectral_norm(x) if spectral_norm else x\n",
    "        self.img_channels = img_channels\n",
    "        self.img_size = img_size\n",
    "        self.n_outputs = n_outputs\n",
    "        self.main = nn.Sequential(\n",
    "            _apply_sn(nn.Conv2d(img_channels, h_filters, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters, h_filters * 2, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(h_filters * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters * 2, h_filters * 4, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(h_filters * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            _apply_sn(nn.Conv2d(h_filters * 4, self.n_outputs, 3, 1, 0, bias=False)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.img_channels is not None and self.img_size is not None:\n",
    "            if (\n",
    "                numpy.prod(list(x.size())) % (self.img_size**2 * self.img_channels)\n",
    "                != 0\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Size mismatch. Input size: {numpy.prod(list(x.size()))}. \"\n",
    "                    f\"Expected input divisible by: {self.noise_dim}\"\n",
    "                )\n",
    "            x = x.view(-1, self.img_channels, self.img_size, self.img_size)\n",
    "        x = self.main(x)\n",
    "        return x.view(-1, self.n_outputs)\n",
    "\n",
    "    def load(self, model):\n",
    "        self.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class GeneratorCNN28(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_channels=1, noise_dim=_NOISE_DIM, h_filters=_H_FILTERS, out_tanh=False\n",
    "    ):\n",
    "        \"\"\"Defines the architecture of the Generator.\n",
    "\n",
    "        :param img_channels: [int] e.g. 1 and 3 for greyscale and RGB, resp.\n",
    "        :param noise_dim: [int] the dimension of the noise vector\n",
    "        :param h_filters: [int] the depth of the convolution kernels\n",
    "        :param out_tanh: [bool] if True use Tanh as last non-linearity, Sigmoid otherwise\n",
    "        \"\"\"\n",
    "        if any(\n",
    "            not isinstance(_arg, int) for _arg in [img_channels, noise_dim, h_filters]\n",
    "        ):\n",
    "            raise TypeError(\"Unsupported operand type. Expected integer.\")\n",
    "        if min([img_channels, noise_dim, h_filters]) <= 0:\n",
    "            raise ValueError(\n",
    "                \"Expected strictly positive input arguments for the \"\n",
    "                \"number of output channels, the dimension of the noise \"\n",
    "                \"vector, as well as the depth of the convolution kernels.\"\n",
    "            )\n",
    "        super(GeneratorCNN28, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_dim, h_filters * 8, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 8, h_filters * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 4, h_filters * 2, 4, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(_H_FILTERS * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(h_filters * 2, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() if out_tanh else nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if numpy.prod(list(x.size())) % self.noise_dim != 0:\n",
    "            raise ValueError(\n",
    "                f\"Size mismatch. Input size: {numpy.prod(list(x.size()))}. \"\n",
    "                f\"Expected input divisible by: {self.noise_dim}\"\n",
    "            )\n",
    "        x = x.view(-1, self.noise_dim, 1, 1)\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "    def load(self, model):\n",
    "        self.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dataloader\n",
    "def load_mnist(data_dir=\"datasets\"):\n",
    "    \"\"\"Load the MNIST dataset, or download it if not found.\n",
    "\n",
    "    :param data_dir: [string]\n",
    "    :return: [torch.utils.data.Dataset]\n",
    "    \"\"\"\n",
    "    trans = _transforms.ToTensor()\n",
    "    _data = _datasets.MNIST(data_dir, train=True, download=True, transform=trans)\n",
    "    return _data\n",
    "\n",
    "\n",
    "def get_sampler(dataset, batch_size, shuffle=True, num_workers=1, drop_last=True):\n",
    "    \"\"\"A wrapper of PyTorch DataLoader class which allows to loop over the dataset\n",
    "    infinitely. Returns a function we will use to sample batches from our dataset.\n",
    "\n",
    "    :param dataset: [torch.utils.data.Dataset]\n",
    "    :param batch_size:  [int]\n",
    "    :param shuffle: [bool]\n",
    "    :param num_workers: [int]\n",
    "    :param drop_last: [bool]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "    dataloader_iterator = iter(data_loader)\n",
    "\n",
    "    def sampler():\n",
    "        nonlocal dataloader_iterator\n",
    "        try:\n",
    "            data = next(dataloader_iterator)  # if there is a next batch we use it\n",
    "        except StopIteration:\n",
    "            dataloader_iterator = iter(data_loader)  # else we loop again the dataset\n",
    "            data = next(dataloader_iterator)\n",
    "        return data\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training Functions\n",
    "\n",
    "\n",
    "def get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake):\n",
    "    \"\"\"Helper function, returns loss of Discriminator. The Discriminator classifies\n",
    "    samples as real or fake, hence uses binary cross entropy.\n",
    "\n",
    "    :param D: [torch.nn] discriminator\n",
    "    :param x_real: [torch.tensor] real data samples, size n_channels x im_size x im_size\n",
    "    :param x_gen: [torch.tensor] fake data samples, size n_channels x im_size x im_size\n",
    "    :param lbl_real: [torch.tensor] label for real samples\n",
    "    :param lbl_fake: [torch.tensor] label for fake samples\n",
    "    :return: total loss of the discriminator\n",
    "    \"\"\"\n",
    "    D_x = D(x_real)  # discriminator probabilities for the real samples\n",
    "    D_G_z = D(x_gen)  # discriminator probabilities for the fake samples\n",
    "    lossD_real = torch.binary_cross_entropy_with_logits(D_x, lbl_real).mean()\n",
    "    lossD_fake = torch.binary_cross_entropy_with_logits(D_G_z, lbl_fake).mean()\n",
    "    lossD = lossD_real + lossD_fake\n",
    "    return lossD\n",
    "\n",
    "\n",
    "def get_generator_loss(G, D, z, lbl_real):\n",
    "    \"\"\"Helper function, returns loss of Generator. The Generator aims at making\n",
    "    the discriminator output the real label, given fake images.\n",
    "\n",
    "    :param G: [torch.nn] generator\n",
    "    :param D: [torch.nn] discriminator\n",
    "    :param z: [torch.tensor] latent/noise vector\n",
    "    :param lbl_real: [torch.tensor] label for real samples\n",
    "    :return: loss of the generator\n",
    "    \"\"\"\n",
    "    D_G_z = D(\n",
    "        G(z)\n",
    "    )  # Get the probability of the fake images (created from G with the noise z)\n",
    "    lossG = torch.binary_cross_entropy_with_logits(\n",
    "        D_G_z, lbl_real\n",
    "    ).mean()  # Fake samples classified as real\n",
    "    return lossG\n",
    "\n",
    "\n",
    "def train(\n",
    "    G,\n",
    "    D,\n",
    "    dataset,\n",
    "    iterations,\n",
    "    batch_size=32,\n",
    "    lrD=0.01,\n",
    "    lrG=0.01,\n",
    "    beta1=0.99,\n",
    "    plot_every=100,\n",
    "    n_workers=5,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "    \"\"\"Training function.\n",
    "\n",
    "    :param G: [torch.nn] generator\n",
    "    :param D: [torch.nn] discriminator\n",
    "    :param dataset: [torch.utils.data.Dataset]\n",
    "    :param iterations: [int] number of training steps\n",
    "    :param batch_size:  [int]\n",
    "    :param lrD: [float] learning rate for the discriminator\n",
    "    :param lrG: [float] learning rate for the generator\n",
    "    :param beta1: [float] hyperparameter for the Adam optimizer\n",
    "    :param plot_every: [int] plotting frequency w.r.t. iterations\n",
    "    :param n_workers: [int] used for the dataloader to load batches in parallel\n",
    "    :param device: torch.device('cpu') or torch.device('cuda')\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    sampler = get_sampler(\n",
    "        dataset, batch_size, shuffle=True, num_workers=n_workers, drop_last=True\n",
    "    )  # the sampler is used to iterate over the dataset of real images\n",
    "\n",
    "    # Optimizers\n",
    "    optimizerD = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, 0.999))\n",
    "    optimizerG = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
    "\n",
    "    # LBLs\n",
    "    lbl_real = torch.ones(\n",
    "        batch_size, 1, device=device\n",
    "    )  # we define 1 as being the output of D when he thinks the image is real\n",
    "    lbl_fake = torch.zeros(\n",
    "        batch_size, 1, device=device\n",
    "    )  # we define 0 as being the output of D when he thinks the image is fake\n",
    "\n",
    "    fixed_noise = torch.randn(\n",
    "        100, G.noise_dim, device=device\n",
    "    )  # we use this noise to generate the images to print\n",
    "\n",
    "    G.to(device)  # move the generator to the proper device (e.g. GPU)\n",
    "    D.to(device)  # move the discriminator to the proper device (e.g. GPU)\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # STEP 1: D optimization step using G\n",
    "        x_real, _ = sampler()  # we get some real images\n",
    "        x_real = x_real.to(device)  # send them to the proper device (e.g. GPU)\n",
    "        z = torch.randn(batch_size, G.noise_dim, device=device)  # we sample some noise\n",
    "        with torch.no_grad():\n",
    "            x_gen = G(z)  # We generate random images from the noise\n",
    "        optimizerD.zero_grad()  # Always zero your gradient before doing backward !\n",
    "        lossD = get_disciminator_loss(D, x_real, x_gen, lbl_real, lbl_fake)\n",
    "        lossD.backward()  # compute gradient for the discriminator\n",
    "        optimizerD.step()\n",
    "\n",
    "        # STEP 4: G optimization step using D\n",
    "        z = torch.randn(\n",
    "            batch_size, G.noise_dim, device=device\n",
    "        )  # we get some random noise\n",
    "        optimizerG.zero_grad()  # Always zero your gradient before doing backward !\n",
    "        lossG = get_generator_loss(G, D, z, lbl_real)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()  # Important remark here: we backpropagated to both the discriminator and the\n",
    "        # generator with `lossG.backward()`, but optimizerG is only updating the weights\n",
    "        # of the generator. Updating the weights of the discriminator would be cheating.\n",
    "\n",
    "        # plotting things\n",
    "        if i % plot_every == 0 or i == iterations:\n",
    "            with torch.no_grad():\n",
    "                probas = torch.sigmoid(D(G(fixed_noise)))\n",
    "                mean_proba = probas.mean().cpu().item()\n",
    "                std_proba = probas.std().cpu().item()\n",
    "                samples = G(fixed_noise).cpu()\n",
    "                fig = plt.figure(figsize=(12, 5), dpi=100)\n",
    "                samples = samples.view(100, 1, 28, 28)\n",
    "                grid_img = vision_utils.make_grid(\n",
    "                    samples, nrow=10, normalize=True, padding=0\n",
    "                )\n",
    "                plt.imshow(grid_img.permute(1, 2, 0), interpolation=\"nearest\")\n",
    "                print(\n",
    "                    f\"Iter {i}: Mean proba from D(G(z)): {mean_proba:.4f} +/- {std_proba:.4f}\"\n",
    "                )\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train DCGAN\n",
    "\n",
    "# Learning rate of the generator:\n",
    "g_lr = 1e-3  # @param {type:\"number\",  min:0.000001}\n",
    "# Learning rate of the discriminator:\n",
    "d_lr = 1e-3  # @param {type:\"number\",  min:0.000001}\n",
    "# batch size:\n",
    "batch_size = 50  # @param {type:\"integer\",  min:1}\n",
    "# number of training iterations\n",
    "total_iterations = 50000  # @param {type:\"slider\", min:1, max:150000, step:1}\n",
    "\n",
    "\n",
    "dataset = load_mnist(data_dir=\"datasets\")  # load the MNIST dataset\n",
    "\n",
    "G = GeneratorCNN28(noise_dim=_NOISE_DIM, out_tanh=True)\n",
    "D = DiscriminatorCNN28(spectral_norm=False, img_size=28)\n",
    "\n",
    "train(\n",
    "    G,\n",
    "    D,\n",
    "    dataset,\n",
    "    iterations=total_iterations,\n",
    "    batch_size=batch_size,  # Hyper-parameter: batch size\n",
    "    lrD=d_lr,  # Hyper-parameter: learning rate of the discriminator\n",
    "    lrG=g_lr,  # Hyper-parameter: learning rate of the generator\n",
    "    beta1=0.05,  # Hyper-parameter: for Adam (smaller values empirically shown better)\n",
    "    plot_every=1000,\n",
    "    n_workers=5,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
