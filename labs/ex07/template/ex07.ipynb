{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa20d9-c702-4407-9cb7-63b33da08915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2d3051b0-426f-4c90-81d2-32098bbeec04",
   "metadata": {
    "id": "2d3051b0-426f-4c90-81d2-32098bbeec04"
   },
   "source": [
    "# Introduction to Neural Networks and PyTorch\n",
    "This notebook contains a brief introduction to PyTorch and three exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f47d58eb-3394-4a19-8669-6f569a266a4c",
   "metadata": {
    "id": "f47d58eb-3394-4a19-8669-6f569a266a4c",
    "tags": []
   },
   "source": [
    "# PyTorch Basics\n",
    "[PyTorch](https://pytorch.org/) is a deep learning framework. PyTorch works with tensors which are multidimensional arrays similar to the ones used in NumPy. Since you have already used NumPy in previous exercises we will introduce PyTorch in terms of the main differences from NumPy:\n",
    "* PyTorch can run operations on both CPU and hardware accelerators like GPUs. GPUs are particularly well suited for doing matrix multiplications which make up most of the computation in neural networks. Using GPUs can speed training up drastically, often on the order of ~100x for decently sized neural networks involving large matrix-matrix multiplications.\n",
    "* PyTorch has automatic differentiation that allows it to compute the derivatives of most functions. You don't need to derive or implement the formulas for the gradients. This is especially useful for training neural networks through gradient descent.\n",
    "* PyTorch provides implementations of various neural network components, dataloading utilities, and optimization tools, making it much easier to specify and train neural networks.\n",
    "\n",
    "Make sure you have PyTorch installed ([see instructions](https://pytorch.org/get-started/locally/)) or run this notebook on Google colab (recommended if you don't have a GPU). You can import PyTorch as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adde2e-65b2-4db2-8529-7ae426d41691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4e67d143-68a2-4ec6-89bc-0e57dc1c470f",
   "metadata": {
    "id": "4e67d143-68a2-4ec6-89bc-0e57dc1c470f",
    "tags": []
   },
   "source": [
    "## PyTorch Tensors\n",
    "Tensors (`torch.Tensor`) are similar to the multidimensional arrays used in NumPy (`numpy.ndarray`). The functions to create them often have similar names and arguments. We will use the function `torch.ones()` to highlight some of the differences between NumPy and PyTorch. The signatures for the two functions are:\n",
    "```python\n",
    "torch.ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "numpy.ones(shape, dtype=None, order='C', *, like=None)\n",
    "```\n",
    "* There are often subtle differences in the syntax. For example, the size argument of `torch.ones` can either accept a tuple or a variable number of integers but  NumPy only supports a tuple/list. Another common one is that PyTorch referes to `axis` as `dim` in operations such as `sum` and `mean`.\n",
    "* Each tensor has a device associated with it and resides in the memory of that device. This is specified via the device argument used in all tensor creation ops. By default, this is the CPU (`device='cpu'`), but can also be the GPU (`device='cuda'`) or e.g. `device='cuda:0'` if you have many GPUs. All tensors involved in a given operation e.g. addition or matrix multiplication must recide on the same device. An existing tensor `T` can be moved to other devices in through commands such as `T.cpu()`, `T.cuda()`, `T.to(device='cuda')`. Each of these will return a new copy of the tensor on the target device.\n",
    "* The default floating point precision in PyTorch is `torch.float32` but in NumPy this varies more and is often `numpy.float64`. If you are unfamiliar with floating point numbers, you can think of them as being equivalent to the scientific notation e.g. $\\pm 1.23456 \\cdot 10^{E}$, but in binary. Each format supports a specific range for the exponent $E$ and has a given number of significant digits. Float32 operations typically run significantly faster than float64 operations and require less memory, but are not as precise. For neural networks you should not use float64 unless you have a really good reason for it. In fact many operations in neural networks can be performed in even lower precision such as `torch.bfloat16` or `torch.float16`. These formats require less memory and run faster on hardware that supports it, but may require special tricks for training.\n",
    "* Each tensor T in PyTorch can either have `T.requires_grad==True` or `T.requires_grad==False`. When requires_grad is True, PyTorch will keep track of operations involving this tensor in a structure referred to as a computation graph. PyTorch can backpropagate through a computation graph to compute gradients for the tensors involved."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "33b8dc94-bd35-4976-8b69-345525366e19",
   "metadata": {
    "id": "33b8dc94-bd35-4976-8b69-345525366e19"
   },
   "source": [
    "## PyTorch Autograd\n",
    "The automatic differentiation in PyTorch (autograd) records operations on tensors that have `requires_grad==True`. The following example demonstrates autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8cfb9-cfae-41e3-b578-417286b92d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones(4, requires_grad=True)  # A has shape (4,)\n",
    "# Creates a tensor with the same shape, dtype and device as A with all values equal to 5\n",
    "B = torch.full_like(A, 5)\n",
    "C = A * B\n",
    "\n",
    "# Note that C has a grad_fn, PyTorch remembers how this tensor was created and\n",
    "# can differentiate through the op\n",
    "print(C)\n",
    "\n",
    "# True because it is derived from A which requires_grad\n",
    "print(C.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b856c-b107-431f-b11f-5a610ab77c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_loss = torch.sum(C)  # Create a scalar that we can compute derivatives for\n",
    "scalar_loss.backward()  # Compute the gradients of all tensors with respect to scalar_loss\n",
    "\n",
    "# 5 for all elements as expected\n",
    "print(A.grad)\n",
    "# None because we did not explicitly set requires_grad to True (not set by default using full_like)\n",
    "print(B.grad)\n",
    "# None because C is an intermediate tensor in the computation graph and not a leaf tensor\n",
    "print(C.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00082abe-f9d7-4001-9ecf-1c6683d93f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(2 * A).backward()\n",
    "print(A.grad)  # Note that the gradients add up if we have multiple backward calls."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a97bd6c0-3a77-42ea-837f-bf65a70adc88",
   "metadata": {
    "id": "a97bd6c0-3a77-42ea-837f-bf65a70adc88"
   },
   "source": [
    "Since gradients add upp with multiple backwards calls we have to remember to set the gradients to zero between iterations of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d549ab2f-6268-40d4-95b7-3e91eb29174e",
   "metadata": {
    "id": "d549ab2f-6268-40d4-95b7-3e91eb29174e"
   },
   "source": [
    "The gradient computation graph for tensors generally stores intermediate values required to compute the derivatives. By default calling `.backward()` on a tensor frees up these resources unless we specify `retain_graph=True` in the `backward` call. In many cases we have tensors that require gradients, such as model parameters, but we don't want to compute gradients e.g. when we are evaluating the model. In this case we can use `torch.no_grad()` which prevents torch from storing the computation graph and intermediate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7e286-3adb-46c6-9151-c713165ab66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    D = 2 * A\n",
    "\n",
    "print(D.requires_grad)  # False since D was computed inside torch.no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "15e1a7c1-c2b8-46de-b87a-c203c991f90a",
   "metadata": {
    "id": "15e1a7c1-c2b8-46de-b87a-c203c991f90a"
   },
   "source": [
    "Generally intermediate variables stored in computation graphs are also freed if subsequent tensors have been deleted (either explicitly or gone out of scope). Not using torch.no_grad, especially in loops where some tensors in the graph are not deleted (e.g. stored in a list), can result in a type of memory leak. Tensors can be detached from the computation graph through `T.detach()` which will return a reference to the tensor without the grad function or the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "76387b32-dfa6-498c-9b28-8549f1f2dc01",
   "metadata": {
    "id": "76387b32-dfa6-498c-9b28-8549f1f2dc01"
   },
   "source": [
    "## Resources\n",
    "This wraps up our quick intro to PyTorch. There are many good resources available to learn PyTorch:\n",
    "* [Deep Learning with PyTorch: a 60-minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "We recommend going through them later, especially if you will use PyTorch in your own projects or work. We will try to introduce concepts needed in the exercises as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fc366907-67a4-4401-8bc1-0b0eb02401fb",
   "metadata": {
    "id": "fc366907-67a4-4401-8bc1-0b0eb02401fb"
   },
   "source": [
    "# Exercise 1\n",
    "In this exercise you will define and train a fully connected neural network. As we saw in lecture, neural networks are general function approximators. Here we will train a neural network to approximate simple functions and visualize the results.\n",
    "\n",
    "In this exercise we mostly use basic PyTorch operations instead of higher level packages such as `torch.nn`. We do this as a learning opportunity to go deeper into the workings of the neural network and PyTorch. In general we recommend the use of higher level implementations when possible and we will do so from exercise 2 onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1505c7f0-cf76-4c15-a4b5-5fb427ec0399",
   "metadata": {
    "id": "1505c7f0-cf76-4c15-a4b5-5fb427ec0399"
   },
   "source": [
    "### Exercise 1.1 - Data Creation\n",
    "We will be approximating the following 2d function:\n",
    "$$ f_{true}(x_1, x_2) = \\cos\\left(10 \\sqrt{x_1^2+x_2^2}\\right) \\cdot \\exp{\\left(-4 (x_1^2+x_2^2)\\right)} $$\n",
    "over the rectangle $(x_1, x_2) \\in [-1:1]\\times[-1:1]$\n",
    "\n",
    "We provide a function below that samples a grid of points and returns an $N \\times 2$ tensor $X$. Fill in the other function that computes $Y=f_{true}(X)$ according to the formula above.\n",
    "\n",
    "Note that PyTorch has common math functions such as torch.sum, torch.cos etc. See more [here](https://pytorch.org/docs/stable/torch.html#math-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259144ed-7b31-4baa-b769-4009f79d09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_grid(N_sqrt=100):\n",
    "    \"\"\"\n",
    "    Returns an Nx2 tensor of grid points in [-1:1]x[-1:1].\n",
    "    N=N_sqrt*N_sqrt.\n",
    "    \"\"\"\n",
    "    x1 = torch.linspace(-1, 1, N_sqrt)\n",
    "    x1_grid, x2_grid = torch.meshgrid(x1, x1, indexing=\"xy\")\n",
    "    X = torch.stack((x1_grid, x2_grid), -1).view(-1, 2)\n",
    "    return X\n",
    "\n",
    "\n",
    "def compute_f_true(X):\n",
    "    \"\"\"\n",
    "    Returns Y=f_true(X)\n",
    "    X is an Nx2 vector, Y is an Nx1 vector\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "5cc54de9-1a36-4452-9d13-c5b70956c709",
   "metadata": {
    "id": "5cc54de9-1a36-4452-9d13-c5b70956c709"
   },
   "source": [
    "### Exercise 1.2 - Model Definition\n",
    "In this part we will create a simple model function for a ReLU fully connected network. We break the implementation down into several functions.\n",
    "\n",
    "We assume the model has $L$ layers, an input dimension of 2, and output dimension of 1.\n",
    "The inputs to the model are $X_0$ an $N \\times 2$ matrix.\n",
    "We use $K_{i}$ to denote the number of units in the $i$-th layer (i.e. their width or dimension, $i=1$ is the first layer). We assume the width of all the hidden layers (every layer except the last one) is constant, $K_i=K$. We set $K_0=2$ and $K_{L}=1$ corresponding to the number of input features and the dimension of the output.\n",
    "\n",
    "Layer $i$ computes the following:\n",
    "$$X_{i} = \\phi(X_{i-1} W_{i} + b_{i})$$\n",
    "Where $\\phi=ReLU$ for all layers except the last one where it is identity, $W_{i}$ is a $K_{i-1} \\times K_{i}$ matrix and $b_{i}$ is a bias vector with $K_{i}$ elements.\n",
    "\n",
    "The first function `get_model_weights(L, K)` will create the weight and bias tensors. It takes in $L$ and $K$ and returns a list of $L$ matrices and a separate list of $L$ biases corresponding the layers in order.\n",
    "* The biases should be set to zero, have dtype=torch.float32 and have requires_grad=True.\n",
    "* The matrices should be initialized with an element-wise normal distribution with mean zero and standard deviation $\\sqrt{2/K_{i-1}}$ where $K_{i-1}$ is the width of the input to the layer. This initialization strategy is called either Kaiming He initialization and aims to preserve the variance of activations on the forward pass accounting for a ReLU non-linearity. The weight matrices should also have dtype=torch.float32 and requires_grad=True.\n",
    "* Note that you might need to create the weights in a couple of steps. Make sure that the weights returned have requires_grad=True and are not intermediate tensors in a computation graph, i.e. they are not derived from other tensors with requires_grad=True. PyTorch has [various functions](https://pytorch.org/docs/stable/torch.html#random-sampling) to sample random tensors.\n",
    "\n",
    "The second function `predict(X, weights, biases)` takes in the inputs $X$ (i.e. $X_0$) as well as the list of weights and biases created by `get_model_weights`. It computes the output of the neural network according to the formula above. Here [torch.clamp](https://pytorch.org/docs/stable/generated/torch.clamp.html), [torch.mm](https://pytorch.org/docs/stable/generated/torch.mm.html) and/or the `@` operator could be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dab809-f3be-464a-9eb7-731d73261e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_weights(L, K, device):\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: Create the weights, biases and add them to the lists\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def predict(X, weights, biases):\n",
    "    assert len(weights) == len(biases)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: Compute a forward pass through the network\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "5b741b62-817e-4718-be0e-d6535c739a69",
   "metadata": {
    "id": "5b741b62-817e-4718-be0e-d6535c739a69"
   },
   "source": [
    "### Exercise 1.3 - Training Loop\n",
    "In this part we write a function that trains the model for a given number of steps.\n",
    "We will use full-batch gradient descent with the [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) (using the PyTorch implementation).\n",
    "The Adam optimizer performes a slightly modified version of gradient descent using a couple of tricks that make it more robust to hyperparameters than standard gradient descent (the link above has the algorithmic details).\n",
    "Typically some hyperparameter tuning is still required, especially for the learning rate and weight decay (if used).\n",
    "In this case the default hyperparameters should work quite well.\n",
    "\n",
    "Fill in the missing details in the function below.\n",
    "* The loss should be the MSE error (implemented without torch.nn). Make sure that your model predictions and Y_true have the same shape when you compare them.\n",
    "* Note the use of optimizer.step and optimizer.zero_grad\n",
    "* Note that the optimizer changes the weights in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209edacf-5338-40e1-a4b4-d95c0bcfc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_steps, weights, biases, X, Y_true, lr=1e-3, verbose=True):\n",
    "    parameters = weights + biases\n",
    "    optimizer = torch.optim.Adam(weights + biases, lr)\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO\n",
    "        # ***************************************************\n",
    "        Y = None  # TODO\n",
    "        loss = None  # TODO\n",
    "        # TODO: Use backpropagation on the loss\n",
    "        raise NotImplementedError\n",
    "        losses.append(loss.item())\n",
    "        if verbose and step % 100 == 0:\n",
    "            print(f\"step={step} - loss={loss.item():0.4f}\")\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if verbose:\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e2306ffb-040a-4e0a-b371-71c6975125b4",
   "metadata": {
    "id": "e2306ffb-040a-4e0a-b371-71c6975125b4"
   },
   "source": [
    "### Exercise 1.4 - Train the Network and Visualize the Results\n",
    "Below we provide a function that creates the dataset, trains the model and plots the resulting predictions. Note the use of `torch.no_grad` when we compute the final predictions for the model.\n",
    "\n",
    "Experiment with the depth and width of the network.\n",
    "* How wide / deep does the network have to be to approximate this function well?\n",
    "* In lecture we learned that the output of ReLU networks is picewise linear. Can you see evidence of this in the plots?\n",
    "* We also provide an alternative function that is more complicated. How does the neural network fare on this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94060a-5731-46c1-8d94-625e34f5ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(X, Y, N_sqrt, mark_level=None):\n",
    "    x1_grid = X[:, 0].reshape((N_sqrt, N_sqrt))\n",
    "    x2_grid = X[:, 1].reshape((N_sqrt, N_sqrt))\n",
    "    plt.pcolormesh(x1_grid, x2_grid, Y.reshape_as(x1_grid), cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "    plt.axis([x1_grid.min(), x1_grid.max(), x2_grid.min(), x2_grid.max()])\n",
    "    plt.colorbar()\n",
    "    if mark_level is None:\n",
    "        mark_level = float(Y.mean())\n",
    "    if isinstance(mark_level, (int, float)):\n",
    "        plt.contour(\n",
    "            x1_grid,\n",
    "            x2_grid,\n",
    "            Y.reshape_as(x1_grid),\n",
    "            levels=[mark_level],\n",
    "            colors=\"k\",\n",
    "            linewidths=1,\n",
    "        )\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "\n",
    "\n",
    "def plot_cross_section(X, Y, value=0, **kwargs):\n",
    "    x2 = X[:, 1]\n",
    "    value_rounded = x2[torch.argmin(torch.abs(x2 - value))]\n",
    "    mask = torch.isclose(X[:, 1], value_rounded)\n",
    "    x1 = X[mask, 0]\n",
    "    y = Y[mask]\n",
    "    plt.plot(x1, y, **kwargs)\n",
    "\n",
    "\n",
    "def get_alternative_data(N_sqrt):\n",
    "    img_path = Path(\"./img.png\")\n",
    "    if not img_path.exists():\n",
    "        imgURL = \"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl.png\"\n",
    "        urllib.request.urlretrieve(imgURL, img_path)\n",
    "\n",
    "    img = -np.asarray(matplotlib.image.imread(\"img.png\")).sum(axis=-1)\n",
    "    rows = np.rint(np.linspace(0, img.shape[0] - 1, N_sqrt)).astype(np.int32)[::-1]\n",
    "    cols = np.rint(np.linspace(0, img.shape[1] - 1, N_sqrt)).astype(np.int32)\n",
    "    rows, cols = np.meshgrid(rows, cols, indexing=\"ij\")\n",
    "    X = sample_grid(N_sqrt)\n",
    "    Y = img[rows.reshape(-1), cols.reshape(-1)]\n",
    "    Y = Y - Y.min()\n",
    "    Y = Y / Y.max()\n",
    "    return X, torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f1e7c-60ac-4109-867c-8e0360cbdc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play around with these settings\n",
    "N_sqrt = 100\n",
    "D = 4\n",
    "H = 32\n",
    "\n",
    "# Alternative might require more steps, a wider network or other learning rates\n",
    "alternative_dataset = False\n",
    "\n",
    "# You can set this to 'cuda' if you have a GPU available.\n",
    "# In this case the dataset and model is so small that we only expect a minimal speed difference.\n",
    "device = torch.device(\"cpu\")\n",
    "steps = 1000\n",
    "lr = 1e-3\n",
    "\n",
    "# Create dataset and move to device (if needed)\n",
    "if alternative_dataset:\n",
    "    X, Y_true = get_alternative_data(N_sqrt)\n",
    "    X = X.to(device)\n",
    "    Y_true = Y_true.to(device)\n",
    "else:\n",
    "    X = sample_grid(N_sqrt).to(device)\n",
    "    Y_true = compute_f_true(X).to(device)\n",
    "\n",
    "# Create model\n",
    "weights, biases = get_model_weights(D, H, device)\n",
    "assert all(\n",
    "    [p.requires_grad for p in weights + biases]\n",
    "), \"All model parameters should have requires_grad=True\"\n",
    "assert all(\n",
    "    [p.is_leaf for p in weights + biases]\n",
    "), \"All model parameters must be leaf tensors\"\n",
    "\n",
    "# Train the model\n",
    "train_network(steps, weights, biases, X, Y_true, lr)\n",
    "\n",
    "# Compute the final model predictions\n",
    "# Typically this would be on some sort of validation or test data\n",
    "with torch.no_grad():\n",
    "    Y_model = predict(X, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d65f1f-abc7-43ae-a11c-b8cb409e4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move things back to the CPU for plotting (if needed)\n",
    "X = X.cpu()\n",
    "Y_true = Y_true.cpu()\n",
    "Y_model = Y_model.cpu()\n",
    "\n",
    "plt.figure(figsize=(2 * 6.4, 4.8))\n",
    "plt.subplot(121)\n",
    "plot_heatmap(X, Y_true, N_sqrt, 0.5 if alternative_dataset else 0.0)\n",
    "plt.title(\"$F_{true}$\")\n",
    "plt.subplot(122)\n",
    "plot_heatmap(X, Y_model, N_sqrt, 0.5 if alternative_dataset else 0.0)\n",
    "plt.title(\"Model\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "x2_value = 0.0\n",
    "plot_cross_section(X, Y_true, x2_value, label=\"$F_{true}$\", color=\"r\", linewidth=3)\n",
    "plot_cross_section(X, Y_model, x2_value, label=\"Model\", color=\"k\", ls=\"-\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.title(f\"Cross Section at $x_2={x2_value}$\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9d8846a7-cd1f-42ff-8e64-56a09fcffd9a",
   "metadata": {
    "id": "9d8846a7-cd1f-42ff-8e64-56a09fcffd9a"
   },
   "source": [
    "# Exercise 2 - Digit Classification\n",
    "In this exercise you will build and train a neural network to classify handwritten digits in the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). The link contains information about the dataset including example images. There are 10 classes corresponding to digits from 0 through 9. Each image is 28x28 pixels and grayscale. We will be using a fully connected network so we flatten each element into a $d=784$ element vector. We will have one output for each class. We will use softmax to turn these outputs into probabilities for each class and the cross entropy with a one-hot encoding of the label as our loss.\n",
    "\n",
    "## Exercise 2.1 - Model Creation\n",
    "In this exercise you will use the higher level neural network functions in the `torch.nn`. A linear layer can be implemented using [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) and a ReLU with [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU). Our inputs to the network are a $n \\times d$ vector, i.e. a minibatch of $n$ flattened images. We will use three hidden layers of width 128 and a final output layer with 10 features corrsponding to each class. The output shape of a layer with width $K$ is $n \\times K$. \n",
    "\n",
    "Our PyTorch model is a class that inherits from [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). The module class provides various methods for keeping track of parameters and other nested modules such as the `torch.nn.Linear` layers we will use. Read through the code and fill in the missing details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4H5jHtLATXtu",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_model(torch.nn.Module):\n",
    "    # Models in PyTorch usually inherit from this Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: Fill in the missing arguments here!\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "        self.input_layer = torch.nn.Linear()  # TODO\n",
    "        self.input_phi = torch.nn.ReLU()\n",
    "        self.layer1 = torch.nn.Linear()  # TODO\n",
    "        self.phi1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear()  # TODO\n",
    "        self.phi2 = torch.nn.ReLU()\n",
    "        self.output_layer = torch.nn.Linear()  # TODO\n",
    "\n",
    "    def forward(self, Z):\n",
    "        Z = torch.flatten(Z, 1)  # Flatten (n, 28, 28) to (n, 784)\n",
    "        Z = self.input_layer(Z)\n",
    "        Z = self.input_phi(Z)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: Fill in the rest of the forward pass\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "m1xrcVxRbqdc",
   "metadata": {
    "id": "m1xrcVxRbqdc"
   },
   "source": [
    "## Exercise 2.2 - Training Script\n",
    "Here we provide most of the functionality needed to train the model. Read through the code and fill in the missing details in train_epoch. Note that we use the Adam optimizer again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iSrR-ecmdxJH",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def train_epoch(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()  # Important set model to train mode (affects dropout, batch norm etc)\n",
    "\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # ***************************************************\n",
    "        # TODO: Move data and target to the specified device\n",
    "        output = None  # TODO\n",
    "        loss = None  # TODO\n",
    "        # TODO Backpropagate\n",
    "        # TODO Optimizer step\n",
    "        # TODO Zero the gradients\n",
    "        raise NotImplementedError\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        accuracy_history.append(correct / len(data))\n",
    "\n",
    "        if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch}-{batch_idx} batch_loss={loss.item()/len(data):0.2e} batch_acc={correct/len(data):0.3f}\"\n",
    "            )\n",
    "\n",
    "    return loss_history, accuracy_history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()  # Important set model to eval mode (affects dropout, batch norm etc)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item() * len(data)\n",
    "        pred = output.argmax(\n",
    "            dim=1, keepdim=True\n",
    "        )  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(val_loader.dataset),\n",
    "            100.0 * correct / len(val_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    return test_loss, correct / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_predictions(model, device, val_loader, criterion, num=None):\n",
    "    model.eval()\n",
    "    points = []\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        data = np.split(data.cpu().numpy(), len(data))\n",
    "        loss = np.split(loss.cpu().numpy(), len(data))\n",
    "        pred = np.split(pred.cpu().numpy(), len(data))\n",
    "        target = np.split(target.cpu().numpy(), len(data))\n",
    "        points.extend(zip(data, loss, pred, target))\n",
    "\n",
    "        if num is not None and len(points) > num:\n",
    "            break\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def run_mnist_training(num_epochs, lr, batch_size, device=\"cpu\"):\n",
    "    # ===== Data Loading =====\n",
    "    # The input images should be normalized to have zero mean, unit variance\n",
    "    # We could also add data augmentation here if we wanted\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    train_set = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "    # Here we use the official test set as a validation set\n",
    "    # This is not a good practice (but quite common since it is easier to setup)\n",
    "    val_set = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "\n",
    "    # The dataloaders can run in separate threads and handle the actual data\n",
    "    # reading, augmenting and forming mini-batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,  # Can be important for training\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # ===== Model, Optimizer and Criterion =====\n",
    "    model = MNIST_model()\n",
    "    model = model.to(device=device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.functional.cross_entropy\n",
    "\n",
    "    # ===== Train Model =====\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, device, train_loader, optimizer, epoch, criterion\n",
    "        )\n",
    "        train_loss_history.extend(train_loss)\n",
    "        train_acc_history.extend(train_acc)\n",
    "\n",
    "        val_loss, val_acc = validate(model, device, val_loader, criterion)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "    # ===== Plot training curves =====\n",
    "    n_train = len(train_acc_history)\n",
    "    t_train = num_epochs * np.arange(n_train) / n_train\n",
    "    t_val = np.arange(1, num_epochs + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(t_train, train_acc_history, label=\"Train\")\n",
    "    plt.plot(t_val, val_acc_history, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t_train, train_loss_history, label=\"Train\")\n",
    "    plt.plot(t_val, val_loss_history, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    # ===== Plot low/high loss predictions on validation set =====\n",
    "    points = get_predictions(\n",
    "        model,\n",
    "        device,\n",
    "        val_loader,\n",
    "        partial(torch.nn.functional.cross_entropy, reduction=\"none\"),\n",
    "    )\n",
    "    points.sort(key=lambda x: x[1])\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for k in range(5):\n",
    "        plt.subplot(2, 5, k + 1)\n",
    "        plt.imshow(points[k][0].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(f\"true={int(points[k][3])} pred={int(points[k][2])}\")\n",
    "        plt.subplot(2, 5, 5 + k + 1)\n",
    "        plt.imshow(points[-k - 1][0].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(f\"true={int(points[-k-1][3])} pred={int(points[-k-1][2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "J67L0FfofKYh",
   "metadata": {
    "id": "J67L0FfofKYh"
   },
   "source": [
    "## Exercise 2.3 - Train the model\n",
    "Experiment with the learning rate and number of epochs and see how the accuracy is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DtvkeRKTqq9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play around with these settings\n",
    "lr = 5e-3\n",
    "batch_size = 128\n",
    "num_epochs = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "run_mnist_training(num_epochs, lr, batch_size, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
