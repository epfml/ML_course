{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_loss(y, tx, w, mse=True):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    e = y - tx@w\n",
    "    if not mse:\n",
    "        return 1/(y.shape[0]) * np.sum(np.abs(e))\n",
    "    return 1/(2*y.shape[0]) * e@e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i in range(losses.shape[0]):\n",
    "        for j in range(losses.shape[1]):\n",
    "            losses[i,j] = compute_loss(y, tx, np.array([grid_w0[i], grid_w1[j]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=15.55870336860953, w0*=72.72727272727272, w1*=13.636363636363626, execution time=1.400 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAF5CAYAAAAmk6atAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAB/DUlEQVR4nO2deXhU9fX/X4d9RxSJgPELKtiKVlEUXKpYW7e2CigurUCrLVViq10N+mudtiK41aUFlKpVrBsqoG0F90hbATdwwQVRrEEQRZFdZDm/P869ziRMkkkyM/dO5ryeJ8/c+dzt3CRM3pxVVBXHcRzHcRwn/jSL2gDHcRzHcRwnM1y4OY7jOI7jFAgu3BzHcRzHcQoEF26O4ziO4zgFggs3x3Ecx3GcAsGFm+M4juM4ToEQuXATkdtE5CMReS1lLSEiH4jIwuDrpJR9Y0VkiYi8JSLHR2O14zj5QkTaiMhzIvKyiCwSkd8H61eLyJsi8oqIzBCRnVLOSfs5ISIHi8irwb4bRUSC9dYicl+wPl9EeuX7OR3HcTIhcuEG3A6ckGb9OlU9MPh6BEBE9gXOBPoF50wSkeZ5s9RxnCjYDHxDVQ8ADgROEJFBwOPAfqr6NWAxMBbq/JyYDIwG+gRf4WfPucBqVd0buA64Mg/P5TiOU28iF26qOgf4NMPDTwHuVdXNqroUWAIcmjPjHMeJHDXWB29bBl+qqo+p6tZgfR6we7Cd9nNCRLoDnVR1rlrn8anAkJRz7gi2HwCODb1xjuM4cSJy4VYLFwQhkNtEpEuw1hOoTDlmWbDmOE4TRkSai8hC4CPgcVWdX+2Qc4BZwXZNnxM9g+3q61XOCcTgGmCXLD6C4zhOVmgRtQE1MBn4I6DB67XYB3O6/wGnndklIqOxkAj77rvvwaWfvs6qnRtv2GdtOzX+ItX4mF2zfs26WLt+p7zf09mRTh0+y+v9duXjWve/8+LaVapar1/IgSK6phE2vQWLgM9Tlqao6pTUY1R1G3BgkMc2Q0T2U9XXAETkUmArcFdweE2fE7V9fmT82RIFXbt21V69emV07IYNG2jfvn1uDYoJxfKsxfKcUDzPWtdzvvjiizV+FsdSuKnqynBbRP4K/DN4uwwoTTl0d2B5DdeYAkwBGNBPdPYxwMWNt+3hAwY1/iIp3MRP6JvVK9bOrDnD8ng3py7WAiceNT2v9zyPm2vcd4o89r/6Xm8NcGsj7DkSPlfVAZkcq6qfiUgFlpv2moiMAr4DHKvJwcs1fU4sIxlOTV1PPWeZiLQAOpN5CkfO6dWrFy+88EJGx1ZUVDB48ODcGhQTiuVZi+U5oXieta7nFJEaP4tjGSoNclFChgJhxenDwJlBBVhvLLn4uXzZ9fABx2X1ejfxk6xery5ctMWTfP9c8v1711hEZNewYlRE2gLfBN4UkROw/46drKobU05J+zmhqiuAdSIyKMhfGwk8lHLOqGD7NOCpFCHoOI4TGyIXbiJyDzAX2EdElonIucBVQcn+K8AxwM8BVHURMA14HZgNlAUhlLrJgretkHHRFm9cvNVKd+Dp4PPgeSzH7Z/AX4COwONB26CboM7PifOBW7CChXdI5sXdCuwiIkuAXwDleXkyx3GcehJ5qFRVz0qzXGPkRVXHAeNyZ1F6Ctnb5qKtMJg1Z1jew6aFgKq+AvRPs753Leek/ZxQ1ReA/dKsfw4Mb5yljuM4uSdyj1sh4KLNyRf5/HkVmNfNcRzHwYVbk8ZFW2Hi4s1xHMepCRdudVCo3jYXbYWN//wcx3GcdLhwyyMu2pz6kK+fo3vdHMdxCgcXbrWQbW9bPnDR1rRw8eY4juOk4sItT+TjD6OLtqaJ/1wdx3GckMjbgcSVQvS2FQWJeq43EfLRKsT+c/FYTu/hOI7jNA4XbnnAvW31IJGD8xp6zZjhfd4cx3EcF25pyKa3zUVbHSQiuEc+7pkjXLw5juMUNy7cCpyCFG2JGN0/UcMxjuM4jtNAKith4kQoK4PS0uxe24VbNQrJ21ZQoi0RtQE1kKhhO8a4181xHCfeTJwIV14JIjB+fHav7cKtQCkY0ZaI2oB6kKj2GmNcvDmO48SXsjITbWPGZP/aLtxSKCRvW+xJRG1AI0jUsB0zXLw5juPEk9LS7HvaQryPWw4o6hBpgliLnXqTiNqA2on170KRICK3ichHIvJaytrVIvKmiLwiIjNEZKeUfWNFZImIvCUix0ditOM4BYsLt4BC6dsW2z/UCWIvchpMgqb7bE42uB04odra48B+qvo1YDEwFkBE9gXOBPoF50wSkeb5M9VxnELHhVuWyaW3LZaiLUHxiJoEsXzWWP5eFBGqOgf4tNraY6q6NXg7D9g92D4FuFdVN6vqUmAJcGjejHUcp+DxHDen4SSiNiAiEsTu2T3fLdacA9wXbPfEhFzIsmBtB0RkNDAaoKSkhIqKioxutn79+oyPLXSK5VmL5Tmh6T5rt8cfZ2vnznx6qP0/rTHP6cKN7IVJi8bblojagBiQqPYaA1y8xQ8RuRTYCtwVLqU5TNOdq6pTgCkAAwYM0MGDB2d0z4qKCjI9ttAplmctlueEJvqszz0H11wDgwfDr38NIo16Tg+VFgAu2mJMImoDnLgiIqOA7wDfV9VQnC0DUttx7g4sz7dtjuPkiZUrYdgw6NED7r7beoQ0kqIXboXgbYsNiagNiCkJYvO9iZXIL2JE5ATgYuBkVd2Ysuth4EwRaS0ivYE+wHNR2Og4To7ZsgWGD4dPP4UZM2CXXbJy2aIXbnEnFn+IE8RGmMSaRNQGGLH4nSkiROQeYC6wj4gsE5Fzgb8AHYHHRWShiNwEoKqLgGnA68BsoExVt0VkuuM4ueSXv4R//xtuuQUOPDBrly3qHLe4e9ti8Qc4EbUBBUYC/54VGap6VprlW2s5fhwwLncWOY4TOXfcAX/+M/z85/C972X10u5xc2omEbUBBUoiagNiIvodx3GKkRdfhJ/8BL7xDbjqqqxf3oVbI2my3rZEtLcveBJE/j2M/HfIcRyn2Pj4Yxg6FEpK4N57oUX2A5tFK9ziPCkh8j+4iWhv36RIRG2A4ziOkxe2bIHTTzfxNmMG7LprTm5TtMItGzTJStJE1AY0QRLR3Try/wQ4juMUEJWVUF5ur/U+9te/hooKmDIFDjooZzYWpXBzb1sNJKK7dZMnEbUBjuM4Tl1MnAhXXgmTJtXz2DvvhBtugAsvhBEjcmpjUQq3bJALb5uLtiZOIprbutfNcRwnM8rKzIs2ZEjdnrfw2Au//hKMHg1HHw1XX51zG124OS7a8kkimtu6eHMcx6mb0lIYP95S1OryvJWWwvhffMxu5w9l68678sf9p1H5Ycuc21h0wi0bYdIm5W1LRHPboiYRtQGO4zhObYTetDFj7H3a3LetW+GMM2DlSiZ/czq/+0u3jEKsjaXohJuTQiJqA4qYRP5v6V43x3GczCgtNdE2caKJtbS5bxdfDE8/DVOmMOTyAVWEXi4pqskJ7m1LIZH/WzrVSOA/B8dxnJgSijUR88CJpAizu++GP/0JfvpTGDmSUizEmg/c41aMJKI2wImKQvS6iUipiDwtIm+IyCIRuTBYP1BE5gWzQF8QkUNTzhkrIktE5C0ROT5l/WAReTXYd6OISLDeWkTuC9bni0ivvD+o4zhZo3posz5tPkJSw6Vh7ltpKbBgAfzoR3DUUXDttTmxvzaKyuMWR/L+hzSR39s5dZDAfyZ1sxX4paq+JCIdgRdF5HHgKuD3qjpLRE4K3g8WkX2BM4F+QA/gCRHpGwxznwyMBuYBjwAnALOAc4HVqrq3iJwJXAmckd/HdBwnW4TesnXroGNHWLsWJk82r9n48cnwZ1lZIMZqQLXawqpVNhlhl13g/vuhZe6LEapTNB63OIZJXbQ5QN5/LoXmdVPVFar6UrC9DngD6Ako0Ck4rDOwPNg+BbhXVTer6lJgCXCoiHQHOqnqXFVVYCowJOWcO4LtB4BjQ2+c4ziFR+gtU02GO1Nz0DLp17bDMVu3wllnwYcfwvTp0K1bzp8jHUXhcfusbae6D3KcKEngwjoDghBmf2A+cBHwqIhcg/0n9PDgsJ6YRy1kWbC2Jdiuvh6eUwmgqltFZA2wC7AqF8/hOE5uCUOblZXQuXMy3BlSPWctnQduh7y2Sy6BJ56Av/0NDjkkr8+TSlEIt2zg3jYn5yTI289p1pxhnHjU9Kxcq8POcMTxdR9XI/fQVUReSFmZoqpTqh8mIh2AB4GLVHWtiFwO/FxVHxSR04FbgW8C6TxlWss6dexzHKdACQVcXeuhd+2ZZ2DaNNtf5Zh777XmumPGwA9+kA/Ta6RoQqVFTSJqAxynVlap6oCUr3SirSUm2u5S1VBxjgLC7fuBsDhhGZCatbI7FkZdFmxXX69yjoi0wEKvnzb2wRzHKQzKymDQIJg3L0349OWX4Zxz4Mgj4brrarxGQwogGoILtwwoaG9bIn+3crJAIn+3KpRctyDX7FbgDVX9U8qu5cDRwfY3gLeD7YeBM4NK0d5AH+A5VV0BrBORQcE1RwIPpZwzKtg+DXgqyINzHKcIKC01T9sOvdg+/dSKEXbeGR54AFq1qvEa9Zlz2hg8VNqUSURtgNMgEvjPripHACOAV0VkYbB2CfBj4IbAQ/Y5Vi2Kqi4SkWnA61hFallQUQpwPnA70BarJp0VrN8K3CkiSzBP25k5fibHcWLGDmHVbdusGOGDD2DOHCgpqfX8HXLicoQLtzxTKF4OJ2IS5EW8ZTPXLVeo6n9In4MGcHAN54wDxqVZfwHYL83658DwRpjpOE4ToEqRwsRL4bHH4JZbYODAOs+tKZ8u23iotA5yMSkhLySiNsBxHMdxCosw3Pnvn06zjfPOg3PPBfKXw1YXLtyaIomoDXCyQiI/t3EvsOM4hUpDxFRt55SVweVnvMqp//ohmw8+HG644ct9+cphqwsPleYR/wPp1JsELsQdx3FqoPqEhKFDYcaM9BMRwjBorVMU2n/KeY8O4ZOtnRmtD/DbBa2+vF6+ctjqwoVbLRRkmDQRtQFOIVIIuW6O4zjVCcXUmjXJPmzz5iUFVmpT3VDkjRmTforC+jXbOP9f32OftZX8qO8z/Oul7nxyUfJ648fnb5B8bbhwyxN58bYlcn8LJwIS+M/WcRwnDakTEkTM83bQQUnRFo67Gj8+KfKGDDGvXEi4PvjR/0e/ykcZzc20/+ZhlA+zY2fOjN7LlkrkOW4icpuIfCQir6Ws7Swij4vI28Frl5R9Y0VkiYi8JSKN6ddeKwXpbXOaLonc38JD+Y7jFBphvhpYqPTOO6FTJxN04bzSUHSFIm/GjKq5aqWlMP6g+zl+wQT+s+9oWo4ZTXm5HTtwoL3WNog+38TB43Y78Bds4HNIOfCkqk4QkfLg/cUisi/WX6kf0AN4QkT6pvRoiiXubWsgT89v+LnH1F267TiO4xQ2qV616jlopaU7hkuhquetvBwu+uZr7PbDH8KgQRxZcSNHto7scTIicuGmqnOCwdGpnAIMDrbvACqAi4P1e1V1M7A0aJZ5KDA3L8Y6uacxYq226zQFIZegaQp0x3GcehIWFAwdmhRr6fqojR9vhQjr1tnxkDyuvBxuvnI1v755iLnrHnwQWrdOO3A+TkQu3GqgJBhPg6quEJFuwXpPYF7KccuCtR0QkdEEndR33aNNvW6ezTCpe9syIFtiLdN7NAUR5ziOU8RUz1+rD1+KvpO3cebfz6bT8vdZdOXT9OvRo9HXzgdxFW41ka57etp5gsGg6ikAew/o3HRnDiaiNqCB5EOs1XXvQhRwCQr3Z+44jpMlamvNkeoxGzsWOneuelwozI77d4KBHzzC+Uxi4d+OYO5o2z90qFWnDhmSl0epN5EXJ9TAShHpDhC8fhSsLwNSHZe7Y4OmnULh6fnRirZU4mRLfUhEbYDjOE60hOHOdKHM1Ea56Y4rK4M7h07nG89ezkffPZeFA89j7NhkU94ZM6wFyMyZeXucehFX4fYwMCrYHgU8lLJ+poi0FpHeQB/guWzeuKDCpIncXj7rxFUkFaqAcxzHKSKqTzyoaQJCWE0aFh/Mn1/1uNJ1r3P246Ng4EC63T+RufOEZ59Nir2hQ2HQoKoet7iMu4IYhEpF5B6sEKGriCwDLgMmANNE5FzgfYLhz6q6SESmAa8DW4GyuFeUOhSOKHp6fuGETxMUnnB3HMdpBKEn7ZlnYNq0HXPRUkOk48dbeHTyZJsTv2BBcNzFn7HlO0P4XNvz9qUPcsvPrYR01KiqTXtDj1s4Wz5OeW+RCzdVPauGXcfWcPw4YFzuLMoO7m2jcARbKoWc/+Y4jtOEGToUpk41UTVpEhx+OJSUwOLFJtrSVZAC7L8/HH88jDlvO5tOPZuW7y3lJH2Kd37SkxUr7BiR5Dnp8ufiMu4KYiDc4oQ33c0ihSjaUikE71uCwhDwjuM49SQUYmAFBgAXXggrVlgYc8wYGD4cVq6E6dOhb98dz4fkeKvSUuCy38NT/6KMv/B2yddZuQK6doVVq5LnpHrsUknXaiQqXLgVIomoDaiDQhdtIe59cxzHyRmpQgls+9hjk9uTJ9u2CLz4ouWqDRpkYVIwsbZ6NbRsmcxHW7AARo5Mnj9oECxfDk9f9BAjp/+B9af/kE69x3DccpuycNJJ0KEDqCY9dmEoNo493MCFW04o2tFBTUWwVSfO3rcE8RfyjuM4KYSCbe1aE0oiJpyuvNLCmpAMgx5xhO0LRdv11yfPvfNO6N7dvHBTp8Kzz8LChXDyyfDXv9rx8+bBdT95kykvj+D19gPoePkkxvcRKiuhZ8+qM01HjLDrhaHY6nlzcRFyLtwCCiZMmojagBpoqqItJM7izXEcp4AIhVIYxgzzxkSgWzcTS+edZ2HQ5ctNrIW92FLPDYVZSQlUVMDrr9t1PvoILroINmyAESev4eaXh7CxeRuO3zCdQ8vb0KdP1XBomL+2Zk3VUGyqrXEoSghx4eY0nqYu2kLiKt4SxFfQO47jVCM10T/VizV+vAmw8eNNQJWUWH5bqserepHAiSfCokUm8vr1g02b4N134b33AN3O92aPoNXWd/hJ6ZMs+18pW/5rOXGpFaShiJs/30Kt119v1y4vrzpSqzby6Zlz4ZZlchomTeTu0g2mWERbSFzFm+M4TsyoScxkmuh/6qkwe3bVStHSUstnGz4cbrjB8ttCjj7awqqTJ9vr7/gjJ3zxD37KjcjRRzFosQnBRx+FDz6wkOrChcmcttTGu2HoNlNPWz49cy7cnIZTbKItJI7iLUE8hb3jOEVFqlhrqJgZOxbWr7ectT33tLX1680Ddvjh8L3vWRh06FC46SZIJKB3bxN3AMOGQbd5D/P75Qme3mMk2799AZePTYrHZ5+1HDaAHj2SOW3VvXn1af+Rz3YhLtzIXn5bUXnbilW0hcRRvDmO40RMqlhLFTPpKkhrCisuX27hzA0b4K23rHp07lx4+21r37Fhgx23YoV5z447LlnoAHD6AW8xYfkIXuBgTnr/Joavly89dAMH2n3XrbOvDRtg6VLz4lX3BNZHbOazXYgLN6f+FLtoC4mbeEsQP4HvOE5RUT1/LRQz5eVJQZcahkzNMwtF3IUXJsXZpk3WYDdk7drkdvfucPPNsG2bibu+faH9trVc++4QtjZvzbBt0+lc0paXXrI8uGHDzLsG0LGj2XHnnfY+dUpC3HHhVggkojYgBRdtVYmbeHPyjojcBnwH+EhV9wvWdgbuA3oB7wGnq+rqYN9Y4FxgG/AzVX00ArMdJyfU5HmqKQyZLpx6ww1wyilWcBBes1Mn+PBD+OST5DXDqQdg4k7YzoOMYjfe5i+nPEHPlXswb5555FatMk/ehAkm2sLK1NCeOExEyJS4DpnPGwURJo0LLtrS49+XYud24IRqa+XAk6raB3gyeI+I7AucCfQLzpkkIs3zZ6rj5I7aBrGHgq60tOp2WZmJpjVrkucNHGhiK6RtW/OYHXqobaeja1f4Y5srGMpMEu2v4dQ/D2baNLOnrMy8c2C5cmvXJluRTJyYLHooFIpeuMWeRNQGOBkRF/GWiNqA4kNV5wCfVls+Bbgj2L4DGJKyfq+qblbVpcAS4NB82Ok4uSb0noWJ/6lUF3WVlSaexo9PVoKefrqFSMvLq567dKm9LlxoodNWrZL7WraELl3gH+f/i7Gf/447OZv7e1wIJCtQw+rRfv3glVfsXp06FZZYS8WFm5MZcREmcca/R06SElVdARC8dgvWewKp/ohlwZrjFDxlZSa6DjvMmtjOT/lIvPRSE3X/7/+ZaBs+3ATU5Mlw773mSZs3D955x47r2NGEFsCWLfYahk6/+MJemze3fbusfpt+V3yf93c+kJ9wM4vfFi691O4zdKg15AUTfAsXVm2wW4h4jptTNy5ICosE7nmLL5JmTdMeKDIaGA1QUlJCRUVFRjdYv359xscWOsXyrHF9zi1bknlmYShyv/2sR9rw4dbMdtMmWz/sMDjgAGjXDubMsf2jRpkI27rVjhGBkpL1TJpUwd57wyGHJHPamjeHNm3MI9e8uRUkALTZspEzrx9Di3XKnIt+w7gOz7F9e/I+v/yleeU6dIBmzeyre3cTiO+8k7/vVXUa8zMtauEW+/y2RG4uWy9ctNUPL1ZwjJUi0l1VV4hIdyD4Pz/LgNQAze7A8nQXUNUpwBSAAQMG6ODBgzO6cUVFBZkeW+gUy7PG8TlDr1noVSsvT1aLjhhhrTuuvz5Zqbl2LZx/vnnY+ve3IoGwHcfChSYAN22Ca66pYMKEwZx4op0XVn2CjcM6/nibZ3rxxaCqzGh+Gjttq+RbPM7Tv/sGYPluRx0F7dub527kSBuhtXAh7LKLDZYfNy7aUGljfqZFLdwcJyfEQbwliIfwL14eBkYBE4LXh1LW7xaRPwE9gD7Ac5FY6DiNYOJEE239+5s3bcwY87B1724TDHr0sOPGjLGCgFdftarORMK8ci++mJwt2r+/janq2tW8aatWmWDr1w9at4bNm+24jz6Cf/4zKebGMoEh26bzS67lab7xpW3Nm1sfODBBOWOGiTYwD96dd5qgmzix6jPFcaB8Oly4xZVE1Abg3rbGEAfx5uQFEbkHGAx0FZFlwGWYYJsmIucC7wPDAVR1kYhMA14HtgJlqrotEsMdpxFUb+8xcaI1w12xAn76U/OePfaYibmQkhJ7n7q2zz4m5BYsMMGWyqJF9hqGRtu3h9Wrbe14ZnM5l3IX3+NP/PzLc1q3tly4rl3Nsxbat26drT/zjN1n7lwTaqkCLY4D5dPhwq2RNNk2IC7aHCcjVPWsGnYdW8Px44BxubPIcXJPuua6I0eaV2x5EPzfbTfLNdu40Zrjfvyxrffta56vTz6BFoEK2Wkn+OwzC7dWZ9s2u85++8Hzz0Pv7Uu4h7N4ha/xY/5Kaupot24myFatMq9fKMxC71plpVWvhmOuUgVaPsdWNYairSrNVn6b49RI1OI3Ee3ts4WIlIrI0yLyhogsEpELq+3/lYioiHRNWRsrIktE5C0ROT5l/WAReTXYd6OISLDeWkTuC9bni0ivvD2g4xQ4YTXp5ZebIAob265YYaItDJuG3rKWLS1HrU0beP99C11+9pnt27696rXbtbPWHRs3Wmi27fb1zGQI22nGUGawuVm7Ksd36mT3HjHCiiTCcVshpaV82d+tukBL7S8XZ4pWuMWaRMT3j1pwNCX8e5kNtgK/VNWvAoOAsqCRLSJSCnwLC0cSrNXW5HYyVqnZJ/gKG+eeC6xW1b2B64Arc/1QjtMUqKw0sbNmjb0vLTVRtG6drXXpAl/7WrIlB1gINJGAzz9PDoZPR+/eJtgkcKjtsrMytfkP+SpvcCb38h692b7dQqlgr1dcYd61Hj1MEIb94ebPT/aRKxSBVhMu3BpBkwyTutBwYoaqrlDVl4LtdcAbJHufXQf8hqotNdI2uQ2qOzup6lxVVWAqVRvjhg1zHwCODb1xjuMkqd5Id+LEZD+2sPHu+PEmmpYuNS/b7NnmUQsFVtu2JshSad06KdBC3nvPxFUoCs9bcyXDtj3AtV0n8OF+3/ryWmFrkG3b4IEHLKR6993QubMJx3nz4KKLam4OXNOzxRXPcXOcXBNloUKC3HtwdwMubsT599BVRF5IWZkStMLYgSCE2R+YLyInAx+o6svVNFZPYF7K+7DJ7ZZgu/p6eE4lgKpuFZE1wC5AtXRpxyluUhP4x4yxNh8jRliVZhiWnDs3eXznzia82rQxDxtY4UKzZlXDomHlaCqqyXy5E2U2f9h2CfdxOlfrr5hwUXIYfXjt1q3h4YeTQi9k0CBrTTJzZu35a16cEGNind+WiPDe7m1zomGVqg6o6yAR6QA8CFyEhU8vBY5Ld2iaNa1lvbZzHMdJITWBf/x487SNHGkhzxNOsGa6ixebp2zVKmv1UVFhwqpFi2Sz3eq5bDWxyy7Q4aN3uEvP4lX2Z3Tz21j7iXDBBXbNZs2SgnDz5qQA7NzZqlgHDkz2bBs4MOlVS9fyI/XZ4twaxEOljuGiLbf497dRiEhLTLTdparTgb2A3sDLIvIe1sj2JRHZjZqb3C4Ltquvk3qOiLQAOrPj/FHHcTCP1vjxyRmi//63hUZff91EG1jF6KZN8MYb5pHr1s1EW5s29bvX+o82MIOhKMJQZrB2W3tKSpJiLRSAzQI107mz9X87+WSYOtUmJowfXzW0W1PINDX3rbbjoqYoPW7ZoEnmtzlNkwTRF7w0giDX7FbgDVX9E4Cqvkpy/ieBeBugqqtEJG2TW1XdJiLrRGQQMB8YCfw5uETYMHcucBrwVJAH5zhOCmFOG5hHC5IVoV26WFjy/fdNML36Kpx7Lvztb1ac0K6dCatwTFbdKLdxDv1YxInMYil7AnDkkfDUU5Y/17GjefJWr7Zrb9xoxQ+LFsFrryV7xnXubKIs05YfcW4N4sItTiQiuq97g/KDN+VtKEcAI4BXRWRhsHaJqj6S7uA6mtyeD9wOtAVmBV9gwvBOEVmCedrOzMFzOE5BU1lZNaetVy+rDg29aN/5jlVzzpqVPOeWW5IVpRs37liUUBu/4hrOYBq/4UoeD7IiRGzqQthaJLUqdfPm5ED61IkL/fsnBVhq/7nayPS4KPBQqePkExfJ9UZV/6OqoqpfU9UDg69Hqh3TS1VXpbwfp6p7qeo+qjorZf0FVd0v2HdB6FVT1c9Vdbiq7q2qh6rqu/l7QsfJD/Wtmkw9PpxNOnmyibaOHa1yc+PGpAdt1Srrkda6tYmlbt3gq1+tes0WGbqLvsnjTKCcaQznan795bqqVZuC9YNr2TJ5zuDBdm8wAfn662bH5MnmKYx7tWimFJ3HLdaFCVHgQqI4SFDQ4VLHcRpPplWTYWL+2rUmep55xgRQOJu0osJE0bBhdq0NGyy37fnnk2OrwhDl5s3Wj235ctsOixNqoxdLuZczWUQ/zuE2QKpUpYaVqqF3DawtyK67mq0zZ9ps0qVLYc89bVZpIVSLZop73BpATvLbEtm/pBNTXCw7jhMB4YSDuvK2UgVe//7WB23lSstf22+/5HD4J5+Ed96xIoS+fZMFAqmsWWMCavPm9Pur0+KLz5nBUJqxnaHMYAMdgKrtQjp1MvEG5sFr29ZsuPNOE23jx8OHH9r+d9+t+tyF0qutNorO4+ak4ALCcRynaMg0bys1Mf/SS817NmeOedMOOsjWp01LetfWrs3s/nW3AFGOu/9q9uEVTuIR3mWv5J6UUqHKymSItH17E4f9+sHRR8OQISbMfv97e9brr08/V7WQvW/ucXOcKIhCNCfyf0vHcQqPdCOhVq0yj9vIkZbfNqDOzov15+dcx1cXPMWljOPRL6fRpScMk4aVrUcfbaLsvPNMmD36qDUCHlitHixTr2OccY9bHEhEcE/3tjmO4zgZss8+Fja94w7Le6uryKB9ewtfZtpo9xs8ydX8msX7H8WEV8t32C+S9Lp16QLHHGMtR1Thm980QTl8uOW21Uacq0Uzpag8btkoTPD+bY7jOE5ToLLSBE9YeBCulZcnh7J//LGtf/ihCbaVK5MTENq0Me9bOjZsyFy0/R/vcR9n8CZfYfYZF5NukElqqLRLF8tdU7W8tk6drAAhLJ4YM8Zsb6q4x81xosL7ujmOk2NqG900caIJH7Ah7HPnJsdYPfaY5ba1a2f7w/mfb71loq1lSysYCCs9G0pbNjKDobRgK0OYyXltPkh7nIgVN2zbZqINrA3JiBFm26hRVRvmxnVcVTZw4VaMeJi0eEnguW6OU0TU1gKkrMzEz6uvWhJ/6oD4/feHL76wCQQdO9r5u+1mQmjRIhNvjZ8totzMTziAl/ku/2AJfYCqwq13b6tKDUVbSLNm5v17+22rel2wwAomSkubRgFCbbhwi5pE1AY4keJeN8dxckhto5tKSy1nDSzMOHCgCblBg+Dyy21oPJiA27zZqkc/+cTWsjEQ7kJuYAR/57f8gUf4dtpjwma7qaINYO+9rY/ckCFw/vkm3iZMSHra4jquKhu4cCs23NvmOI5TNGSSjF9ZCUOHmmjr0cO8bxMnJkdZtWqV7KMWCrfGMpinuYZfMYMhjOPSGo+rLhC7dIGddrLtxYtNePbubR63cPxVUyhAqI2iKk5oLF6Y4OSEfIvpRH5v5zhO/EgtQjjhBBNtXbvC9OnJSQMvv2zHps4DzQalvM80TmcxfRnJVLQWKdKsWbLZbv/+Ngf1889NtE2fbvl4b71l+2sqlGhqFI1w81FXjuM4TrFSfWLA+PEmzs4/PzkJoaTEPFjLl1uY8cgjk+e3bm1TExpLGzYxnWG04guGMJP11K62tm+3UC3AYYeZqFyxIlk0AdaqpLy8aVeSplI0wi2WJPJ8Pw+TOo7jFCVhkcKkSVXX99vPcsVKSizEOHmyVZquWwe77JIcU9WxY3KYfMNRbuI8DuIlvs9dvE3fWo8WMW/bpk2Wd1denmyg+9RT5oEDK5pInYxQyOOsMsGFm+PEARfVjuPkkKFDTeh88IEJm7FjTeSMGwd9+liFZkVF8vhZsywUGfZiW7UKVq9unA0X8BdGMZXL+D3/4js77A/7wjVvbu/btoVrrzXR9qMfWYPd5ctNpPXoYeupPdtqEqdNjaIoTviYXevQ9UWACwPHcZyiZcYMS+BfsMDEUZj0v3y5tdRo3jzZk62kxIRcdTJtqJuOo3iG6/g5MzmlxmKEzz83T19YQbpxo81KXbkSliwx8Xj++fDSSybSJk824RZWkjb1atKQohBu2cALE5wmRQIvUnCcIqKsLFlkoGqiB2DqVJtyAFY9KmIh0tWrk7lljWV3Krmf4Sxh71qLEcLB8SFdu5qXcPbspC29e5uHbehQs3XNGvOyrVtngrSpNt1NxYVbVCSiNsCJHd7TzXGcHNKxowmeO+6AU0+Ff/zDRFvHjtYXbckSE0BhsUI2aM3nTGcYbficIcxkHZ1qPHbLlmROHZgA693btteuNU/gm29aCDdsrltZaXlwoYBrqk13U3HhVgx4mNRxHKdoSB1zBba9dq152Z55xprVDhqU9GKtW2eNbjduzLYlymTO5xBe4GQe4i2+UuvRIlXDsYmEedxEzO5Fiyxs2r+/CbXKymTPtlDANfUwKRSAcBOR94B1wDZgq6oOEJGdgfuAXsB7wOmq2si0ScdxHMcpfFLHXKnadr9+9tWjh8333LAB9tzTBNv27Y0vPAhp1SopCMcwiR9yOwku4x+cXOe51ZvtPvAAXHEF3HCD7Vu0yETboEEmQjt3TnrXmnrT3VRiL9wCjlHVVSnvy4EnVXWCiJQH7y+OxjTHySL5DJcm8JC94zRBhg41D9WQISbUwoHxkBQ/4fuRI80D97//JacjNIZQtH2dOVzPRfyD7/AHfpfRuakVpf372wzVhQvhootsmsOCBfbao0fxeNfSUajtQE4Bgglr3AEMic4Ux3Ecx4meykoTM+edZ2Js5kzzRA0aZPv79jVBFOaNNW8Ou+9uY6QmTrSihGzQk2Xcz3DeZU/O5u+1TkYIhVrz5hay3XlnKz546CG46Sazt0ePqs8E2ZmVWqgUgsdNgcdERIGbVXUKUKKqKwBUdYWIdIvUwvqSyOO9PL/NcRynyVNZaX3O5gcf+WGPM7CebZ07J8dErVplYdRt22ww+/bt8Pzz9hqGVxtKaz7nQU6lHRsZTAVr6Vzr8WHrj23bTECCTUg48UTz3rVqZTanPlNqKLhYwqOpFIJwO0JVlwfi7HEReTOTk0RkNDAaoM0eXRtlgLcCcRzHceLMxIkm2vr1M7Fz/fXJthilpRY2/cY37H3qZIEWLUwghUUBjfNkKRMpYyDPMZTpvMlX63X2xo3w8ccwalTV9a5d4aST4PLL7VlSQ8HFSOyFm6ouD14/EpEZwKHAShHpHnjbugMfpTlvCjAFoPOAvYvYqeoUHN4WxHGcehI2n12zxhL3p061th/r10OHDjB3rgmjdu3gkENM+ED2erUBnMdNnMttXM6lzGRoRue0bp3MraueY9elixVNrFpl4dJQiM6YkQybDizCj8pY57iJSHsR6RhuA8cBrwEPA6EmHwU8FI2FMcfDpE5dJKI2wHGcbBBWVYajrMImu3feaa/du1uo8amnspfLlsrh/Jcb+Rn/4iQu4/cZndO/vwnIcOZo//5WoABwwgnw8stWAdu/f1XvWjivtFiLE+LucSsBZogImK13q+psEXkemCYi5wLvA8MjtNFxHMdxYoOqFSC0a5dM+n/zTRsp9dpr8K9/Zfd+PfiABzmV9+jF97mL7TSv9fiwZciGDVZ0cMEFcMstFt5dtcoEZiJhxy5ebNWkqd61Ymr9kY5YCzdVfRc4IM36J8Cx+bfIcRzHcXJHavPc2kY3VVYmxcsJJyT7nc2YYYn77dtbaLRrV8t7e/tt+OQTG9aeTVqxmQc5lQ6s51ieZA071XnOF1+YXYsX2/sLLzQRN3Mm7LdfMgyqanl7qYUWTsxDpU2SRNQGOAWBh7kdpygJKyYnTar5mLCCdPJk+zrvPBM4559vExLGjIHLLjOP2157maft4ourzgJt3Rquusry4hrDn/kpg5jPKO7gdfplfF6rVhay7dzZRFv//lZNunathUeHDEk+y7RpTX/+aH1w4dZU8T/8juM4BUcm+VthBWn//iZyDjvMvGpffGFCrlMn865t3GjHLVxohQpnnpnsm9aqleXDtWnTcFt/zBRG81euYCzTObVe5y5fbjaG9z/sMPMabtxo3sEZM5LP4qKtKrEOlcYBbwXiOI7j5ItM8rfKyixvLWzdceedFk4MZ5CGou/WWy1nDODTT+24kHXr7HXTpobZOYi5/IULmMUJ/JY/Nuga7drBlClW8TpkiNnUrl1yOoKIh0jT4R43x3EcxykgSkut+nLyZBM35eUmdsrLzYt28slw6aVw1FHJc1autNdmWfir353lPMipVFLK97i7zmIESB+S7d3bvIZjxlie2513modt4MCqAra8vGrvuWLHPW6OE1fy1c8tgedeOk6BEXrdPvzQWmp88AGMG2eibeFC+yopgT59LPQYNtgNXxtKKzbzAKfRibUcz6N8RpeMzqve2LdrV5ubOmlSsgihf3/YssWE3NixJt6KfUpCOly4OY4Ta0SkFJgK7AZsB6ao6g0isjNwH9ALeA84XVVXB+eMBc4FtgE/U9VHg/WDgduBtsAjwIWqqiLSOrjHwcAnwBmq+l6eHtFx6k3odQuLGBYtgp49Yc89TbQ1a2ZettDTli1u4EIOZy7DmcZr7F/rsZ07W0PgkLANCJg3cPlyy22bPdvEmqrlvU2ebOeOH59sLOwh0yQeKm2KeGGC07TYCvxSVb8KDALKRGRfoBx4UlX7AE8G7wn2nQn0A04AJolIGMuZjI3C6xN8nRCsnwusVtW9geuAK/PxYI7TGIYOtaKEffaxIoUxY8zLBuZZa9s2u/f7EX/lPG5mAhfzQAbtU9u1q/q+Vy+zd8QIs3PePKuIDYsQxo6FXXe15wiFWhgy9QKFJC7c8kkiagMcp/BQ1RWq+lKwvQ54A+gJnALcERx2BzAk2D4FuFdVN6vqUmAJcGgwHq+Tqs5VVcU8bKnnhNd6ADhWpLGNEkBEfi4ii0TkNRG5R0TaiMjOIvK4iLwdvGYWa3KcasyYYZ62tWvhtNPgoousdUY4fWDLFmv70ZjK0ZBBzGUiZTzKcVzKuB3277FHsmIV4OijrSACzPvXv7/1bVu0yJoAjxplaytWJPPcSkvtOhMn2nme25YeD5U6jtMoPmvbiYcPGNSIKzzWVUReSFmYEswa3gER6QX0B+YDJaq6AkzciUi34LCewLyU05YFa1uC7err4TmVwbW2isgaYBdgVUOfSkR6Aj8D9lXVTSIyDfME7ot5CieISDnmKby4ofdxioewOe/QodbeY+VKE2UrVsDo0cmwaKtW9rp1q32F7xvKbqz4shjhLO5JW4zw/vtV369da5WhS5fC//2fedXOP9+mIISTEAYNsu3DDrNzysvh2KC1vue21YwLN8eJM8UxcH6Vqg6o6yAR6QA8CFykqmtrcYil26G1rNd2TmNpAbQVkS1AO2A5MBYYHOy/A6jAhZuTAaGYmTrVxFoqX3xhQmn5cjjiCHjlFcsXC/c1lJZ8wf0MpzNrOIHZrGbnjM57801rNVJSYuLt/PNNvE2davvHjDFbFyyAkSOTz7Z/kDbnuW0148LNcZzYIyItMdF2l6pOD5ZXikj3wNvWHfgoWF8GpGbE7I4JpmXBdvX11HOWiUgLoDPwaWNsVtUPROQabJ7yJuAxVX1MRGryFFZ/5tFYPh4lJSVUVFRkdN/169dnfGyhUyzPGj7n179u4mzLFgs/hhWi4baIJfi3b2/NdsNebY3h2OnXc+Cz/+WfZ/+WUQd+iv0/I3OaN4dt22z7rbfgkEOsye6TT5rdp50GS5aYp23//aFTp+TP9Pjj4Z137Kup0ZjfXRduteDNdx0neoJcs1uBN1T1Tym7HgZGAROC14dS1u8WkT8BPbAihOdUdZuIrBORQViodSTw52rXmgucBjwV5ME1xu4uWO5cb+Az4H4ROTvT84Nw8RSAAQMG6ODBgzM6r6KigkyPLXQK6VkznUGajoqKCvbaa/CXrT5KSiws2r+/9UJ7/nlrB9LYVh/V+SG38Use4ip+zcV//wP8vWHXad0aTj8dOnQwr1tIOObq9NOT35PwWcPwaNgWpKnRmN9dF25NDa8odRpCgjgXzxwBjABeFZGFwdolmGCbJiLnYl6t4QCquijIJ3sdq0gtU9Xg//ycT7IdyKzgC0wY3ikiSzBP25lZsPubwFJV/RhARKYDh1Ozp9BpwjQkZysUe8cea68LF9r6kUfCu+/aQPYXX8wsgT/0xmXKITzHZM7nMb7FWDIzOPSutW4Nmzcn1zdvTjbg7dvXihS6drUwaevWO15n4sSkwAvbgjhJXLg5jhNrVPU/pM9BAzi2hnPGwY6lb6r6ArBfmvXPIYP+BvXjfWCQiLTDQqXHAi8AG0jvKXSaMA3J2QrFXq9eVpAQhj7XrUsm+XfJsCa5PqKtGyuZzjCW06PGYoR0hCHRVNEW8uKLVlE6ZgwMG2Yjri66yFqCTJpUVZyFzYXBc9zS4cLNcRwnB6jqfBF5AHgJ8/wtwEKfHUjjKXSaNnXNIE0XSi0rs6kIGzZYFWbYJuPUYJ57q1awerVtp+a8NYYWbOF+hrMzn3I4z/Ipu9Tr/DZt4PPPrV/bV75i9q9aBQcdBN/9brLtB1jrkkmTdhRn4cQEJz0u3BzHcXKEql4GXFZteTM1eAqd4iVdKLW01MTNnDmWBwYm8P7zH9sOq0VbtLC2H9ngT/yCo/g3Z3E3L3NgxueVlFjD3aVLTbTNmmX2V1YmxVn1XLW6xKyTHhdu+SIRtQFOwVIcLUEcp6ipKZRaWmqjrELRM3EifPSRTUXYtMlyxVY1uNtgVUZxOz/lL1zDL7mXszI+r0ULK5bo2tXet2qVtNfFWfbxyQmO4ziOEzHVRztVViYnB4SD18eMsaHx3bvDz39uXq7UqQjpEv0z5WBe4CbO40m+QTkTMjqne3drPRJ6+9q2NZsSiarHpT6L03jc4+Y4juM4MaKyEoYPh/nzrWHtFVckqyzDys0rrtjxvFat0hcG1MWufMQMhrKSEs7gPrZlKA2qNwEOhdncuXDyycl1n4KQXVy4OY7jOE4eqaun2/jxJtpatzZx9OmnyTYaYeUm7NjioyENd1uwhWmcTldWcQT/5RO61nlOTa1FeveGnXayitFUfApCdvFQaVPCe7g5juPEntADNWlS+v3r19trOGM0nIYwcqQl/h94oFWR7rqrCb/mmXXrSMvV/JrBPMOP+SsLOKjWY8NQbHXRFtoZtik5/3wTafPnW4gUqoaBncbhHjfHcRzHySNhn7I1a8z7Vl3QdOhQ9X2zZlZBetRRMHu2FSdA8rWhnM2dXMQNXMdF3EXdQz169ID33jPhltp+pG1bs2/VKjsm7DG3YIH1afMQaXZx4eY4juM4eaS0FDp2NK9b587mnQpDp8uXW45YGBoFE0qLFsGFF1pPt2xwEC8yhdFUcDS/4ao6j2/WzFp9hKT2jDv8cPjwQ5vkUFZmnsRXX7VxVXPneog027hwcxzHcZw8k5r3FYZOly+H6dNNnJWU2HFt25pwa9HChNuddza+OrMrHzOdYXzMrpzONLbSss5zmjdP3+C3VSvLw1u40IbCDxwIM2ZYUcXcue5pywUu3JyCoz0buZVxnMulbKBd1OY4juM0iDBXLBRxs2ebaOvQAaZMMeHzwQd2zNat1tQ2fN9QmrOVaZxOCSs5gv/yMd0yOm/LlvRFCT17mmjr3j1ZlDB0qE1MOOwwy3GrqQjDaRgu3JyC41he4Aye5C6O5x8cFbU5juM49SK13cczz8D115sguuACuPRS6N8fHnjAjt2wwXq17bmnDZZv7Firq/gNx1DBKG7nJQ7O+LyaRmr1728jrlasMC/bjBmwdq3ltiUSlue2bp2PsMomLtycgmMoFSgwlGdcuGWTRNQGOE5xMHGiibYePUzghMPWu3e3CQSzZ1c9/vDDLYz6+eeNu+/3+Tu/4Dpu5KdMZVTG53XpYm0+why3khITZwMHwltvmWjr398E2qRJVv1aXm7ewVC4pXre6mqH4tSOtwNxCgzlO/wHAb7Lf4A0zYQcx3EiprZpAWVltm/yZBM8PXrYayiARoyw9h+pNFa0HcgC/sqPeYaj+CXX1uvc2283r2DYDuS440ywbdpkRRNgYdGwGrZDB8ttGzcu2Q7kyithQjCQoa52KE7tuMfNKSj2ZSltsMnKbdjMV3mPN+gdsVWO4zhVqWtawJo1cNlllh+2YIF5qVq3hmHD4JprqlaPijTOll1YxQyGsoquDOf+jIoRUjn3XDjxxORUho4dk17D/v2TuWyQrJKF5Biv6lWl3pC3cbhwcwqKk3iW5liiRXO2cxL/deHmOE7sqE2cjB+fHGEVCh9VC5fOn79jAYCqibqdd7a2G+mmFtREc7ZyL2eyGx/ydf6dcTFCKqtWmbgE2Gcfu//hh8OgQeaJ69EjKU7Hjt0x/Dl2bHpB5zQMD5U6BcXpPEHbwOPWli84nScjtihP+FQMxykoqg+NT0e/fiZ+Ro609yUlJoqaN7diAEh62zZvtlBqfUQbwHjG8k2e5Dxu4gUOqde5u+xidpSWWl85gHbtTHR+73vJ/LxQiE6eXHP4s752OzXjHjcnVjxAOadSUeP+zdVc/AewBGVQjcc/yGBOY0K2zHMcx2k0oQdqzRoTO/PmmUdr2DArQujTx/q1QeMEz5ncw6+5hj9zAXfwg3qfv3q1VZJWVsKhh5rI/NGPko2A27c32w86KOlNC19TCxB8yHx2ceHmxIpyxrAnH9CHSjqwYzZua7bU+j5kPW1YzB6U40kUjuPEi9AbV1lpAi6svmzf3kKnr79ec/uNTPkaL3Mr5zKHr/ML/tSga6Te/z//sYpXSIq2yy6De+6xNiUffgg33ZT0MKaKNc9pyy4u3Gpg1pxhUZtQlCxhDwZwOxdxH3/gZlqzhRZk/um1lWZspiW/YzTXcybq2QCO4+SIutpa1LU/VcD17GkCLsx9S0e6Brjp2JlPmMFQPmXnBhUjiMDuu8Mhh5hAmz/fxm917WrewvHjzdN23XUWvg256CJrGgxVxZrntGUXF25O7NhOc/7E93iYI5nGpTV636oTetnO4HKWsEceLHUcp5ipKwRYnxDhmjXw4os172/RwgoU6ppV2pyt3MNZ9OQDjmIOH1FS94NUQ9WGxodjt1oGum/VKnj0UfMKrl8Pr71meXpf+Yp53a6/PnkNF2u5w4WbE1tC79vFTOW3/O3LooR0bKIVVzCKCYxyL5vjOHmhrhBguv3VvXCVldZqI+yHFua+VWfrVvuqi3FcynE8zjncynMMrP9DBagmvX+nnmqi7cgjk+v9+9u+Vq3M8+aNdPOH/4WrgROPmh61CQ7mfVvEXnxRh6v/C1ryGnu5aHMcJ+eEzXVhx8rR1Ma7qV6ncC30wp1yigmiAQOSog2sarOhDGcaF3MVkzmPv3FOwy8EfPSRvbZuDRUVlt/20kswbZrZPXmyFSssWOCNdPONe9yc2DOUCjqysdZjOrLRR2A5jpMX0oVAKytte+5ca6qbui88ft0681j162eCJ+yN1rq1DXHfvh0+/dTWWrTIzMMWsj+v8Dd+yH85nAu5oUHP1a2bVZJuSan5at0aPvnEtsORV488Ysdefz3MnOlFB/nGhZsTc2zEVbOU0VZbacYXtKRVSuFCMzRlBFYj24w7juPUQroQ6MSJydDioEFV22KsXWvvq4cZQzZvtnDpSy9Z6LGysn6irQufMoOhrKEzp/EAW2jVoOcKvWyprF1rRQk772wFFPPm2airyZMtrOt5bPnH40pOrNmXpVVy29bThlfYm1O4ilfYm/W0+XJf22AEVpPkmIbnqjiOk13SNdcdOjQ5Z3TatKptMSZPhk6dYNQoO6akxAa3g42PAnjnHevjlm62aW00Yxt38z1KqeRUHuRDujf6+URMQIKJtuHDrar0s8/gtNPsGceMgSFD0s9jrW1Oq9N4XLg5scZGXG1jK83YQGt+x2gGcDtPMJBD+BuXMZoNtGYrzWgWjMByHMfJNzNmWOizZ09rojtokLXRCAfKjxmTPGb2bAtJ9utn1Ztgwmj9euhdzwl+l/P/OIFHuYC/MI/DsvIsffok7erYEZ55JhnevfNOe8aJE+150g2L9yHyucVDpU6sOZ0naMk2XmbvHdp8VG8b8jXe4XSe5FrOjtBix3GKkdTw6fDhJtouusi8b2vW2PZbb8HgwZYD162bhU7Dwe0AH38M772X+T1P437GMoGbGc1fGV0ve2vqCde3L+y3X7IFSFg40a/fjtMRaqqq9Ya7ucWFmxNrPmQXfs0FtTbTTW3aO5haGiE5juPkiNQK0ksugR//ODl8PbWpbrt2sHGjhR333LPqNZ54IvMRV/vxKrfzA57lMH7GjfW2NxxcHwrHUMi1bAnTg6YKofBatMj2l5dXDQ/X1KvNe7jlFg+VOrHmZK7lOr5XZ5uP0Pt2MtfmybImSCJqAxynsAlzu2bPtkT/6dNhzhzzYpWWWtgxDEF27mxNa8HGW7VsaQUJzZvXfZ+dWM0MhrKWTpzGA3xB6wbZG4q2Zs1MtHXpYmFegDZt4PjjYdYsC/u+9pqHPuNCg4WbiFycTUMcx3EcpxCoKfk+tU3ImDFWiLBokeWvVVZaO5CwWnTTpuR527cnW3C0qCMO1oxt3MX32YP3OY0HWEGPRj9Ps0AJNG9uuXcAn38OiYQ90/XXJ/P0QrwAIToyDpWKyLTUt8CBwJXZNihTROQE4AagOXCLqk6IyhbHcXKLiNwGfAf4SFX3S1n/KXABsBX4l6r+JlgfC5wLbAN+pqqPBusHA7cDbYFHgAtVVUWkNTAVOBj4BDhDVd/Lz9M5hUZNo6yq53aNH28jqhYvNm/bunXJY0PPW/Vcs512Sg5zT8fvuYyTmMV5TOZZjsjK82zdaoJx9GiYMgXatrUq2N69k/3nJk6sek59xnk52aU+OW5rVfVH4RsRqWUUbm4RkebAROBbwDLgeRF5WFVfj8omx3Fyyu3AXzBxBYCIHAOcAnxNVTeLSLdgfV/gTKAf0AN4QkT6quo2YDIwGpiHCbcTgFmYyFutqnuLyJnYf0rPyNOzOQVGTcn31SclTJ5s7TTAvGrpqJ7TVptoG8p0/h/j+Cs/4mZ+0jDj0yBi4m3KFBttBfZsodBMFZwhXoAQHXWGSkUkbJQ1rtquS7NvTsYcCixR1XdV9QvgXuwD3HGcJoiqzgE+rbZ8PjBBVTcHx4TtQ08B7lXVzaq6FFgCHCoi3YFOqjpXVRUTgUNSzlkvIl2AB4BjRcQ7OTtpSdfHrTpDh0L37kkhlDqNoCHsyyKmMpJ5DOQC/kI2G42H4dm2bTM/J5PvgZMbMslxe15ErsVCkl+iqtU/RPNJTyA1sr4sWCtuvEmrU1z0Bb4uIvNF5BkROSRYr+nzoWewXX09PKcl8DxwN/AFsEsObXeaODNmwIoVSTF00EHJFhv1pfWm9cxgKOvpwKk82OBihFTC/5Y0b54UlZ06mQdt5MiqnsCwSbATDzIJlR4AfBu4TkSaYaGGfwX/Y42KdP/VqGKPiIzGQiK02aNrPmxynKLkY3blpkaFbR7rKiIvpCxMUdUpGZzYAugCDAIOAaaJyJ7U/PlQ2+eGAFcDFwHHAScCz4nIvcCtqvpOJk/iOPPnw4UXWjuQ7t1NvAG8/LIJpGbNag6bpkPYzol3j2MPlnIMT7M8Sz6K8C/4tm3JkVYHHWQh3rCFyYgROxYlONGTiXDrDCwCfg98DbgKyzXplTuz6mQZkOqg3R1YnnpA8ME/BaDzgL2jFJmO49TOKlUd0IDzlgHTg/9EPici24Gu1Pz5sCzYrr4eXqtUVZeJyMfYZ+MXmDB8QEQeDwsfnOKlstKS8svK0ocIKyvh5JOtFcirr1q/ttatrRAhrCKtj2gDSJBgrzfmcT6T+C9HNv4hqtG1q4Vz9947ORUh5LXXYNw4D4fGjUyE2yfAXOC/wDpMDK3NpVEZ8DzQR0R6Ax9gicjfi9Ykx3HyzEzgG0CFiPQFWgGrgIeBu0XkT1hxQh/gOVXdJiLrRGQQMB8YCfw5uNbDwNUi0hb7XFwAHK2qW4JIw9uAC7cip6ZKylDQrV2bHNS+caMdlzoZASx0mtoKpDZOYSa/44+8dsgJ3PT8eY22v0UL86yFNnbpArfeCnPn2tzRmTOT3rUFC2yg/KRJXjUaNzIRbgOAnwL7A7cAM1S1nv9nyC6qulVELgAexXLvblPVRVHaVCcJvMGp0zA8dxERuQcYDHQVkWXAZcBtwG0i8hrmHRsVeN8WBe2LXsfahJQFFaVgBQ23Y+1AZgVfALcCZUA74CPgbFXdAqCq20XkOzl/SCfWVFaaMBszJiluUgXb5MnWt23kSHj/faio2LFitHdv2Gcfa9BbF1/hDaYykuc4hGeH/Ryeb3wxwtatJtpCL9vq1TblYVbwryC0t7TURnVNmuRh0jhSZ3GCqr6kqj8Evg/sDcwRkUtyblkdqOojqtpXVfdS1eoVr47jNCFU9SxV7a6qLVV1d1W9VVW/UNWzVXU/VT1IVZ9KOX5c8Nmwj6rOSll/ITh+L1W9IMzVVdXPVXVfVe2lqoeq6rvV7v9G/p7WiQPVG8xOnJgcXTVxYlK0XXmltfBo3968VD16WJg0HR98AE8+Wfe9O7GGmQxhE205lQfZ1rJVdh4Km4Jw4onJ94sWmUBLNxg+0kx2p0bq9LiJSAXQAfufqADbgdOAK3JqmeM4juNERPWwaNi3bM2a5HpZmfU4mzbNGu22aweHHQY332zX6N3bGuouWGDvw6a7tSFs505GsCfvcixPsoxSIDu1MSImLMvK7P1LL1lBwpgxNurqmWfM/vLypBfRG+zGj0xCpT8APgPWRFxJ6jiO4zh5oXqD2bBvWWWlzRkdMyY5fzTs1bZxo42JWr3a1pcvt2Pbtzdhlwm/5Y+czD/4KTfyb45q1DNUn8qgmpyfeuKJyRBpGO6dN8+ecd48ez6vKI0ndQo3H/viOI7jFCPpXBWpAq68HHbZxTxt++9vHrXevc3DFk4bWLgw8/t9l4f5PQluZxR/4YKs2x8WRqxaZRWkb79teXmTJyeFWmqRgleTxpP6jLwqOk48ajqz5gyL2oz6ccxAeHp+1FY4hUYiagMcJ17UNoszbPuxcGFSDL34oiX/b9gAJSW1j65KR1/e4u+czQsczPlMJluTEbp1s4KE3r1NpC1YAJ99Brvuap61sHdbqlAb6PVQscaFm+M4To4QkZ2wavz9sGa/5wBvAfdhvTDfA05X1dXRWOjURG2zOMePT3rSWrWy3mxh24/Fi+21PuHRjqxlJkPYTGuGMZ3PqcfsqTrYdVc47TR49lkLk4Z85zvWGmTkSBdqhUYmI68cx4kKbwVS6NwAzFbVr2BTaN4AyoEnVbUP8GTw3okZmczibN7cihXatdtxX6azSYXtTGUkfXib4dxPJXs0zODq1xWz/aCDLGS6cKF5Ak891cRohw7mcbvoomTlrFMYuHBzHMfJASLSCTgK6xFH0L7kM2yg/R3BYXeQHHTvxJDKymTvtvnzLax4wgk2zmpb0B2wQ4fk8R07Wvh0lwwn3V7KOIbwEL/kWp5hcNbsVrWcuzvvNM9ft24Wvn33XXuGsWMtdDpvHkyYkLXbOnnAQ6WO4zi5YU/gY+BvInIA8CJwIVCiqisAVHWFiHSL0EanDlL7t4XTBAYNshmk7dpZJWk4SL5582RRQiaD2U/iX/yey7iTs7mRn2Xd9oMOsgpX1eS0hAULLDQ6Y4Y9R9iqxCkcpBg6fHQesLce8cJVDTo368UJiexerka8QKFpkK9QaSJ4PVperO/c0Mb8+wKYJafW+56FgIgMAOYBR6jqfBG5ARsX+FNV3SnluNWq2iXN+aOB0QAlJSUH33vvvRndd/369XRIdQE1YbLxrFu2mKjp1g1atqy6ttNO8MknlsO2bZvlsbVube/Xr7fXdu2S++pDl48r+f4N5/PZLj2494I/s7Vl6xqP3X339SxbVvtzprb+aNHCcuzWrzfbOnUygbl1a/K49u0tlFpZaa/t29fP/lxRLL+/dT3nMcccU+PnonvcHMdxcsMyYJmqhv+LegDLZ1spIt0Db1t3bMTWDqjqFGw2NAMGDNDBgwdndNOKigoyPbbQycazlpdb9Wh5ebJ6NFwbNCjpYfv882SeWFgx2ry5iZ733qvfPTuwjvmMYS1tOfSDx3l/7P/Vevw111Twq18NrvWYjh0tfAtWIBF6AyE54iq0vUcPK1SYMWPHZ4+aYvn9bcxzunBzHMfJAar6oYhUisg+qvoWcCw2P/V1YBQwIXh9KEIzi57q1aOVldY4t39/ywMLG9KOGGHHffZZ8txt25KirXVr89Rtr2OSt7CdOxhFXxbzLR7nfWoXbZmybp199e5t7zduNMFWUmLzUadPt/ennmqVpDNmwNChNVfOOvHFhZvjxJV8h0mdXPBT4C4RaQW8C/wQKwqbJiLnAu8DwyO0z6Fqo9qJEy2hH+DRR03A9elj77/4ApYute2+fa0VyKJFdv7mzba2ZUvymHSMZTzDmMHP+RMVHNMou3faqaqQBBOdYGLtG98w8Rmuz5sH3/1u0tPm46wKExdujuM4OUJVFwLp8lSOzbMpTg2km0kaFhioWmFC2Lg2pG1b+M1vrOnu//6XPH7xYjjmmJqF24k8wh/5LX/n+1zPRY22vbpog2SuXbt2yVmjHTvC9dcnJyKAe9oKGRdudVCQ0xPAJyg4juNkQLqZpBMn2nZlpSX4//OfyePD3LExY9IPjX/66fT32Ysl3M33eJkDGM0UsjUZoVmzHcOz4RirmTOtz1x171plZfpxXk5h4H3cHMdxnCZPaj+21IaztTXaLS01L9rqYK5F796WM9a6dVXR1qIOF0h71jOTIWylBUOZwSbSdOzNAEnRes2bmz2haOvXL/k6aJAVITz9NBx8sL0fMsT2V1bC8OEm5iZNapAZTsS4xy3fJPCcIqdufGKC42SV1H5snTvXnNtVWWnHlpXZ+7594Z13rCrzww9tLmkqu+xiIi4Ml+6Icjs/4Ku8wXE8xv/o1eBnaNs2WSm6bZv1kgMTcF98YaLtoIPsOadPN/H23nv2OnWq5batXWuNhAcN8lBpoeLCzXEcx2nypOau1SZYwpy3devg8cfh7betzUY4TB6s31s40uqTT6qen9pPDeBiruQ0HuSXXMNTjUxtDEVbyOef22uqoDz6aGvvsXixibcjjjDx+cEH5mEbOXLHofJOYeHCrSnjeW5OXSSiNsBx8kNq7lptDB0Kzzxjwu3tt20t9GyF4q1Nmx1nkbZoYQ1uU0Xb8czmCi7hHs7kT/wiOw+Shk2bkq0/jj/eBsr/5jcm2EKRevLJ9tqhg1eSFjqe4+Y4juM4ATNmWNuMjh1N+IBt9+sHJ55o79OFRbdurfp+T97hbr7Hq+zPj7iFhhQjtGxpuWzpphp06WLtSMBCv82aWWuS8ePNYzhzZjJ3b+JEax48aFDS4+aD5QsX97g5Ttzw/DbHiYzUKtORI80Dt2KF5Y69+qqJpDVrar9GWIwAMJQZbKRh86RCr96GDVXXw0kIfftaSLRlSxvR1aPHjm0/qj9T9fYnTuHhHrcMOPGo6dm9YCK7l3Mcx3HqT2Xljt6nsMp0+fKkaCspgUceMa9Vszr/aiq3ci778jpncB9L2TOrNrdpk/T8tWxpEx2OOspagFx0kdl82GFV89dSK2fLypI5bk5h4sKtqePeG8dxnLSE3qdJk3YUcRdeaKKtfXuryvzkExNwnTvXfs1fczVnMI2xjOcJvpV1mwcPNvHVvbuFRt9+24oQjj8errvObD7//JrPr639iVMYuHBznDiRT6GdyN+tHCeOhN6nIUOSvc1OP93E2w03WE7Y3XfbfM+uXaFTp6oD5Zs3r3q9b/I44xnLNIZzNb9utH2hd2+nncyjBrDnnnDVVSbQ+va10Gj4DIcdZnZ+7Ws79qtzmg4u3BzHcZyiIdWzFnqfZsyw3mY9elhhwqRJtn300eZlq6iwnLK3304WBID1Ugvpzbvcxxksoh/ncBuNnYzQubM11+3b13q0JRJmd3k5/Oc/dsyaNTBwYPIZpk+HvfeG2bOtl5s32G2aeHFCMeBtQRzHcYCqfdo6djSvW5i8f9hhJoKGDIFLL7Vh8//8Z9VebenGXLVjAzMYiqAMZQYb6NBoO8PK1U8/tf5t48fDtGlm/xVXwO9+l2woDMlnGDLEmu2C57E1VVy4RUUCD1U5juPkmVDghDM8n3nGBNGYMRYunT/fkvxD4bR8uU0mqD44PtloV7mFH7E/r3ISj/AuezXKvlatLEQaNtft2NFCsp9+mhST5eXWUDeV0HsI5oVzmi4eKs2QrFeWOk51vJDEcXJOaal5pebOtd5s8+ZZXtv48Sba2re3tYMOsnDp6tU7ijZINtr9BX/iLO7lUsbxKCc02r4vvoA99jDb+vWze2/bZm0/XnvNK0IdF27Fg4sCJ5VE1AY4TnRceKG19li50nLY5s0zD9qgQdYzrV8/69k2eXKy+rJNmx1bgRzLE1zFb7if05hAecb3b9266vswby5cX7wYvvtdmDXLRFqXLlacMHmyV4Q6RSLcduXjqE1wnNpxYe04OSFdr7YbbjBv2qpVJt66dIEnnrC1MWOsP9rChSaSPg7+fHz+uRULhPRiKfdxBm/wVX7I36hPMcLmzVXff/GFCbPTT4dhw0w4Ll5s9y8vN6E2aBDccYdXijpFItwcx3Gc4iS1V1tIjx5wyikmkMDCoeFQdrCWG126WF7ZAQfseM22bGQ6w2jGdoYws0HFCF27Jre7dLHQ6513Wk7dokVmS1gZ+tFHtl29UjSdKHWaPl6cECUJ8huy8upSx3GKjNRxT/Pnw3nnmYfr9ddtLfS6gQmoigrbBybo+ve36QSPPRYep/yVH3MAL/Nt/sU77F3jvVu33tG7FrLzznZvsPDswoV2r+uvt6rQ9etNzK1ZY33cRo608O2QIclr+Piq4qRoPG7ncXOjr+EFCk5OyHeYNJHf2zlOlKROCghz215/3SYPjBwJDz1kgqlvXxNqr79uuWzdu5tn7vXXrW9aKO4u4nq+z938Vi5nNifWeN927ar2eatO9+6wyy62HbYYOewwqwidONHCoj16mJfts8/s+AULbA5piI+vKk6KRrg5jlO4iMhtIvKRiLyWsna1iLwpIq+IyAwR2Sll31gRWSIib4nI8SnrB4vIq8G+G0VEgvXWInJfsD5fRHrl8/mc/HDDDbDffhamXLHCRFCPHnDccbYesn277V+50jxmYVXpMTzF1fyaBxnGFTq21ntt3Fi7LQsXWn+4Nm3sfUlJMuwZvpaVmSjbts1mkFYXaTWNr/IQatPGhVvUJPJ8P0+CdwqT22GHXguPA/up6teAxcBYABHZFzgT6BecM0lEwuFEk4HRQJ/gK7zmucBqVd0buA64MmdP4kTGwIHw7W9biHLQIJsy8NWvWrjxv/+t/dw9+B/3cQZvsQ8/4HbqKkbYZx/Ybbea94cVqnvsYR6/hx4yAZaak1daan3cPv7YRGamFaXp8vqcpoMLN8eJEhfSGaGqc4BPq609pqpbg7fzgN2D7VOAe1V1s6ouBZYAh4pId6CTqs5VVQWmAkNSzrkj2H4AODb0xjlNizC8OG0a/Pa3ll/WooV51/r1s3y23r3t2DDU2YZNzGAoLdnCEGayno41Xj/8rdm4MdnEN6RFkFXetStcfbV52RYvToZIKyth7VrzqoWetbIyE4D1CYd6CLVp48KtGHGxULwkojYgZ5wDzAq2ewKpQaJlwVrPYLv6epVzAjG4Btglh/Y6DaQhYcB080lLS+GmmyxU+s1v2nGtWpnoads29WxlCqM5kIWMan4XS+hT673CxrwrV1phAZhg69IFtm610OhRR8GLLybz5j780F4nTrSctk6dkp610lLo2bN+vdtqCqE6TYOiqio9j5u5iZ806honHjWdWXOGZckip6hpIgJ67fqdGvtvoquIvJDyfoqqTsn0ZBG5FNgK3BUupTlMa1mv7RwnZqRWUh5/fPpjKivtuLKyquHH1PmkAM8+a203Jk6Ebt0s+X/kSPOChfyMGxnB3/ktf+Dhbd9OGXWVpF27HXPaUo/ZutUKH8D6wU2fbp69tm1h06bk/VIrYB2nJopKuMWWBNHkunlrECcerFLVAQ05UURGAd8Bjg3Cn2CetFRfw+7A8mB99zTrqecsE5EWQGeqhWadeJAqbt55J/0xqeJuzJhk+HHdOsv7CgXclVda640VK+y87t2TXjCAb7ao4Nqtv2QmpzCOSwHzmG3aZPvbtrWK0OpB9TZtkrNGU+nc2dqAtGtnIdrwOn372mvqvFHHqQkPlTpOFDQRb1uUiMgJwMXAyaqa6u94GDgzqBTtjRUhPKeqK4B1IjIoyF8bCTyUcs6oYPs04KkUIejEiEzCgGE15po1dmwYfgxZv96O6d/fRNuee5oIW7ECtmwJ7sP73L3tdN6mDyOZigZ/LlukuDs2bbI8uFatTIyB5a917548JhxnBWZPSYk12J0zJ7leWxGD41THhVsx4+KhuEhEbUDDEZF7gLnAPiKyTETOBf4CdAQeF5GFInITgKouAqYBrwOzgTJVDTtqnQ/cghUsvEMyL+5WYBcRWQL8AuoxeNKJhMpK+OCD9LluYTXm5Mnm2erf344N6dDBwqJvvWXv165Ner82brRihOkMo7V+ziVfncl1t3T6sm1HqnDr3t0E29ixlocGVrG6dKl50caMgfvvt/sPG2bvx46188Lmu/37W/6d42SKh0obQE7y3BIU9B9Wpx64YK43qnpWmuVbazl+HDAuzfoLwH5p1j8HhjfGRid/VFbC8OH2NWlS+vBiWZmFRJ991nqmLVhgwimsthw40ERau3bJKQa9e8OnnyjXrz2fAbzIyTzEW9v2YfGfLPRZUmKh1BYtLG9t+3a7xjXX2FiqVLZssXuvW5ds9QF2/xUrTLAddpjl1KXm46U+Y7juOKkUncctGxMUmhQuIhzHKTAmTrTxVe3bV03kr1492rFjcpTUmDEmksIAeFhRetRRVhwQziY9e+1EfsAdJLiMf3AyixcnR2A1b275bFu3mtdu5UoTc6mirXVre/3f/+zed95p4nL+fLPj7bfNlsmTzb477rBcu9NPr+o99F5sTk24x81x8klUQjkRzW0dJxeEBQp77VXVSxWKnWeesT5tqYUMpaUm6lKrS6dPh3POsXNXr4avM4fr+DkP813+wO++vG44c3T58uS9jjzSvHXPPJNc69cP3njDtrdvt9fOnS1Me/LJJvAWLjQ7ZswwW0aOtNDpvHlVvYeZFGE4xYkLN8crTB3HKSjCAoWKiqrrZWUmpObNMw/W9ddXbcsRiqE1a3asKO3JMu5nOO+yJyO4k7btmrFxo4VFu3eH996req/Zs5PtPFq3tikM++xjhQeplJSY1w3Mq9ezpw2K79EjacuKFTbJId04K3Dh5lSl6EKl2SInA+cT2b+kEyM8LO04OaW01DxtgwaZeLvooqrhxtJSE0fr11vfthUrTFjtvfvnTGcY7djIEGbyRZvOnHNOMpftvfeqNuXt0sWqR8OChs2bTbCFM03DY8BmoPbvb9u9esFrr9n4qtAWsNdp0xreMNdnkxYX7nFzDPe6NV0SURvgOPmlf3846CD7+t//rAggTPZfuzbpAeveHVasUG7eeQyH8jw/K53Bm5Vfhc/htttMtIVs2mSh0ZKSpEDr0iXZWLd/f8tbmzrV3q9bZ/fZbTfz/E2aZJ62mTOTgi2clFBe3rgpB6l967wPXNMnlsJNRBLAj4GPg6VLVPWRYN9YbCD0NuBnqvpofa+fjQkKjlMv3NvmOHkhFEMjRsCFF1o7kETCQqYLF1pOWeh1U4WT/jeZU+b8jQktf4ueMoQuwfyNr33Nwq6tWlmT3TZtdpw/uuuuJty6drV7DhxoIdCJE+0+PXsm8+tCQTUw5aMgW5MSfOJCcRFL4RZwnapek7ogIvsCZwL9gB7AEyLSN6VHU+GTIDoPiXvdcoOLNsfJOaFHbehQEzGzZ5to69DB2nxMD7JbVO3rlVeg/cL/MLzZhcxufhKXbEnAxGRO3B57JEOuAMccAy+9lGyo26+f5bQtXmytRC66yMKd1cdr1Ua2JiX4xIXiotBy3E4B7lXVzaq6FGuieWhUxuQkzy1qXGQ0LRJRG+A4+SEUTOefbwn/v/+9Ca8nnrDwJpjYevVV8459tPADHmp5Gp907MWZ2+6iRctmqCYnIHTsaEJszBj76tXL2n+EeWRf+Qq8+65tl5Qkq0LLyszb98AD3s7DyQ1x9rhdICIjgReAX6rqaqAnMC/lmGXB2g6IyGhgNMCue7TJsalZJoH/wW0quBB2nJxTWWm5a/37W6PdBQusDcfcubZ/5cpkQcDs2bBfn83c/cGpdNy2gbO6P8Wxx+7Exo123rhxsGRJMsQ5caJdf/x42GUX+OQTu+Yzz5inrUcPE4Jz5ybPWbzYWn/06OHhSyf7ROZxE5EnROS1NF+nAJOBvYADgRXAteFpaS6Vdp6gqk5R1QGqOqDTrq122O+NeGvBxYbjOAVEmNd22GHm7erf3woBwmrLyy6zCtKXXgJQLn6/jP03zufHLe9g+pv78swzJuhWroTf/S4pwELCeaedO/Pl6KtQtC1fnhRtoci74Qbz9k2f3riiA8dJR2QeN1X9ZibHichfgX8Gb5cBqf8MdgeW73CS48SBqAVwItrbO04uSZ1VmpqcP3GiVXNOnZocdzVsmO3v3RsOen4KZ1feyuVcyr93HQbrk560tm1NiE2YkBRhEydaIQNUDY0ef7zdL6wSrV7ZGXr7HCfbxDLHTUS6p7wdCrwWbD8MnCkirUWkN9AHeC7f9qWSszy3RG4umzFRi45Cx79/jpNTJk6EDz+0HLIwOb+01ERcebntW7jQju3QwTxgH07/L79Z9lPmdDiRy/g9Rx5px06daq+nnbbjPa680vLdwl5s4bzSHj2sQrT6fRsSGvU+bE59iGuO21UiciAWBn0PrHeHqi4SkWnA68BWoKxJVZTGDa8yLVwSURvgOLmlrAzmzLEJCamEIi4UWmDCrdP65TzAabyn/8fd376bQ//XnLKyZPuOoUNtbmg4iD68R2qbjQkTktWi1QVaYyo7vQ+bUx9iKdxUdUQt+8YB47JxH+/nlgEu3uqPe9scJ+eUllqftJpyyG66ySpM99sPyn++mZ2GnUbzZuu48pjH+c9rO7FoEVx9tYVTV6xIjsrq3t16sIEJqrKy5D06djQPXxgaTd3XGLwPm1MfYhkqLTSabLjUqT8u2hwnb2zZkgwxVlYmW3dUVpon7bjjrEq09Oqf0fHVuYza/jfmfLrfl/NE//1vE21du5onrWtXe3/RRUkvWGo7jzAcqprdVh+poV7HqYtYetycmOFet8yIi2hLRG2Ak4qINMfaGn2gqt8RkZ2B+4BeWCrI6UG7I6eefPRRMsSoapWfYO9ffBHmz4dDX55C6ewprB1Tzt6dhtP2A2v70b+/TVRIJOD9923WaJculsP2ox/Z+aEQDAkFVmWlVZiGIjGb3jfHqQv3uMWdRNQGBMRFlMQV//44NXMh8EbK+3LgSVXtAzwZvHfqSWUlbNuWFFdDh5oYGzHCvGfz58OovnMZ8sQFvNX7eNb8+nLGjLF8txEjrFihf3/zyn3yia2vXm2FB7fcYiKwU6f0YizVQ5bOM+c4uaToPW7ZynM78ajpzJozLAsWxRj3vMWfRNQGOKmIyO7At7G83F8Ey6cAg4PtO4AK4OJ821YohM1vAcaOTQqpiRNtVmgoriZONE9a69bQpw/sxgquX3YqqzuUctjSu/nJzc2/9MqFo6w6d07mlw0ZkhwQP3Jk1WHwteH5aU6+KXrhVhAkiM8fZBdvO+LeNqdmrgd+A6ROrSxR1RUAqrpCRLpFYVihEDbXBRNaoYirXlVaVpYsMDjkgC+Yu/twOn2yho/um81ZD+3MmjUwalRSpIXCrKYB8AMz/Gftc0KdfOPCzak/Lt6SxEm0JaI2wElFRL4DfKSqL4rI4Aac/+XYvpKSEioqKjI6b/369RkfWwgceyzsu69td+8OqY/WqdN65sypoFs3aNkS/vAHy3s76t7rKF32Xxb97nd83OpTDj+8gg8/tFFWxx8PmzbZ6zvv2Fc6tmyxa4XXrmktHzS1n2ltFMuzNuY5XbhlkaIIl4a4eIuXaHPiyBHAySJyEtAG6CQifwdWikj3wNvWHfgo3cmqOgWYAjBgwAAdPHhwRjetqKgg02MLnbvuquDsswdTXp7i9br1Vvjnw/Cb39Dv978HLNw6aRIceSTMmJEsJKitsKC83HLXUq8dro0ZY61B8lWQUEw/02J51sY8pxcnUCBzSxNRG5CGYhYucXv2RNQGONVR1bGquruq9gLOBJ5S1bOxCTCjgsNGAQ9FZGLBkTphoHpxQmUlTBw5Hx0zBr71Lbjiii/PC8OZM2ZULSSorbAg3SSEXLUDcZz64B43p3EUo+ctbqLNKTQmANNE5FzgfWB4xPYUDKkTBlSrFidcfsGH/ODOU1neqgfjutwDP21epZgBdiwkqK2wIF3uWrp2II6Tb1y4ZZmchksTxNOzUkzizUWb0wBUtQKrHkVVPwGOjdKeQqW60PqyOOGLL/j1c8PRZp8y8Iu5vDJtF6BqMQPsKMYaWljgBQlOlHioNKAgwqVxphgETVyfMRG1AY6TH1L7p1UZefWLX9D6+f+w4YZbOXLMAYwYsWPzXPBh7k7TwD1uhUaC+P6hbqqet7gKNsdx4G9/sxjqL37BLhecxcRaDs3WMHefluBEiXvcckDOZpcWAk1N5MT9eRJRG+A40dHxzTdtkvyxx5oiq4N0BQcNwaclOFHiwi2FggmXJqI2oA6OGRh/wZMJTeEZHKepsnIl/X77O1a33o0Prr0XWtQdQMrWMPdsCUDHaQgu3HJEUXvdQgpV+BSK8ExEbYDjRMSWLXD66bRYu5Zj1s7kL/d2BfKXw5YtAeg4DcGFW6GSiNqADCkUERRSKLYmojbAcXJLrSLsV7+COXN44+e/5MTyA7/0fHkI0ykGXLhVo2DCpVBYf7zjLogKTWA6ThOnRhE2dSrceCP84hd8eOy3UE3uqk8I0ytMnULFq0pzSFGNwMqEUBjFqfK0EMVaImoDHCf3pG2O++KLMHo0fOMbcOWVfHTff6pUidanv1q2KkwdJ9+4x63QSURtQAOIg3crDjY4GSMiPxeRRSLymojcIyJtRGRnEXlcRN4OXrukHD9WRJaIyFsicnzK+sEi8mqw70YRkWieyKmLHfLIPv4Yhg6FkhK414oRunVreJGAFxg4hYoLtzRkM1zqRQq1EIqnfAmofN8vFySiNiD/iEhP4GfAAFXdD2iOzf4sB55U1T7Ak8F7RGTfYH8/4ARgkog0Dy43GRgN9Am+TsjjozgNJShG4OOPYfp0m3UFtGzZ8CIBLzBwChUPlTYFEhT+H/RUMZXNUGohizQnlRZAWxHZArQDlgNjgcHB/juwkVIXA6cA96rqZmCpiCwBDhWR94BOqjoXQESmAkOAWXl7Cqdh/OY3UFFh+W0HHxy1NY4TKS7cnPiRTmxlIuaaukhLRG1ADSynsbZ1FZEXUt5PUdUp4RtV/UBErsEGsm8CHlPVx0SkRFVXBMesEJFuwSk9gXkp11sWrG0JtquvO3Hm73+H66+Hn/0MRoyI2hrHiRwXbjVwHjdzEz/JyrXyUqSQIL5/2LNBUxdldZGI2oCcskpVB9S0M8hdOwXoDXwG3C8iZ9dyvXR5a1rLuhNXXnoJfvxjOPpouOaaBl3Cx1M5TQ3PcWtKJKI2wClkYpyP+U1gqap+rKpbgOnA4cBKEekOELx+FBy/DEj9E7075hdcFmxXX3diRNim44OXV1kxQteuMG2aJbQ1AO/t5jQ1XLjliRj/UXTiTiJqAyLnfWCQiLQLqkCPBd4AHgZGBceMAh4Kth8GzhSR1iLSGytCeC4Iq64TkUHBdUamnOPEhIkT4Zort7LplDNg5UqYMQO6dav7xBrw6lGnqeHCrRYKqhlvSCJqA5ysksjPbeL8HwtVnQ88ALwEvIp9bk0BJgDfEpG3gW8F71HVRcA04HVgNlCmqtuCy50P3AIsAd7BCxNiR1kZPHVoOXv/7ym46SYYUGMUPSO8etRparhwyyN5++OYyM9tHCdfqOplqvoVVd1PVUeo6mZV/URVj1XVPsHrpynHj1PVvVR1H1WdlbL+QnCNvVT1AlX1HLeYUfrvuznquWtNwf3gB1X2+bQDx3Hh5jjxJZGf28TZ2+YUGQsXwo9+BF//Olx33Q67PV/NcVy41Um2w6XudXMyIhG1AY6TZz75xIoRdt4Z7r8/bTGC56s5jgu3pk0iagOcuOPeNicWbN0KZ54Jy5fbZISSkrSHeb6a47hwy4iC9bqBi7dCJBG1AY6TZy65BJ54AiZPhkMPjdoax4k1LtwcJ04k8ncr97Y5seDee+Hqq5nbfwyV3zonamscJ/a4cIsI97o5O5CI2gDHyTOvvALnnMPS3Y/k6AXXedGB42RAUQi3nTatbfQ1CrKnWyqJqA1w4oR725zI+fRTGDIEunSh1cz7+WV5Ky86cJwMKArhFlfy/sczkd/bOfUgEbUBjpNHtm2Ds86CDz6ABx+k58G7edGB42RI0Qi3k19+rNHXKHivG7hAiCOJ/N7OvW1O5Fx6KTz2mDVmGzQoamscp6AoGuEWV/yPaJGTiNoAx8kz999vXXTPO8+a7TqOUy+KSri51y0gEbUBDhDJz8H/o+BEyquv2hirww6DG26I2hrHKUiKSrjFlUj+mCbyf0snhUT+b+mizYmU1attMkLnzvDgg9CqVdQWOU5B4sKtAeTC6+birYhIRG2A4+SZbdvge9+D99+HBx6A7t2jtshxCpaiE27ZCJc2KRJRG1BkJKK5rXvbnEj53e9g9mz485/h8MOjtsZxCpqiE25xJrI/roloblt0JKI2wHEi4MEH4YorrBDhJz+J2hrHKXhcuDWQJlGkkEoiagOaOInobu3eNicyFi2CUaNg4ED4y1+itsZxmgRFKdziHC6N9I9sIrpbN2kS0d3aRZsTGZ99ZpMROnY0r1vr1lFb5DhNgqIUbtkiV143F29NiETUBjhOBGzfDt//Prz3nvVt69kzaoscp8kQqXATkeEiskhEtovIgGr7xorIEhF5S0SOT1k/WEReDfbdKCLSkHvH2esWOYmoDWgiJKK9vXvbnMhIJOCRR+DGG+HII6O2xnGaFFF73F4DhgFzUhdFZF/gTKAfcAIwSUSaB7snA6OBPsHXCXmzNg1N0usGJjoS0ZpQ0CSiNsBxImLmTPjjH+Gcc2w6guM4WSVS4aaqb6jqW2l2nQLcq6qbVXUpsAQ4VES6A51Uda6qKjAVGNLQ+7vXLQMSURtQgCSiNiAGwt8pTt54A0aOhEMPtTmkDQuIOI5TC1F73GqiJ1CZ8n5ZsNYz2K6+HilN1usWkojagAIhQSy+V7H5vXGKizVrrBihbVsrRmjTJmqLHKdJknPhJiJPiMhrab5Oqe20NGtay3q6+44WkRdE5IWPVzfE8ngQmz/CiagNiDmJqA1wnAjZvh3OPhvefdcmI+y+e9QWOU6TJefCTVW/qar7pfl6qJbTlgGlKe93B5YH67unWU933ymqOkBVB+zapeYbZStcmsu+brESb4mIbYgjiagNSBKb3xWnuPj97+Gf/4Trr4evfz1qaxynSRPXUOnDwJki0lpEemNFCM+p6gpgnYgMCqpJRwK1CUAnFySiNiAmJIjV98JFmxMJDz0Ef/gD/PCHMGZM1NY4TpMn6nYgQ0VkGXAY8C8ReRRAVRcB04DXgdlAmapuC047H7gFK1h4B5jVWDvc69YAEsRKtOSdRNQGVCV2vx9OcfDmmzBiBAwYAJMmeTGC4+SBqKtKZ6jq7qraWlVLVPX4lH3jVHUvVd1HVWelrL8QhFr3UtULgurS2FBU4g1iJ2ByToLie2anQYhIqYg8LSJvBP0qLwzWdxaRx0Xk7eC1lmSOGLN2rRUjtGkD06d7MYLj5Im4hkrzjrcGaQQJmr6YSRDbZ4yloHcAtgK/VNWvAoOAsqBHZTnwpKr2AZ4M3hcW27db248lS2wyQmlp3ec4jpMVXLjlgKLzuoUkiK24aRSJqA2omVj/PhQ5qrpCVV8KttcBb2Dti04B7ggOu4NG9KKMjMsvt9y2P/0Jjj46amscp6hoEbUBceLklx/j4QOOi9qMOjnxqOnMmjMsajNqJlHttVBJRG1A7bhoKxxEpBfQH5gPlASFVqjqChHpVsM5o7EpMZSUlFBRUZHRvdavX5/xsQ1hl7lz2f+yy/jwuON4c//9IYf3qotcP2tcKJbnhOJ51sY8pwu3HHEeN3MTP8nZ9WMv3qAwBVwiagOcpoaIdAAeBC5S1bWZjldW1SnAFIABAwbo4MGDMzqvoqKCTI+tN2+9BRMmwEEHsdvMmezWtm1u7pMhOX3WGFEszwnF86yNeU4PlVajkHLdCsbjkiD+YdQE8bavGgXzsy9yRKQlJtruUtXwh7YyGN9H8PpRVPbVi7VrYehQaNXKihEiFm2OU6y4cMshucx1K1gSxEcgJYiXPRlSrKJNRJqLyAIR+WfwvsbqTBEZKyJLROQtETk+Zf1gEXk12HejZOr+api9AtwKvKGqf0rZ9TAwKtgeRSH0oty+HUaNgsWLYdo0+L//i9oixylaXLilIZtet1yLt4L9I54gGuEUxT2zSMH+vLPDhViCf0ja6sygcvNMoB9wAjBJRJoH50zG8sb6BF8n5NDeI4ARwDdEZGHwdRIwAfiWiLwNfCt4H2+uuAJmzoRrroFjjonaGscpajzHrQlQEPludZHIcK2x1yxgilm0icjuwLeBccAvguVTgMHB9h1ABXBxsH6vqm4GlorIEuBQEXkP6KSqc4NrTsUqOhvdxDsdqvof0s9XBjg2F/fMCf/6F/zud/D978OFF0ZtjeMUPS7caiCbFaa5LlSAJiLeqpOI2oD4EGvRtm4DPD2/MVfoKiIvpLyfEiTmp3I98BugY8paTdWZPYF5KcctC9a2BNvV152aePttE2wHHABTpvhkBMeJAR4qbULE+o+7E3sizMlcpaoDUr6qiDYR+Q7wkaq+mOH10qkLrWXdSce6dTYZoUULmDED2rWL2iLHcXDhViuFlOsW4uKt6ZGPn2nMC2mOAE4OQp33Yjljf6fm6sxlQGor/92B5cH67mnWneqo2tD4N9+E++6DXr2itshxnAAXbnnExZtTX/xnCao6Nphp3AsrOnhKVc+m5urMh4EzRaS1iPTGihCeC8Kq60RkUFDxOZJCqOiMgiuvhAcfhKuugmMLJx3PcYoBF251UEh93VLxP/iFT75+hjH3ttVG2upMVV0ETANeB2YDZaq6LTjnfOAWYAnwDjkqTChoZs+GSy6Bs86CX/yi7uMdx8krLtzyTD7/SLp4K1xctKVHVStU9TvB9ieqeqyq9gleP005bpyq7qWq+6jqrJT1F1R1v2DfBarqOW6pLFligu1rX4NbbvFiBMeJIS7cMiDbXjcXb05t+M/MiYT1620yQrNmXozgODHGhVsR4EKgcMjnz6rQvG1ODlGFc86B11+He++F3r2jtshxnBooDuH2YeMvUcheN3DxVgi4aHMi46qr4P77Yfx4+Na3orbGcZxaKA7hFlNcvDkh/rNxIuPRR2HsWDjjDPj1r6O2xnGcOige4XZl4y+RiwpTF2/FzYlHTc/7z8S9bc6XvPOOFSPstx/ceqsXIzhOAVA8wi1LFGp7kFRcvMWDKH4OLtqcL9mwwYoRwIoR2reP1h7HcTKiuIRbFrxuuSCKP6Yu3qLFRZsTKapw7rmwaBHccw/stVfUFjmOkyHFJdyyRFMImUI0YTrHRbMTA6691kZZXXEFHH981NY4jlMPik+4xdTrBtF5RFxI5I+ovtfubXO+5Ikn4OKL4bTT4De/idoax3HqSfEJN4htoUKUuHjLLVF6N120OV+ydKlVj371q/C3v3kxguMUIMUp3LJEUwmZhrh4yw1Rfl9dtDlfsnGjFSNs3w4zZ0KHDlFb5DhOAyhe4eYh07R43lt2cdHmxAJV+PGP4ZVX4O67Ye+9o7bIcZwGUrzCLUvkKmQa9R9dF2+NwwWwEyuuv94E2+WXw4knRm2N4ziNoLiFW4y9bhAP8ebio37E5XsW9e+OEyOeesomIgwbZhMSHMcpaIpbuGWJplaoUJ24iJG4E5fvkYs250v+9z84/XTYZx+4/XYvRnCcJoALtyx53ZpqyDSVuAiTuBEnYRun3xcnYjZtsmKErVutGKFjx6gtchwnC7SI2gCnbs7jZm7iJ1GbASTF26w5wyK2JHriItZCXLQ5X6IKo0fDwoXwj39Anz5RW+Q4TpZwjxvE3usG8fujHCcvU76J47PH7ffDiZgbb4S//x3+8Af49rejtsZxnCziwi3LFJN4g3iKmFwR12eN4++FEx07LVwIv/wlDBkCl1wStTmO42QZF24hWawwLTbxBvEVNdkgzs8W198HJyLef599EwkLjd5xBzTzj3jHaWp4jlsqVwIXR21E3cQp5606qQKn0PPg4irWQly0OTswbx6iasUInTpFbY3jODnAhVuOOPnlx3j4gONydv04i7eQQhRxcRdrIS7anLScfjrz2rfn6/vsE7UljuPkCBdu1cmi183FW5I4i7hCEWshLtqc2tjWvn3UJjiOk0NcuBU4hSTeQqoLpXwLuUITaqm4aHMcxyluXLilo4C8blCY4i2VdEIqG2KukAVaOly0OY7jOC7casLFW6Q0NdHVWFy0OY7jOODtQPJGPuaZ+h/3pkm+fq5Nfeau4zhOU8CFW21ksbdbvnDx1rRw0eY4juOk4sKtLgqkMW8qLt6aBi7aHMdxnOq4cMsz+RRvLuAKFxdtjuM4TjpcuGVClkOm+fxj6eKt8PCfmeM4jlMTkQo3ERkuIotEZLuIDEhZ7yUim0RkYfB1U8q+g0XkVRFZIiI3iojkxVgXb06OybeXtJC8bSJygoi8Ffy7L4/aHsdxnKiI2uP2GjAMmJNm3zuqemDwdV7K+mRgNNAn+Dohkxv9957Gmpp9XLw5Ifn++RSYaGsOTAROBPYFzhKRfaO1ynEcJxoiFW6q+oaqvpXp8SLSHeikqnNVVYGpwJBc2bcDBVhlmornvcUTF211ciiwRFXfVdUvgHuBUyK2yXEcJxKi9rjVRm8RWSAiz4jI14O1nsCylGOWBWt1XOng7FlVwCHTEBdv8SAKIV2Aog3s33hlyvvM/t07juM0QXI+OUFEngB2S7PrUlV9qIbTVgB7qOonInIwMFNE+gHp8tm0hvuOxkKqAIuOBLiHz+tlfE00POzaFVi143Le/5h2hcfS2JF3avh+5J1I7JgVEzuqsU/9T3nzURjUtRH3bCMiL6S8n6KqU1LeZ/zvvqny4osvrhKR/2V4eBx+j/JFsTxrsTwnFM+z1vWc/1fTjpwLN1X9ZgPO2QxsDrZfFJF3gL7Y/7R3Tzl0d2B5DdeYAnz54S8iL6jqgHTH5os42OB2uB112VDfc1Q1ozzTRrAMKE15X+O/+6aKqu6a6bFx+D3KF8XyrMXynFA8z9qY54xlqFREdg0SkhGRPbEihHdVdQWwTkQGBdWkI4GavHaO4zQNngf6iEhvEWkFnAk8HLFNjuM4kRB1O5ChIrIMOAz4l4g8Guw6CnhFRF4GHgDOU9VPg33nA7cAS4B3SBtxchynqaCqW4ELgEeBN4BpqrooWqscx3GiIeeh0tpQ1RnAjDTrDwIP1nDOC8B+DbjdlLoPyTlxsAHcjuq4HUniYMMOqOojwCNR21EgxPJnmCOK5VmL5TmheJ61wc8p1lXDcRzHcRzHiTuxzHFzHMdxHMdxdqTJCbeaxmgF+8YGI3PeEpHjU9ZzOkZLRBIi8kHKCK+T6rIpV0Q1OkhE3gu+xwvDykUR2VlEHheRt4PXLjm4720i8pGIvJayVuN9c/XzqMGOvP9eiEipiDwtIm8E/04uDNbz/j1xGke636lq+78vIq8EX8+KyAH5tjEb1PWcKccdIiLbROS0fNmWTTJ5ThEZHHxWLBKRZ/JpXzbJ4He3s4j8Q0ReDp71h/m2MRvU9Hlb7RgJdMeS4N/qQXVeWFWb1BfwVawXVQUwIGV9X+BloDXQGytsaB7sew4rkBCs2OHELNuUAH6VZr1Gm3L0vWke3GNPoFVw733z9HN5D+habe0qoDzYLgeuzMF9jwIOAl6r6765/HnUYEfefy+A7sBBwXZHYHFwv7x/T/wr+79T1fYfDnQJtk8E5kdtcy6eMzimOfAUlgd5WtQ25+jnuRPwOtbjFKBb1Dbn8FkvSfkM2hX4FGgVtd0NeM60n7fVjjkJ0x0CDMrk32mT87hpzWO0TgHuVdXNqroUq0o9VKIdo5XWphzeL26jg04B7gi27yAH33dVnYP9o8/kvjn7edRgR03k0o4VqvpSsL0Oq9LsSQTfE6dx1PU7parPqurq4O08qvbALBgy/LfzU6yg7aPcW5QbMnjO7wHTVfX94Pim/KwKdAyiXx2CY7fmw7ZsUsvnbSqnAFPVmAfsFOiSGmlywq0Wahqb07AxWvXngsANeltKGCrfo3yiHB2kwGMi8qLYVAuAErXefASv3fJkS033jeL7E9nvhYj0AvoD84nX98TJPufSRFsniUhPYChwU9S25Ji+QBcRqQg+R0dGbVAO+QsWPVsOvApcqKrbozWpcVT7vE2l3p+xBSncROQJEXktzVdt3qOaxuZkZZxOHTZNBvYCDsTGeV1bh025IsrRQUeo6kFYyKZMRI7K033rQ76/P5H9XohIB8xDcZGqrq3t0Fzb4uQWETkGE24XR21LjrgeuFhVt0VtSI5pARwMfBs4HvitiPSN1qSccTywEOiBfT7+RUQ6RWlQY6jj87ben7GR9nFrKNqAMVrUPDYn4zFa2bBJRP4K/LMOm3JFZKODVHV58PqRiMzAwm0rRaS7qq4IXMP5cv3XdN+8fn9UdWW4nc/fCxFpiX2I3KWq04PlWHxPnOwiIl/DGpafqKqfRG1PjhgA3GtRNboCJ4nIVlWdGalV2WcZsEpVNwAbRGQOcACWN9XU+CEwIUhfWiIiS4GvYPnoBUUNn7ep1PsztiA9bg3kYeBMEWktIr2xMVrPaR7GaFWLVw8FwkqatDZl897ViGR0kIi0F5GO4TZwHPY9eBgYFRw2ivyNL6vpvnn9eUTxexH8jt8KvKGqf0rZFYvviZM9RGQPYDowQlWb4h93AFS1t6r2UtVe2KSdMU1QtIH9m/y6iLQQkXbAQCxnqinyPnAsgIiUYAWH70ZqUQOo5fM2lYeBkUF16SBgTZi2UhMF6XGrDREZCvwZq0T5l4gsVNXjVXWRiEzDqnK2AmUprvXzgduBtlgeSLZzQa4SkQMx9+d7wE8A6rAp66jqVhEJRwc1B27T/IwOKgFmBP8jbgHcraqzReR5YJqInIv9Qx2e7RuLyD3AYKCr2Hi1y4AJ6e6by59HDXYMjuD34ghgBPCqiCwM1i4hgu+J0zhq+J1qCaCqNwG/A3YBJgX/9rZqAQ7vzuA5mwR1PaeqviEis4FXgO3ALapaa4uUuJLBz/SPwO0i8ioWSrxYVVdFZG5jqOnzdg/48lkfwSpLlwAbMW9jrfjkBMdxHMdxnAKhmEKljuM4juM4BY0LN8dxHMdxnALBhZvjOI7jOE6B4MLNcRzHcRynQHDh5jiO4ziOUyC4cHMcx3EcxykQXLg5juM4juMUCC7cnJwTTGp4Jtg+SERURHYRkebBPNd2UdvoOI4TF0TkEBF5RUTaBJNnFonIflHb5cSDJjc5wYklnwEdg+2fAvOALlhX6cdVdWNEdjmO48QOVX1eRB4GLscm+vy9UKckONnHhZuTD9YA7URkF6A78F9MuI0GfhHML50EfAFUqOpdkVnqOI4TD/6AzZf+HPhZxLY4McJDpU7OUdXtweaPsYG764CvAc2D4dfDgAdU9cfAydFY6TiOEyt2Bjpg0Yo2EdvixAgXbk6+2I6JshnAWuBXQDggenegMtj2AeaO4zgwBfgtcBdwZcS2ODHChZuTL74AZqnqVky4tQf+Gexbhok38N9Jx3GKHBEZCWxV1buBCcAhIvKNiM1yYoKoatQ2OEVOkOP2FyyX4z+e4+Y4juM46XHh5jiO4ziOUyB4WMpxHMdxHKdAcOHmOI7jOI5TILhwcxzHcRzHKRBcuDmO4ziO4xQILtwcx3Ecx3EKBBdujuM4juM4BYILN8dxHMdxnALBhZvjOI7jOE6B4MLNcRzHcRynQPj/1S34v/TQ7tMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.706078    6.52028757]\n",
      "[-23.293922    -3.47971243]\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx@w\n",
    "    return -(1/y.shape[0]) * tx.T@e\n",
    "\n",
    "print(compute_gradient(y, tx, np.array([100, 20])))\n",
    "print(compute_gradient(y, tx, np.array([50, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.236712759167, w0=21.98817660063156, w1=4.0439137304966914\n",
      "GD iter. 1/49: loss=1376.0427920650948, w0=37.37990022107365, w1=6.874653341844389\n",
      "GD iter. 2/49: loss=682.1077709249996, w0=48.15410675538311, w1=8.856171069787777\n",
      "GD iter. 3/49: loss=342.0796105663528, w0=55.69605132939974, w1=10.243233479348147\n",
      "GD iter. 4/49: loss=175.46581199061586, w0=60.975412531211376, w1=11.214177166040413\n",
      "GD iter. 5/49: loss=93.82505068850475, w0=64.67096537247951, w1=11.893837746725\n",
      "GD iter. 6/49: loss=53.82107765047037, w0=67.25785236136721, w1=12.36960015320421\n",
      "GD iter. 7/49: loss=34.219130861833506, w0=69.0686732535886, w1=12.70263383773966\n",
      "GD iter. 8/49: loss=24.61417693540144, w0=70.33624787814358, w1=12.935757416914475\n",
      "GD iter. 9/49: loss=19.90774951144969, w0=71.22355011533206, w1=13.098943922336847\n",
      "GD iter. 10/49: loss=17.60160007371335, w0=71.844661681364, w1=13.213174476132506\n",
      "GD iter. 11/49: loss=16.471586849222533, w0=72.27943977758636, w1=13.293135863789468\n",
      "GD iter. 12/49: loss=15.917880369222036, w0=72.58378444494201, w1=13.349108835149341\n",
      "GD iter. 13/49: loss=15.646564194021787, w0=72.79682571209096, w1=13.388289915101254\n",
      "GD iter. 14/49: loss=15.513619268173674, w0=72.94595459909523, w1=13.415716671067592\n",
      "GD iter. 15/49: loss=15.448476254508094, w0=73.05034481999822, w1=13.434915400244028\n",
      "GD iter. 16/49: loss=15.416556177811962, w0=73.12341797463031, w1=13.448354510667535\n",
      "GD iter. 17/49: loss=15.400915340230855, w0=73.17456918287277, w1=13.457761887963988\n",
      "GD iter. 18/49: loss=15.393251329816115, w0=73.2103750286425, w1=13.464347052071506\n",
      "GD iter. 19/49: loss=15.38949596471289, w0=73.2354391206813, w1=13.468956666946768\n",
      "GD iter. 20/49: loss=15.387655835812314, w0=73.25298398510847, w1=13.472183397359453\n",
      "GD iter. 21/49: loss=15.386754172651028, w0=73.26526539020749, w1=13.474442108648331\n",
      "GD iter. 22/49: loss=15.386312357701998, w0=73.2738623737768, w1=13.476023206550547\n",
      "GD iter. 23/49: loss=15.386095868376973, w0=73.27988026227531, w1=13.477129975082098\n",
      "GD iter. 24/49: loss=15.385989788607713, w0=73.28409278422427, w1=13.477904713054183\n",
      "GD iter. 25/49: loss=15.385937809520772, w0=73.28704154958855, w1=13.478447029634642\n",
      "GD iter. 26/49: loss=15.385912339768174, w0=73.28910568534354, w1=13.478826651240965\n",
      "GD iter. 27/49: loss=15.385899859589399, w0=73.29055058037204, w1=13.47909238636539\n",
      "GD iter. 28/49: loss=15.3858937443018, w0=73.29156200689198, w1=13.479278400952488\n",
      "GD iter. 29/49: loss=15.385890747810878, w0=73.29227000545595, w1=13.479408611163457\n",
      "GD iter. 30/49: loss=15.385889279530325, w0=73.29276560445072, w1=13.479499758311135\n",
      "GD iter. 31/49: loss=15.385888560072852, w0=73.29311252374706, w1=13.47956356131451\n",
      "GD iter. 32/49: loss=15.385888207538692, w0=73.29335536725449, w1=13.479608223416871\n",
      "GD iter. 33/49: loss=15.385888034796954, w0=73.2935253577097, w1=13.479639486888525\n",
      "GD iter. 34/49: loss=15.385887950153503, w0=73.29364435102835, w1=13.479661371318683\n",
      "GD iter. 35/49: loss=15.38588790867821, w0=73.2937276463514, w1=13.479676690419792\n",
      "GD iter. 36/49: loss=15.385887888355317, w0=73.29378595307753, w1=13.479687413790568\n",
      "GD iter. 37/49: loss=15.3858878783971, w0=73.29382676778583, w1=13.479694920150113\n",
      "GD iter. 38/49: loss=15.385887873517573, w0=73.29385533808164, w1=13.479700174601794\n",
      "GD iter. 39/49: loss=15.385887871126604, w0=73.29387533728871, w1=13.47970385271797\n",
      "GD iter. 40/49: loss=15.385887869955031, w0=73.29388933673366, w1=13.479706427399293\n",
      "GD iter. 41/49: loss=15.385887869380959, w0=73.29389913634512, w1=13.47970822967622\n",
      "GD iter. 42/49: loss=15.385887869099665, w0=73.29390599607314, w1=13.479709491270068\n",
      "GD iter. 43/49: loss=15.38588786896183, w0=73.29391079788275, w1=13.479710374385762\n",
      "GD iter. 44/49: loss=15.38588786889429, w0=73.29391415914948, w1=13.479710992566748\n",
      "GD iter. 45/49: loss=15.385887868861197, w0=73.29391651203619, w1=13.479711425293438\n",
      "GD iter. 46/49: loss=15.385887868844978, w0=73.29391815905689, w1=13.47971172820212\n",
      "GD iter. 47/49: loss=15.385887868837035, w0=73.29391931197138, w1=13.479711940238198\n",
      "GD iter. 48/49: loss=15.38588786883314, w0=73.29392011901152, w1=13.479712088663453\n",
      "GD iter. 49/49: loss=15.385887868831233, w0=73.29392068393962, w1=13.47971219256113\n",
      "GD: execution time=0.045 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.3\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15324d958cdb4596addaa0088cbe4010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx@w\n",
    "    return -(1/y.shape[0]) * tx.T@e\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, x_batch in batch_iter(y, tx, batch_size):\n",
    "            grad = compute_stoch_gradient(y_batch, x_batch, w)\n",
    "            w = w - gamma*grad\n",
    "    \n",
    "        loss = compute_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2626.922186003054, w0=4.7006141253179035, w1=-9.280575585787922\n",
      "SGD iter. 1/49: loss=2422.3516596243235, w0=9.244228997307761, w1=-13.195526448705976\n",
      "SGD iter. 2/49: loss=2026.1617569989048, w0=15.681652988327045, w1=-13.022706272446174\n",
      "SGD iter. 3/49: loss=1728.9860817578833, w0=21.798346592856046, w1=-14.366402214068296\n",
      "SGD iter. 4/49: loss=996.3002833451212, w0=31.03754172281211, w1=0.2046562526463216\n",
      "SGD iter. 5/49: loss=931.1133966683353, w0=33.89693978952637, w1=-3.2335400952864912\n",
      "SGD iter. 6/49: loss=804.2666651054492, w0=38.151487511592535, w1=-5.034359309315807\n",
      "SGD iter. 7/49: loss=474.2109136947638, w0=44.63068585365539, w1=3.6782357528727037\n",
      "SGD iter. 8/49: loss=303.00374594466143, w0=49.31316808689587, w1=13.080766561671645\n",
      "SGD iter. 9/49: loss=237.74659965840326, w0=52.25092795648826, w1=12.096301731228843\n",
      "SGD iter. 10/49: loss=179.79268551125142, w0=55.16953945973275, w1=12.913715312567662\n",
      "SGD iter. 11/49: loss=145.24398682302646, w0=57.18355139502787, w1=13.064794598528332\n",
      "SGD iter. 12/49: loss=115.07214501167812, w0=59.230939289954435, w1=12.212814047806205\n",
      "SGD iter. 13/49: loss=102.51937459479775, w0=60.26234075638418, w1=11.37142789818924\n",
      "SGD iter. 14/49: loss=75.97630180829596, w0=62.28815094498738, w1=13.71172820436908\n",
      "SGD iter. 15/49: loss=69.00856538382432, w0=62.94343033082234, w1=13.144038199018873\n",
      "SGD iter. 16/49: loss=67.48359630464319, w0=63.093017148113645, w1=13.109635373501424\n",
      "SGD iter. 17/49: loss=59.02399200938379, w0=64.0908446170947, w1=11.873606919583123\n",
      "SGD iter. 18/49: loss=59.31110424828202, w0=64.05213654509276, w1=11.917715531836258\n",
      "SGD iter. 19/49: loss=48.70769114295203, w0=65.1410012654843, w1=13.063191114259315\n",
      "SGD iter. 20/49: loss=43.65080358990642, w0=65.84606861149054, w1=12.450483930688922\n",
      "SGD iter. 21/49: loss=40.956492932143405, w0=66.16426838233828, w1=12.92361047601698\n",
      "SGD iter. 22/49: loss=33.64496804918551, w0=67.25537369766238, w1=13.712295185043844\n",
      "SGD iter. 23/49: loss=28.16400724595595, w0=68.25798422535765, w1=13.03748007188102\n",
      "SGD iter. 24/49: loss=27.06612811834838, w0=68.48477671858998, w1=12.997423993201958\n",
      "SGD iter. 25/49: loss=23.079547379795084, w0=69.41561959151929, w1=12.891418771947404\n",
      "SGD iter. 26/49: loss=23.299002760324043, w0=69.36320999611111, w1=12.866741873768293\n",
      "SGD iter. 27/49: loss=19.728398431596318, w0=70.52756539175604, w1=12.463694605464676\n",
      "SGD iter. 28/49: loss=17.622865430084445, w0=71.29902517218025, w1=12.77661801710086\n",
      "SGD iter. 29/49: loss=20.22238069444844, w0=72.67751019148419, w1=10.43126657997292\n",
      "SGD iter. 30/49: loss=19.374750619790618, w0=72.81773569304006, w1=10.695655664558286\n",
      "SGD iter. 31/49: loss=17.876908573660387, w0=73.54004307458639, w1=11.261274735313291\n",
      "SGD iter. 32/49: loss=17.020657445922833, w0=74.19775133468835, w1=11.91362441327726\n",
      "SGD iter. 33/49: loss=16.89448094522143, w0=74.68456848457585, w1=12.438900966193648\n",
      "SGD iter. 34/49: loss=16.912071058847438, w0=74.58907189664394, w1=12.307128480334919\n",
      "SGD iter. 35/49: loss=17.301512878746962, w0=74.75719070519762, w1=12.179676003812858\n",
      "SGD iter. 36/49: loss=17.286050821052527, w0=74.45576357791857, w1=11.914321091855419\n",
      "SGD iter. 37/49: loss=16.536781824749568, w0=74.24336817749659, w1=12.2963528650041\n",
      "SGD iter. 38/49: loss=17.298510748719394, w0=73.34302766092189, w1=11.524505439795915\n",
      "SGD iter. 39/49: loss=16.619995878705364, w0=74.4657469216447, w1=12.433269709514713\n",
      "SGD iter. 40/49: loss=17.59045732037726, w0=75.0426335189509, w1=12.317323974126282\n",
      "SGD iter. 41/49: loss=16.616535745036245, w0=74.78719997342547, w1=12.998654587341534\n",
      "SGD iter. 42/49: loss=16.49217878322827, w0=74.65667713293057, w1=12.883490762951848\n",
      "SGD iter. 43/49: loss=16.56045109090161, w0=73.50935956116825, w1=14.99718174350549\n",
      "SGD iter. 44/49: loss=17.642881734339692, w0=73.84220205508555, w1=15.532363577907503\n",
      "SGD iter. 45/49: loss=17.430757213792738, w0=74.02601982863186, w1=15.364857379634314\n",
      "SGD iter. 46/49: loss=17.987122983986428, w0=75.05487251772875, w1=14.929375675827817\n",
      "SGD iter. 47/49: loss=17.550284655795895, w0=74.10083010188937, w1=15.397443618558757\n",
      "SGD iter. 48/49: loss=17.575862575497204, w0=74.10980398700741, w1=15.406960732445198\n",
      "SGD iter. 49/49: loss=17.41938410851971, w0=73.74735164168447, w1=15.444755447579771\n",
      "SGD: execution time=0.091 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e51bd1ebc84706bba29b7e331a185a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.835114535854, w0=51.84746409844846, w1=7.724426406192428\n",
      "GD iter. 1/49: loss=318.2821247015954, w0=67.40170332798299, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165127, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631794\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249234, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.03488900159354\n",
      "GD iter. 12/49: loss=65.93073010339528, w0=74.06779404612573, w1=11.034893106670433\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260343, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.03489486560434\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260339, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988822\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260339, w0=74.06780585492632, w1=11.034894865989093\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start the grid search\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb4f62fb1bb4be6b0202fd9fb839a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    e = y - tx@w\n",
    "    return -(1/y.shape[0]) * tx.T@np.sign(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w, mse=False)\n",
    "        \n",
    "        w = w - gamma*grad\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7000000000000004, w1=7.625844400394043e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4000000000000008, w1=1.5251688800788087e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.1000000000000014, w1=2.287753320118213e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8000000000000016, w1=3.0503377601576174e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5000000000000018, w1=3.812922200197022e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492639, w0=4.200000000000002, w1=4.575506640236426e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.900000000000002, w1=5.3380910802758305e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492637, w0=5.600000000000002, w1=6.100675520315235e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.3000000000000025, w1=6.863259960354639e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000003, w1=7.625844400394044e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000003, w1=8.388428840433449e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.400000000000004, w1=9.151013280472854e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.100000000000005, w1=9.913597720512259e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.800000000000006, w1=1.0676182160551664e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.500000000000007, w1=1.1438766600591069e-14\n",
      "SubGD iter. 15/499: loss=63.56780585492637, w0=11.200000000000008, w1=1.2201351040630474e-14\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.90000000000001, w1=1.2963935480669879e-14\n",
      "SubGD iter. 17/499: loss=62.16780585492637, w0=12.60000000000001, w1=1.3726519920709284e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492636, w0=13.300000000000011, w1=1.448910436074869e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492637, w0=14.000000000000012, w1=1.5251688800788094e-14\n",
      "SubGD iter. 20/499: loss=60.06780585492637, w0=14.700000000000014, w1=1.60142732408275e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492637, w0=15.400000000000015, w1=1.6776857680866904e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926364, w0=16.100000000000016, w1=1.753944212090631e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492636, w0=16.800000000000015, w1=1.8302026560945714e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926365, w0=17.500000000000014, w1=1.906461100098512e-14\n",
      "SubGD iter. 25/499: loss=56.56780585492637, w0=18.200000000000014, w1=1.9827195441024524e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492637, w0=18.900000000000013, w1=2.058977988106393e-14\n",
      "SubGD iter. 27/499: loss=55.16780585492637, w0=19.600000000000012, w1=2.1352364321103335e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492636, w0=20.30000000000001, w1=2.211494876114274e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926365, w0=21.00000000000001, w1=2.2877533201182145e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492637, w0=21.70000000000001, w1=2.364011764122155e-14\n",
      "SubGD iter. 31/499: loss=52.36780585492637, w0=22.40000000000001, w1=2.4402702081260955e-14\n",
      "SubGD iter. 32/499: loss=51.66780585492637, w0=23.10000000000001, w1=2.516528652130036e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492636, w0=23.800000000000008, w1=2.5927870961339765e-14\n",
      "SubGD iter. 34/499: loss=50.26780585492637, w0=24.500000000000007, w1=2.669045540137917e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492638, w0=25.200000000000006, w1=2.7453039841418575e-14\n",
      "SubGD iter. 36/499: loss=48.86780585492637, w0=25.900000000000006, w1=2.821562428145798e-14\n",
      "SubGD iter. 37/499: loss=48.16780585492637, w0=26.600000000000005, w1=2.8978208721497385e-14\n",
      "SubGD iter. 38/499: loss=47.46780585492637, w0=27.300000000000004, w1=2.9740793161536787e-14\n",
      "SubGD iter. 39/499: loss=46.76780585492637, w0=28.000000000000004, w1=3.050337760157619e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492638, w0=28.700000000000003, w1=3.126596204161559e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926374, w0=29.400000000000002, w1=3.202854648165499e-14\n",
      "SubGD iter. 42/499: loss=44.66780585492637, w0=30.1, w1=3.2791130921694394e-14\n",
      "SubGD iter. 43/499: loss=43.96780585492637, w0=30.8, w1=3.3553715361733796e-14\n",
      "SubGD iter. 44/499: loss=43.26780585492637, w0=31.5, w1=3.43162998017732e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926384, w0=32.2, w1=3.50788842418126e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926374, w0=32.900000000000006, w1=3.5841468681852e-14\n",
      "SubGD iter. 47/499: loss=41.16780585492638, w0=33.60000000000001, w1=3.6604053121891404e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492637, w0=34.30000000000001, w1=3.7366637561930805e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926365, w0=35.000000000000014, w1=3.812922200197021e-14\n",
      "SubGD iter. 50/499: loss=39.06780585492637, w0=35.70000000000002, w1=3.889180644200961e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492637, w0=36.40000000000002, w1=3.965439088204901e-14\n",
      "SubGD iter. 52/499: loss=37.667805854926364, w0=37.10000000000002, w1=4.041697532208841e-14\n",
      "SubGD iter. 53/499: loss=36.967805854926354, w0=37.800000000000026, w1=4.1179559762127815e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492636, w0=38.50000000000003, w1=4.1942144202167217e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492635, w0=39.20000000000003, w1=4.270472864220662e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492634, w0=39.900000000000034, w1=4.346731308224602e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492634, w0=40.60000000000004, w1=4.422989752228542e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492633, w0=41.30000000000004, w1=4.4992481962324824e-14\n",
      "SubGD iter. 59/499: loss=32.76780585492634, w0=42.00000000000004, w1=4.5755066402364226e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926334, w0=42.700000000000045, w1=4.651765084240363e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492633, w0=43.40000000000005, w1=4.728023528244303e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926332, w0=44.10000000000005, w1=4.804281972248243e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926326, w0=44.800000000000054, w1=4.8805404162521834e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926323, w0=45.50000000000006, w1=4.9567988602561235e-14\n",
      "SubGD iter. 65/499: loss=28.56780585492632, w0=46.20000000000006, w1=5.033057304260064e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926317, w0=46.90000000000006, w1=5.109315748264004e-14\n",
      "SubGD iter. 67/499: loss=27.173270209668893, w0=47.59306930693076, w1=0.011147845678281268\n",
      "SubGD iter. 68/499: loss=26.490451563751183, w0=48.279207920792146, w1=0.03308574108990965\n",
      "SubGD iter. 69/499: loss=25.817212322770157, w0=48.965346534653534, w1=0.055023636501538034\n",
      "SubGD iter. 70/499: loss=25.155039434656434, w0=49.630693069307, w1=0.10538326388308852\n",
      "SubGD iter. 71/499: loss=24.524103413894757, w0=50.28910891089116, w1=0.16746568532794484\n",
      "SubGD iter. 72/499: loss=23.89929534603557, w0=50.94752475247532, w1=0.22954810677280116\n",
      "SubGD iter. 73/499: loss=23.284392925657123, w0=51.59207920792086, w1=0.31242512932748595\n",
      "SubGD iter. 74/499: loss=22.686876444181824, w0=52.22277227722779, w1=0.4119501328840099\n",
      "SubGD iter. 75/499: loss=22.10626756964053, w0=52.84653465346541, w1=0.5208167847923866\n",
      "SubGD iter. 76/499: loss=21.53781882800841, w0=53.45643564356442, w1=0.6457900912636104\n",
      "SubGD iter. 77/499: loss=20.986339874628445, w0=54.059405940594125, w1=0.7796904498577328\n",
      "SubGD iter. 78/499: loss=20.44556093662042, w0=54.655445544554524, w1=0.9197570104995809\n",
      "SubGD iter. 79/499: loss=19.91191015895782, w0=55.244554455445616, w1=1.0670920297850033\n",
      "SubGD iter. 80/499: loss=19.389644090563205, w0=55.81980198019809, w1=1.2261255948210887\n",
      "SubGD iter. 81/499: loss=18.88798906439586, w0=56.36732673267334, w1=1.4107093426222252\n",
      "SubGD iter. 82/499: loss=18.41596050185421, w0=56.900990099009974, w1=1.6058537322202813\n",
      "SubGD iter. 83/499: loss=17.95489854304036, w0=57.4277227722773, w1=1.8087628022939741\n",
      "SubGD iter. 84/499: loss=17.505757656579803, w0=57.9336633663367, w1=2.0285064197514817\n",
      "SubGD iter. 85/499: loss=17.074957426931594, w0=58.4326732673268, w1=2.24943708486729\n",
      "SubGD iter. 86/499: loss=16.65296729750988, w0=58.91089108910898, w1=2.4837982986028466\n",
      "SubGD iter. 87/499: loss=16.248540731496703, w0=59.38217821782185, w1=2.7260245553531632\n",
      "SubGD iter. 88/499: loss=15.849105212654136, w0=59.83960396039611, w1=2.9787423334691487\n",
      "SubGD iter. 89/499: loss=15.466919791231307, w0=60.26237623762383, w1=3.251528669355451\n",
      "SubGD iter. 90/499: loss=15.108294621512195, w0=60.67821782178225, w1=3.5270865794242927\n",
      "SubGD iter. 91/499: loss=14.754896345922813, w0=61.087128712871355, w1=3.8064591839518287\n",
      "SubGD iter. 92/499: loss=14.404528961620256, w0=61.49603960396046, w1=4.085831788479364\n",
      "SubGD iter. 93/499: loss=14.055787028127256, w0=61.891089108910954, w1=4.373839384328622\n",
      "SubGD iter. 94/499: loss=13.714620911605614, w0=62.27920792079214, w1=4.666037469532062\n",
      "SubGD iter. 95/499: loss=13.381236307284132, w0=62.653465346534716, w1=4.959829093241784\n",
      "SubGD iter. 96/499: loss=13.058821615166217, w0=63.020792079207986, w1=5.257057192056655\n",
      "SubGD iter. 97/499: loss=12.740251724339217, w0=63.38118811881195, w1=5.560434316352422\n",
      "SubGD iter. 98/499: loss=12.42321888875609, w0=63.74158415841591, w1=5.86381144064819\n",
      "SubGD iter. 99/499: loss=12.10756173190115, w0=64.08811881188126, w1=6.172402175278565\n",
      "SubGD iter. 100/499: loss=11.800622097398117, w0=64.42772277227729, w1=6.486369310516515\n",
      "SubGD iter. 101/499: loss=11.495041794646406, w0=64.76732673267333, w1=6.800336445754466\n",
      "SubGD iter. 102/499: loss=11.189461491894695, w0=65.10693069306936, w1=7.114303580992416\n",
      "SubGD iter. 103/499: loss=10.883881189142983, w0=65.44653465346539, w1=7.428270716230367\n",
      "SubGD iter. 104/499: loss=10.584593408313182, w0=65.76534653465352, w1=7.747893210218644\n",
      "SubGD iter. 105/499: loss=10.295816534318924, w0=66.07029702970303, w1=8.073669686866923\n",
      "SubGD iter. 106/499: loss=10.011352081221341, w0=66.37524752475254, w1=8.399446163515202\n",
      "SubGD iter. 107/499: loss=9.72808432666811, w0=66.66633663366343, w1=8.732970280417408\n",
      "SubGD iter. 108/499: loss=9.448125461122489, w0=66.95742574257433, w1=9.066494397319614\n",
      "SubGD iter. 109/499: loss=9.171041104096652, w0=67.23465346534661, w1=9.398630319470307\n",
      "SubGD iter. 110/499: loss=8.903656131158945, w0=67.51188118811889, w1=9.730766241621\n",
      "SubGD iter. 111/499: loss=8.636271158221236, w0=67.78910891089117, w1=10.062902163771692\n",
      "SubGD iter. 112/499: loss=8.376151920302355, w0=68.06633663366345, w1=10.36399928997944\n",
      "SubGD iter. 113/499: loss=8.140540838751479, w0=68.32970297029712, w1=10.66046690927363\n",
      "SubGD iter. 114/499: loss=7.918544501597256, w0=68.59306930693079, w1=10.943174379960832\n",
      "SubGD iter. 115/499: loss=7.705279728376982, w0=68.85643564356445, w1=11.225881850648033\n",
      "SubGD iter. 116/499: loss=7.4936958311786235, w0=69.11287128712881, w1=11.504395843582225\n",
      "SubGD iter. 117/499: loss=7.289992405743398, w0=69.35544554455456, w1=11.78820189306777\n",
      "SubGD iter. 118/499: loss=7.0972340357815265, w0=69.58415841584169, w1=12.06091146519099\n",
      "SubGD iter. 119/499: loss=6.919905294668907, w0=69.8059405940595, w1=12.324245668386068\n",
      "SubGD iter. 120/499: loss=6.750573527315438, w0=70.02772277227733, w1=12.587579871581145\n",
      "SubGD iter. 121/499: loss=6.584744810805648, w0=70.25643564356446, w1=12.824765405096503\n",
      "SubGD iter. 122/499: loss=6.43034327634779, w0=70.47821782178228, w1=13.065616959310168\n",
      "SubGD iter. 123/499: loss=6.278071481890337, w0=70.6930693069308, w1=13.302953389983932\n",
      "SubGD iter. 124/499: loss=6.1336633292633085, w0=70.8940594059407, w1=13.525403099312937\n",
      "SubGD iter. 125/499: loss=6.005840798343017, w0=71.08811881188129, w1=13.742945617944232\n",
      "SubGD iter. 126/499: loss=5.885021825223205, w0=71.27524752475257, w1=13.953548196006865\n",
      "SubGD iter. 127/499: loss=5.771635252269645, w0=71.46237623762386, w1=14.1641507740695\n",
      "SubGD iter. 128/499: loss=5.667162061790244, w0=71.62178217821791, w1=14.349779559473198\n",
      "SubGD iter. 129/499: loss=5.586726765993134, w0=71.75346534653474, w1=14.516890107612335\n",
      "SubGD iter. 130/499: loss=5.523847812160379, w0=71.87128712871295, w1=14.67079118532421\n",
      "SubGD iter. 131/499: loss=5.4800937085918635, w0=71.95445544554464, w1=14.780276456654546\n",
      "SubGD iter. 132/499: loss=5.4530880035020175, w0=72.03762376237633, w1=14.889761727984881\n",
      "SubGD iter. 133/499: loss=5.427392630862901, w0=72.1069306930694, w1=14.985916181776751\n",
      "SubGD iter. 134/499: loss=5.407322445682746, w0=72.17623762376247, w1=15.082070635568622\n",
      "SubGD iter. 135/499: loss=5.387252260502593, w0=72.24554455445555, w1=15.178225089360492\n",
      "SubGD iter. 136/499: loss=5.370460780338691, w0=72.30099009901001, w1=15.259723489715935\n",
      "SubGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049516, w1=15.335091856448162\n",
      "SubGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198031, w1=15.41046022318039\n",
      "SubGD iter. 139/499: loss=5.33571465951747, w0=72.43267326732685, w1=15.46996178675575\n",
      "SubGD iter. 140/499: loss=5.330043910465358, w0=72.46039603960408, w1=15.518645285832834\n",
      "SubGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188131, w1=15.561592159086512\n",
      "SubGD iter. 142/499: loss=5.322176726526589, w0=72.50198019801992, w1=15.59782833203255\n",
      "SubGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722784, w1=15.624722856626738\n",
      "SubGD iter. 144/499: loss=5.318478284898437, w0=72.55049504950507, w1=15.642690329098025\n",
      "SubGD iter. 145/499: loss=5.317240048565144, w0=72.56435643564369, w1=15.664356578291116\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485161, w1=15.677095775361309\n",
      "SubGD iter. 147/499: loss=5.315557122666142, w0=72.60594059405953, w1=15.689834972431502\n",
      "SubGD iter. 148/499: loss=5.314707697380738, w0=72.62673267326745, w1=15.702574169501695\n",
      "SubGD iter. 149/499: loss=5.313876880922166, w0=72.64059405940607, w1=15.724240418694786\n",
      "SubGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861399, w1=15.736979615764978\n",
      "SubGD iter. 151/499: loss=5.312377839024388, w0=72.6683168316833, w1=15.748110294231305\n",
      "SubGD iter. 152/499: loss=5.312132229725042, w0=72.6752475247526, w1=15.759240972697631\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782191, w1=15.770371651163957\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782191, w1=15.774323911906711\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782191, w1=15.778276172649464\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782191, w1=15.782228433392218\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782191, w1=15.786180694134972\n",
      "SubGD iter. 158/499: loss=5.311594306869985, w0=72.68217821782191, w1=15.790132954877725\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782191, w1=15.794085215620479\n",
      "SubGD iter. 160/499: loss=5.311549677255759, w0=72.68217821782191, w1=15.798037476363232\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782191, w1=15.801989737105986\n",
      "SubGD iter. 162/499: loss=5.3115050476415355, w0=72.68217821782191, w1=15.80594199784874\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782191, w1=15.809894258591493\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782191, w1=15.813846519334247\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782191, w1=15.817798780077\n",
      "SubGD iter. 166/499: loss=5.311415788413084, w0=72.68217821782191, w1=15.821751040819754\n",
      "SubGD iter. 167/499: loss=5.311393473605972, w0=72.68217821782191, w1=15.825703301562507\n",
      "SubGD iter. 168/499: loss=5.311371158798861, w0=72.68217821782191, w1=15.82965556230526\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782191, w1=15.833607823048014\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782191, w1=15.837560083790768\n",
      "SubGD iter. 171/499: loss=5.3113042143775235, w0=72.68217821782191, w1=15.841512344533522\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782191, w1=15.845464605276275\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782191, w1=15.849416866019029\n",
      "SubGD iter. 174/499: loss=5.311237269956186, w0=72.68217821782191, w1=15.853369126761782\n",
      "SubGD iter. 175/499: loss=5.3112149551490715, w0=72.68217821782191, w1=15.857321387504536\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782191, w1=15.86127364824729\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782191, w1=15.865225908990043\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782191, w1=15.869178169732796\n",
      "SubGD iter. 179/499: loss=5.311125695920623, w0=72.68217821782191, w1=15.87313043047555\n",
      "SubGD iter. 180/499: loss=5.3111033811135115, w0=72.68217821782191, w1=15.877082691218304\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782191, w1=15.881034951961057\n",
      "SubGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782191, w1=15.88498721270381\n",
      "SubGD iter. 183/499: loss=5.311036436692174, w0=72.68217821782191, w1=15.888939473446564\n",
      "SubGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782191, w1=15.892891734189318\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782191, w1=15.896843994932071\n",
      "SubGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782191, w1=15.900796255674825\n",
      "SubGD iter. 187/499: loss=5.3109471774637225, w0=72.68217821782191, w1=15.904748516417579\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782191, w1=15.908700777160332\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782191, w1=15.912653037903086\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.6752475247526, w1=15.910526938117348\n",
      "SubGD iter. 191/499: loss=5.31089223718627, w0=72.6752475247526, w1=15.914479198860102\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.6752475247526, w1=15.918431459602855\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.6683168316833, w1=15.916305359817118\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.6683168316833, w1=15.920257620559871\n",
      "SubGD iter. 195/499: loss=5.310837296908815, w0=72.6683168316833, w1=15.924209881302625\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.6683168316833, w1=15.928162142045379\n",
      "SubGD iter. 197/499: loss=5.310823570190171, w0=72.66138613861399, w1=15.926036042259641\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861399, w1=15.929988303002395\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861399, w1=15.933940563745148\n",
      "SubGD iter. 200/499: loss=5.310772500182622, w0=72.65445544554468, w1=15.93181446395941\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554468, w1=15.935766724702164\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554468, w1=15.939718985444918\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554468, w1=15.943671246187671\n",
      "SubGD iter. 204/499: loss=5.31073343431896, w0=72.64752475247538, w1=15.941545146401934\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247538, w1=15.945497407144687\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247538, w1=15.949449667887441\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940607, w1=15.947323568101703\n",
      "SubGD iter. 208/499: loss=5.3106844802203375, w0=72.64059405940607, w1=15.951275828844457\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940607, w1=15.95522808958721\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940607, w1=15.959180350329964\n",
      "SubGD iter. 211/499: loss=5.310643298447748, w0=72.63366336633676, w1=15.957054250544227\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633676, w1=15.96100651128698\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633676, w1=15.964958772029734\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326745, w1=15.962832672243996\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633676, w1=15.9673013720514\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326745, w1=15.965175272265663\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633676, w1=15.969643972073067\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326745, w1=15.96751787228733\n",
      "SubGD iter. 219/499: loss=5.310603273452553, w0=72.63366336633676, w1=15.971986572094734\n",
      "SubGD iter. 220/499: loss=5.31061357387479, w0=72.62673267326745, w1=15.969860472308996\n",
      "SubGD iter. 221/499: loss=5.310588318629315, w0=72.63366336633676, w1=15.9743291721164\n",
      "SubGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326745, w1=15.972203072330663\n",
      "SubGD iter. 223/499: loss=5.3105749661498844, w0=72.62673267326745, w1=15.970593411609576\n",
      "SubGD iter. 224/499: loss=5.310583639649727, w0=72.63366336633676, w1=15.97506211141698\n",
      "SubGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326745, w1=15.972936011631242\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326745, w1=15.971326350910156\n",
      "SubGD iter. 227/499: loss=5.31057896067014, w0=72.63366336633676, w1=15.97579505071756\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326745, w1=15.973668950931822\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326745, w1=15.972059290210735\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326745, w1=15.970449629489648\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633676, w1=15.974918329297052\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326745, w1=15.972792229511315\n",
      "SubGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326745, w1=15.971182568790228\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633676, w1=15.975651268597632\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326745, w1=15.973525168811895\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326745, w1=15.971915508090808\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633676, w1=15.976384207898212\n",
      "SubGD iter. 238/499: loss=5.310626930749845, w0=72.62673267326745, w1=15.974258108112474\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326745, w1=15.972648447391387\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326745, w1=15.9710387866703\n",
      "SubGD iter. 241/499: loss=5.310580796439088, w0=72.63366336633676, w1=15.975507486477705\n",
      "SubGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326745, w1=15.973381386691967\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326745, w1=15.97177172597088\n",
      "SubGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633676, w1=15.976240425778284\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326745, w1=15.974114325992547\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326745, w1=15.97250466527146\n",
      "SubGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326745, w1=15.970895004550373\n",
      "SubGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633676, w1=15.975363704357777\n",
      "SubGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326745, w1=15.97323760457204\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326745, w1=15.971627943850953\n",
      "SubGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633676, w1=15.976096643658357\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326745, w1=15.97397054387262\n",
      "SubGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326745, w1=15.972360883151532\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326745, w1=15.970751222430446\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633676, w1=15.97521992223785\n",
      "SubGD iter. 256/499: loss=5.310623394481998, w0=72.62673267326745, w1=15.973093822452112\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326745, w1=15.971484161731025\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633676, w1=15.97595286153843\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326745, w1=15.973826761752692\n",
      "SubGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326745, w1=15.972217101031605\n",
      "SubGD iter. 261/499: loss=5.310574998409099, w0=72.62673267326745, w1=15.970607440310518\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633676, w1=15.975076140117922\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326745, w1=15.972950040332185\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326745, w1=15.971340379611098\n",
      "SubGD iter. 265/499: loss=5.310578871112921, w0=72.63366336633676, w1=15.975809079418502\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326745, w1=15.973682979632764\n",
      "SubGD iter. 267/499: loss=5.310578369219392, w0=72.62673267326745, w1=15.972073318911677\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326745, w1=15.97046365819059\n",
      "SubGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633676, w1=15.974932357997995\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326745, w1=15.972806258212257\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326745, w1=15.97119659749117\n",
      "SubGD iter. 272/499: loss=5.310579788997397, w0=72.63366336633676, w1=15.975665297298574\n",
      "SubGD iter. 273/499: loss=5.310624747213173, w0=72.62673267326745, w1=15.973539197512837\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326745, w1=15.97192953679175\n",
      "SubGD iter. 275/499: loss=5.310575110017809, w0=72.63366336633676, w1=15.976398236599154\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326745, w1=15.974272136813417\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326745, w1=15.97266247609233\n",
      "SubGD iter. 278/499: loss=5.310576022555871, w0=72.62673267326745, w1=15.971052815371243\n",
      "SubGD iter. 279/499: loss=5.310580706881869, w0=72.63366336633676, w1=15.975521515178647\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326745, w1=15.97339541539291\n",
      "SubGD iter. 281/499: loss=5.310577707961018, w0=72.62673267326745, w1=15.971785754671822\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633676, w1=15.976254454479227\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326745, w1=15.97412835469349\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326745, w1=15.972518693972402\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326745, w1=15.970909033251315\n",
      "SubGD iter. 286/499: loss=5.310581624766343, w0=72.63366336633676, w1=15.97537773305872\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326745, w1=15.973251633272982\n",
      "SubGD iter. 288/499: loss=5.310577377331833, w0=72.62673267326745, w1=15.971641972551895\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633676, w1=15.9761106723593\n",
      "SubGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326745, w1=15.973984572573562\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326745, w1=15.972374911852475\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326745, w1=15.970765251131388\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633676, w1=15.975233950938792\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326745, w1=15.973107851153054\n",
      "SubGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326745, w1=15.971498190431968\n",
      "SubGD iter. 296/499: loss=5.310577863671229, w0=72.63366336633676, w1=15.975966890239372\n",
      "SubGD iter. 297/499: loss=5.310625663237011, w0=72.62673267326745, w1=15.973840790453634\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326745, w1=15.972231129732547\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326745, w1=15.97062146901146\n",
      "SubGD iter. 300/499: loss=5.310583460535291, w0=72.63366336633676, w1=15.975090168818864\n",
      "SubGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326745, w1=15.972964069033127\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326745, w1=15.97135440831204\n",
      "SubGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633676, w1=15.975823108119444\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326745, w1=15.973697008333707\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326745, w1=15.97208734761262\n",
      "SubGD iter. 306/499: loss=5.3105747000391235, w0=72.62673267326745, w1=15.970477686891533\n",
      "SubGD iter. 307/499: loss=5.3105843784197635, w0=72.63366336633676, w1=15.974946386698937\n",
      "SubGD iter. 308/499: loss=5.310622563676499, w0=72.62673267326745, w1=15.9728202869132\n",
      "SubGD iter. 309/499: loss=5.310576385444271, w0=72.62673267326745, w1=15.971210626192113\n",
      "SubGD iter. 310/499: loss=5.310579699440177, w0=72.63366336633676, w1=15.975679325999517\n",
      "SubGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326745, w1=15.97355322621378\n",
      "SubGD iter. 312/499: loss=5.3105780708494175, w0=72.62673267326745, w1=15.971943565492692\n",
      "SubGD iter. 313/499: loss=5.310575020460588, w0=72.63366336633676, w1=15.976412265300096\n",
      "SubGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326745, w1=15.974286165514359\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326745, w1=15.972676504793272\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326745, w1=15.971066844072185\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633676, w1=15.97553554387959\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326745, w1=15.973409444093852\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326745, w1=15.971799783372765\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633676, w1=15.976268483180169\n",
      "SubGD iter. 321/499: loss=5.310626579260849, w0=72.62673267326745, w1=15.974142383394431\n",
      "SubGD iter. 322/499: loss=5.310579425625379, w0=72.62673267326745, w1=15.972532722673344\n",
      "SubGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326745, w1=15.970923061952258\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633676, w1=15.975391761759662\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326745, w1=15.973265661973924\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326745, w1=15.971656001252837\n",
      "SubGD iter. 327/499: loss=5.310576856229537, w0=72.63366336633676, w1=15.976124701060241\n",
      "SubGD iter. 328/499: loss=5.310626142553515, w0=72.62673267326745, w1=15.973998601274504\n",
      "SubGD iter. 329/499: loss=5.310579094996193, w0=72.62673267326745, w1=15.972388940553417\n",
      "SubGD iter. 330/499: loss=5.310575393556711, w0=72.62673267326745, w1=15.97077927983233\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633676, w1=15.975247979639734\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326745, w1=15.973121879853997\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326745, w1=15.97151221913291\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633676, w1=15.975980918940314\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326745, w1=15.973854819154576\n",
      "SubGD iter. 336/499: loss=5.310578764367006, w0=72.62673267326745, w1=15.97224515843349\n",
      "SubGD iter. 337/499: loss=5.310575062927523, w0=72.62673267326745, w1=15.970635497712403\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633676, w1=15.975104197519807\n",
      "SubGD iter. 339/499: loss=5.310623042993001, w0=72.62673267326745, w1=15.97297809773407\n",
      "SubGD iter. 340/499: loss=5.31057674833267, w0=72.62673267326745, w1=15.971368437012982\n",
      "SubGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633676, w1=15.975837136820386\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326745, w1=15.973711037034649\n",
      "SubGD iter. 343/499: loss=5.310578433737818, w0=72.62673267326745, w1=15.972101376313562\n",
      "SubGD iter. 344/499: loss=5.310574732298337, w0=72.62673267326745, w1=15.970491715592475\n",
      "SubGD iter. 345/499: loss=5.310584288862548, w0=72.63366336633676, w1=15.97496041539988\n",
      "SubGD iter. 346/499: loss=5.310622606285666, w0=72.62673267326745, w1=15.972834315614142\n",
      "SubGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326745, w1=15.971224654893055\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633676, w1=15.975693354700459\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326745, w1=15.973567254914721\n",
      "SubGD iter. 350/499: loss=5.310578103108631, w0=72.62673267326745, w1=15.971957594193634\n",
      "SubGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633676, w1=15.976426294001039\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326745, w1=15.974300194215301\n",
      "SubGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326745, w1=15.972690533494214\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326745, w1=15.971080872773127\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633676, w1=15.975549572580531\n",
      "SubGD iter. 356/499: loss=5.310624395724174, w0=72.62673267326745, w1=15.973423472794794\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326745, w1=15.971813812073707\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633676, w1=15.976282511881111\n",
      "SubGD iter. 359/499: loss=5.310626621870017, w0=72.62673267326745, w1=15.974156412095374\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326745, w1=15.972546751374287\n",
      "SubGD iter. 361/499: loss=5.310575756445111, w0=72.62673267326745, w1=15.9709370906532\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633676, w1=15.975405790460604\n",
      "SubGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326745, w1=15.973279690674866\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326745, w1=15.97167002995378\n",
      "SubGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633676, w1=15.976138729761184\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326745, w1=15.974012629975446\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326745, w1=15.97240296925436\n",
      "SubGD iter. 368/499: loss=5.310575425815922, w0=72.62673267326745, w1=15.970793308533272\n",
      "SubGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633676, w1=15.975262008340676\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326745, w1=15.973135908554939\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326745, w1=15.971526247833852\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633676, w1=15.975994947641256\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326745, w1=15.973868847855519\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326745, w1=15.972259187134432\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326745, w1=15.970649526413345\n",
      "SubGD iter. 376/499: loss=5.310583281420853, w0=72.63366336633676, w1=15.975118226220749\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326745, w1=15.972992126435011\n",
      "SubGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326745, w1=15.971382465713925\n",
      "SubGD iter. 379/499: loss=5.310578602441265, w0=72.63366336633676, w1=15.975851165521329\n",
      "SubGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326745, w1=15.973725065735591\n",
      "SubGD iter. 381/499: loss=5.310578465997032, w0=72.62673267326745, w1=15.972115405014504\n",
      "SubGD iter. 382/499: loss=5.310574764557549, w0=72.62673267326745, w1=15.970505744293417\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633676, w1=15.974974444100821\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326745, w1=15.972848344315084\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326745, w1=15.971238683593997\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633676, w1=15.975707383401401\n",
      "SubGD iter. 387/499: loss=5.310624875040678, w0=72.62673267326745, w1=15.973581283615664\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326745, w1=15.971971622894577\n",
      "SubGD iter. 389/499: loss=5.310574841346151, w0=72.63366336633676, w1=15.976440322701981\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326745, w1=15.974314222916243\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326745, w1=15.972704562195156\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326745, w1=15.97109490147407\n",
      "SubGD iter. 393/499: loss=5.310580438210213, w0=72.63366336633676, w1=15.975563601281474\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326745, w1=15.973437501495736\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326745, w1=15.97182784077465\n",
      "SubGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633676, w1=15.976296540582053\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326745, w1=15.974170440796316\n",
      "SubGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326745, w1=15.972560780075229\n",
      "SubGD iter. 399/499: loss=5.310575788704322, w0=72.62673267326745, w1=15.970951119354142\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633676, w1=15.975419819161546\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326745, w1=15.973293719375809\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326745, w1=15.971684058654722\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633676, w1=15.976152758462126\n",
      "SubGD iter. 404/499: loss=5.310626227771851, w0=72.62673267326745, w1=15.974026658676388\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326745, w1=15.972416997955301\n",
      "SubGD iter. 406/499: loss=5.310575458075135, w0=72.62673267326745, w1=15.970807337234215\n",
      "SubGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633676, w1=15.975276037041619\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326745, w1=15.973149937255881\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326745, w1=15.971540276534794\n",
      "SubGD iter. 410/499: loss=5.3105775949995735, w0=72.63366336633676, w1=15.976008976342198\n",
      "SubGD iter. 411/499: loss=5.310625791064516, w0=72.62673267326745, w1=15.97388287655646\n",
      "SubGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326745, w1=15.972273215835374\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326745, w1=15.970663555114287\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633676, w1=15.975132254921691\n",
      "SubGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326745, w1=15.973006155135954\n",
      "SubGD iter. 416/499: loss=5.310576812851097, w0=72.62673267326745, w1=15.971396494414867\n",
      "SubGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633676, w1=15.975865194222271\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326745, w1=15.973739094436533\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326745, w1=15.972129433715446\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326745, w1=15.97051977299436\n",
      "SubGD iter. 421/499: loss=5.310584109748107, w0=72.63366336633676, w1=15.974988472801764\n",
      "SubGD iter. 422/499: loss=5.310622691504004, w0=72.62673267326745, w1=15.972862373016026\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326745, w1=15.97125271229494\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633676, w1=15.975721412102343\n",
      "SubGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326745, w1=15.973595312316606\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326745, w1=15.971985651595519\n",
      "SubGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633676, w1=15.976454351402923\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326745, w1=15.974328251617186\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326745, w1=15.972718590896099\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326745, w1=15.971108930175012\n",
      "SubGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633676, w1=15.975577629982416\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326745, w1=15.973451530196678\n",
      "SubGD iter. 433/499: loss=5.310577836997871, w0=72.62673267326745, w1=15.971841869475591\n",
      "SubGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633676, w1=15.976310569282996\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326745, w1=15.974184469497258\n",
      "SubGD iter. 436/499: loss=5.3105795224030174, w0=72.62673267326745, w1=15.972574808776171\n",
      "SubGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326745, w1=15.970965148055084\n",
      "SubGD iter. 438/499: loss=5.310581266537468, w0=72.63366336633676, w1=15.975433847862488\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326745, w1=15.973307748076751\n",
      "SubGD iter. 440/499: loss=5.310577506368683, w0=72.62673267326745, w1=15.971698087355664\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633676, w1=15.976166787163068\n",
      "SubGD iter. 442/499: loss=5.31062627038102, w0=72.62673267326745, w1=15.97404068737733\n",
      "SubGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326745, w1=15.972431026656244\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326745, w1=15.970821365935157\n",
      "SubGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633676, w1=15.975290065742561\n",
      "SubGD iter. 446/499: loss=5.310623607527843, w0=72.62673267326745, w1=15.973163965956823\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326745, w1=15.971554305235736\n",
      "SubGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633676, w1=15.97602300504314\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326745, w1=15.973896905257403\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326745, w1=15.972287244536316\n",
      "SubGD iter. 451/499: loss=5.310575159705161, w0=72.62673267326745, w1=15.97067758381523\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633676, w1=15.975146283622633\n",
      "SubGD iter. 453/499: loss=5.310623170820507, w0=72.62673267326745, w1=15.973020183836896\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326745, w1=15.971410523115809\n",
      "SubGD iter. 455/499: loss=5.310578423326828, w0=72.63366336633676, w1=15.975879222923213\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326745, w1=15.973753123137476\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326745, w1=15.972143462416389\n",
      "SubGD iter. 458/499: loss=5.310574829075974, w0=72.62673267326745, w1=15.970533801695302\n",
      "SubGD iter. 459/499: loss=5.3105840201908885, w0=72.63366336633676, w1=15.975002501502706\n",
      "SubGD iter. 460/499: loss=5.310622734113172, w0=72.62673267326745, w1=15.972876401716968\n",
      "SubGD iter. 461/499: loss=5.310576514481122, w0=72.62673267326745, w1=15.971266740995882\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633676, w1=15.975735440803286\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326745, w1=15.973609341017548\n",
      "SubGD iter. 464/499: loss=5.310578199886271, w0=72.62673267326745, w1=15.971999680296461\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633676, w1=15.976468380103865\n",
      "SubGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326745, w1=15.974342280318128\n",
      "SubGD iter. 467/499: loss=5.310579885291417, w0=72.62673267326745, w1=15.972732619597041\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326745, w1=15.971122958875954\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633676, w1=15.975591658683358\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326745, w1=15.97346555889762\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326745, w1=15.971855898176534\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633676, w1=15.976324597983938\n",
      "SubGD iter. 473/499: loss=5.310626749697522, w0=72.62673267326745, w1=15.9741984981982\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326745, w1=15.972588837477113\n",
      "SubGD iter. 475/499: loss=5.310575853222749, w0=72.62673267326745, w1=15.970979176756027\n",
      "SubGD iter. 476/499: loss=5.310581176980248, w0=72.63366336633676, w1=15.97544787656343\n",
      "SubGD iter. 477/499: loss=5.310624086844346, w0=72.62673267326745, w1=15.973321776777693\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326745, w1=15.971712116056606\n",
      "SubGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633676, w1=15.97618081586401\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326745, w1=15.974054716078273\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326745, w1=15.972445055357186\n",
      "SubGD iter. 482/499: loss=5.310575522593562, w0=72.62673267326745, w1=15.970835394636099\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633676, w1=15.975304094443503\n",
      "SubGD iter. 484/499: loss=5.3106236501370105, w0=72.62673267326745, w1=15.973177994657766\n",
      "SubGD iter. 485/499: loss=5.310577207998708, w0=72.62673267326745, w1=15.971568333936679\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633676, w1=15.976037033744083\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326745, w1=15.973910933958345\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326745, w1=15.972301273237258\n",
      "SubGD iter. 489/499: loss=5.310575191964373, w0=72.62673267326745, w1=15.970691612516172\n",
      "SubGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633676, w1=15.975160312323576\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326745, w1=15.973034212537838\n",
      "SubGD iter. 492/499: loss=5.310576877369523, w0=72.62673267326745, w1=15.971424551816751\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633676, w1=15.975893251624155\n",
      "SubGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326745, w1=15.973767151838418\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326745, w1=15.972157491117331\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326745, w1=15.970547830396244\n",
      "SubGD iter. 497/499: loss=5.310583930633669, w0=72.63366336633676, w1=15.975016530203648\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326745, w1=15.97289043041791\n",
      "SubGD iter. 499/499: loss=5.310576546740334, w0=72.62673267326745, w1=15.971280769696824\n",
      "SubGD: execution time=0.039 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e7b774cf43416386c2b5ba201af836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        for y_batch, x_batch in batch_iter(y, tx, batch_size):\n",
    "            grad = compute_subgradient_mae(y_batch, x_batch, w)\n",
    "            w = w - gamma*grad\n",
    "    \n",
    "        loss = compute_loss(y, tx, w, mse=False)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=73.36780585492637, w0=0.7, w1=-0.5355196828262054\n",
      "SubSGD iter. 1/499: loss=72.66780585492639, w0=1.4, w1=-0.2653812325365354\n",
      "SubSGD iter. 2/499: loss=71.96780585492638, w0=2.0999999999999996, w1=-0.9994955555375244\n",
      "SubSGD iter. 3/499: loss=71.26780585492638, w0=2.8, w1=-0.34743865809506913\n",
      "SubSGD iter. 4/499: loss=70.56780585492638, w0=3.5, w1=-0.1510940645778411\n",
      "SubSGD iter. 5/499: loss=69.86780585492637, w0=4.2, w1=-0.5470703077996735\n",
      "SubSGD iter. 6/499: loss=69.16780585492639, w0=4.9, w1=-0.2815866055754432\n",
      "SubSGD iter. 7/499: loss=68.46780585492637, w0=5.6000000000000005, w1=0.3815577114676045\n",
      "SubSGD iter. 8/499: loss=67.76780585492638, w0=6.300000000000001, w1=1.2377797639721426\n",
      "SubSGD iter. 9/499: loss=67.06780585492639, w0=7.000000000000001, w1=0.3489479398056492\n",
      "SubSGD iter. 10/499: loss=66.36780585492637, w0=7.700000000000001, w1=-0.0009899916746680115\n",
      "SubSGD iter. 11/499: loss=65.66780585492639, w0=8.4, w1=-0.5256229163864075\n",
      "SubSGD iter. 12/499: loss=64.96780585492638, w0=9.1, w1=-0.9215991596082398\n",
      "SubSGD iter. 13/499: loss=64.26780585492638, w0=9.799999999999999, w1=-0.7782685690137233\n",
      "SubSGD iter. 14/499: loss=63.56780585492638, w0=10.499999999999998, w1=0.23416976885269747\n",
      "SubSGD iter. 15/499: loss=62.867805854926374, w0=11.199999999999998, w1=1.0903918213572354\n",
      "SubSGD iter. 16/499: loss=62.167805854926385, w0=11.899999999999997, w1=0.7051076810291075\n",
      "SubSGD iter. 17/499: loss=61.46780585492638, w0=12.599999999999996, w1=1.873438481546576\n",
      "SubSGD iter. 18/499: loss=60.76780585492638, w0=13.299999999999995, w1=1.5235005500662588\n",
      "SubSGD iter. 19/499: loss=60.067805854926384, w0=13.999999999999995, w1=2.248520740147063\n",
      "SubSGD iter. 20/499: loss=59.367805854926374, w0=14.699999999999994, w1=2.966684064178251\n",
      "SubSGD iter. 21/499: loss=58.66780585492638, w0=15.399999999999993, w1=2.5964159736494543\n",
      "SubSGD iter. 22/499: loss=57.96780585492638, w0=16.099999999999994, w1=2.251238885924456\n",
      "SubSGD iter. 23/499: loss=57.26780585492638, w0=16.799999999999994, w1=2.0504893488752045\n",
      "SubSGD iter. 24/499: loss=56.567805854926384, w0=17.499999999999993, w1=2.7755095389560087\n",
      "SubSGD iter. 25/499: loss=55.867805854926374, w0=18.199999999999992, w1=2.4903350444067143\n",
      "SubSGD iter. 26/499: loss=55.167805854926385, w0=18.89999999999999, w1=2.701499438972918\n",
      "SubSGD iter. 27/499: loss=54.46780585492639, w0=19.59999999999999, w1=2.2704887542243046\n",
      "SubSGD iter. 28/499: loss=53.767805854926394, w0=20.29999999999999, w1=1.8447026069335655\n",
      "SubSGD iter. 29/499: loss=53.067805854926384, w0=20.99999999999999, w1=1.4915868553662508\n",
      "SubSGD iter. 30/499: loss=52.36780585492639, w0=21.69999999999999, w1=1.657393114933748\n",
      "SubSGD iter. 31/499: loss=51.667805854926385, w0=22.399999999999988, w1=2.0809343679877634\n",
      "SubSGD iter. 32/499: loss=50.9678058549264, w0=23.099999999999987, w1=1.2448955107149478\n",
      "SubSGD iter. 33/499: loss=50.267805854926394, w0=23.799999999999986, w1=1.8588099240925309\n",
      "SubSGD iter. 34/499: loss=49.56780585492639, w0=24.499999999999986, w1=1.476683939542196\n",
      "SubSGD iter. 35/499: loss=48.867805854926395, w0=25.199999999999985, w1=1.5905042663565088\n",
      "SubSGD iter. 36/499: loss=48.167805854926385, w0=25.899999999999984, w1=0.7718458965508807\n",
      "SubSGD iter. 37/499: loss=47.46780585492639, w0=26.599999999999984, w1=1.7299809874526877\n",
      "SubSGD iter. 38/499: loss=46.767805854926394, w0=27.299999999999983, w1=1.5292314504034363\n",
      "SubSGD iter. 39/499: loss=46.06780585492639, w0=27.999999999999982, w1=2.4084037626342427\n",
      "SubSGD iter. 40/499: loss=45.367805854926395, w0=28.69999999999998, w1=2.673188871825011\n",
      "SubSGD iter. 41/499: loss=44.6678058549264, w0=29.39999999999998, w1=1.5472564583238402\n",
      "SubSGD iter. 42/499: loss=43.9678058549264, w0=30.09999999999998, w1=1.1169248698631042\n",
      "SubSGD iter. 43/499: loss=43.2678058549264, w0=30.79999999999998, w1=1.6261107138093567\n",
      "SubSGD iter. 44/499: loss=42.567805854926405, w0=31.49999999999998, w1=0.5885965199958161\n",
      "SubSGD iter. 45/499: loss=41.867805854926395, w0=32.19999999999998, w1=0.5632987546069748\n",
      "SubSGD iter. 46/499: loss=41.167805854926385, w0=32.899999999999984, w1=-0.38020772893014554\n",
      "SubSGD iter. 47/499: loss=40.46780585492639, w0=33.59999999999999, w1=-1.174577073420926\n",
      "SubSGD iter. 48/499: loss=39.767805854926394, w0=34.29999999999999, w1=-0.08195255909513155\n",
      "SubSGD iter. 49/499: loss=39.067805854926384, w0=34.99999999999999, w1=1.0106719552306629\n",
      "SubSGD iter. 50/499: loss=38.36780585492638, w0=35.699999999999996, w1=0.9782429474633031\n",
      "SubSGD iter. 51/499: loss=37.66780585492638, w0=36.4, w1=0.4512859116944552\n",
      "SubSGD iter. 52/499: loss=36.967805854926375, w0=37.1, w1=0.79925295808487\n",
      "SubSGD iter. 53/499: loss=36.26780585492637, w0=37.800000000000004, w1=1.3402100100560121\n",
      "SubSGD iter. 54/499: loss=35.56780585492637, w0=38.50000000000001, w1=2.880272348443198\n",
      "SubSGD iter. 55/499: loss=34.86780585492637, w0=39.20000000000001, w1=4.086288638213645\n",
      "SubSGD iter. 56/499: loss=34.167805854926364, w0=39.90000000000001, w1=3.720367907483248\n",
      "SubSGD iter. 57/499: loss=33.46780585492636, w0=40.600000000000016, w1=3.1934108717144003\n",
      "SubSGD iter. 58/499: loss=32.76780585492636, w0=41.30000000000002, w1=4.151545962616208\n",
      "SubSGD iter. 59/499: loss=32.06780585492635, w0=42.00000000000002, w1=4.775034084902385\n",
      "SubSGD iter. 60/499: loss=31.36780585492635, w0=42.700000000000024, w1=4.907357611256224\n",
      "SubSGD iter. 61/499: loss=30.667805854926343, w0=43.40000000000003, w1=4.107526877193514\n",
      "SubSGD iter. 62/499: loss=29.967805854926343, w0=44.10000000000003, w1=3.5555554452558353\n",
      "SubSGD iter. 63/499: loss=29.267805854926348, w0=44.80000000000003, w1=2.8688608097496986\n",
      "SubSGD iter. 64/499: loss=28.567805854926338, w0=45.500000000000036, w1=3.4827752231272817\n",
      "SubSGD iter. 65/499: loss=27.867805854926342, w0=46.20000000000004, w1=2.646736365854466\n",
      "SubSGD iter. 66/499: loss=27.16780585492634, w0=46.90000000000004, w1=1.907403566410522\n",
      "SubSGD iter. 67/499: loss=26.467805854926336, w0=47.600000000000044, w1=1.9497301230140127\n",
      "SubSGD iter. 68/499: loss=25.76780585492633, w0=48.30000000000005, w1=2.2278570918157006\n",
      "SubSGD iter. 69/499: loss=25.067805854926327, w0=49.00000000000005, w1=3.4338733815861477\n",
      "SubSGD iter. 70/499: loss=24.367805854926324, w0=49.70000000000005, w1=3.0835364482177567\n",
      "SubSGD iter. 71/499: loss=23.66780585492632, w0=50.400000000000055, w1=3.523460891101939\n",
      "SubSGD iter. 72/499: loss=22.96863206641775, w0=51.10000000000006, w1=2.9024093175313457\n",
      "SubSGD iter. 73/499: loss=22.27435016789402, w0=51.80000000000006, w1=3.152007780218524\n",
      "SubSGD iter. 74/499: loss=21.572524767865257, w0=52.500000000000064, w1=3.765922193596107\n",
      "SubSGD iter. 75/499: loss=20.86800149542191, w0=53.20000000000007, w1=4.858546707921901\n",
      "SubSGD iter. 76/499: loss=20.167805854926304, w0=53.90000000000007, w1=5.714768760426439\n",
      "SubSGD iter. 77/499: loss=19.468849206169775, w0=54.60000000000007, w1=6.210395020895272\n",
      "SubSGD iter. 78/499: loss=18.769453477635622, w0=55.300000000000075, w1=6.867577724090257\n",
      "SubSGD iter. 79/499: loss=18.09784436391123, w0=56.00000000000008, w1=5.683575523696366\n",
      "SubSGD iter. 80/499: loss=17.39559760385755, w0=56.70000000000008, w1=6.299962472533616\n",
      "SubSGD iter. 81/499: loss=16.74245958064011, w0=57.400000000000084, w1=5.876723047761101\n",
      "SubSGD iter. 82/499: loss=16.076369946262012, w0=58.10000000000009, w1=5.883150928639448\n",
      "SubSGD iter. 83/499: loss=15.507131142910021, w0=58.80000000000009, w1=5.452819340178712\n",
      "SubSGD iter. 84/499: loss=14.89955783277731, w0=59.50000000000009, w1=5.596149930773228\n",
      "SubSGD iter. 85/499: loss=15.39937658407431, w0=58.80000000000009, w1=6.2767491084293106\n",
      "SubSGD iter. 86/499: loss=14.806839817605269, w0=59.50000000000009, w1=6.09258833011099\n",
      "SubSGD iter. 87/499: loss=14.151668307635106, w0=60.200000000000095, w1=6.542768076317284\n",
      "SubSGD iter. 88/499: loss=13.550579540705797, w0=60.9000000000001, w1=6.808251778541514\n",
      "SubSGD iter. 89/499: loss=12.982292013356359, w0=61.6000000000001, w1=7.004596372058742\n",
      "SubSGD iter. 90/499: loss=12.664205010416783, w0=62.300000000000104, w1=6.479963447347003\n",
      "SubSGD iter. 91/499: loss=12.02022943965115, w0=63.00000000000011, w1=7.092955944530562\n",
      "SubSGD iter. 92/499: loss=12.221816445441675, w0=62.300000000000104, w1=8.036462428067683\n",
      "SubSGD iter. 93/499: loss=11.793107635094236, w0=63.00000000000011, w1=7.795482513401034\n",
      "SubSGD iter. 94/499: loss=11.157014354427542, w0=63.70000000000011, w1=8.410143494531422\n",
      "SubSGD iter. 95/499: loss=11.451819065208683, w0=63.00000000000011, w1=9.218257235573354\n",
      "SubSGD iter. 96/499: loss=12.01164009439309, w0=62.300000000000104, w1=9.232012431548844\n",
      "SubSGD iter. 97/499: loss=11.424247413187057, w0=63.00000000000011, w1=9.37534302214336\n",
      "SubSGD iter. 98/499: loss=10.842952768043409, w0=63.70000000000011, w1=9.625624138605417\n",
      "SubSGD iter. 99/499: loss=10.228677459976451, w0=64.4000000000001, w1=10.134809982551669\n",
      "SubSGD iter. 100/499: loss=9.765750322096219, w0=65.10000000000011, w1=10.102380974784309\n",
      "SubSGD iter. 101/499: loss=8.95005364098308, w0=65.80000000000011, w1=11.46775042744792\n",
      "SubSGD iter. 102/499: loss=8.62897058900645, w0=66.50000000000011, w1=11.114634675880605\n",
      "SubSGD iter. 103/499: loss=8.832927447057754, w0=65.80000000000011, w1=11.95605318560655\n",
      "SubSGD iter. 104/499: loss=8.177296104252727, w0=66.50000000000011, w1=13.013968492434108\n",
      "SubSGD iter. 105/499: loss=8.674557960249034, w0=65.80000000000011, w1=13.425043676976333\n",
      "SubSGD iter. 106/499: loss=8.199476972567002, w0=66.50000000000011, w1=12.85203359396171\n",
      "SubSGD iter. 107/499: loss=7.603848363315925, w0=67.20000000000012, w1=13.864471931828131\n",
      "SubSGD iter. 108/499: loss=8.075721394960512, w0=66.50000000000011, w1=14.144400924264595\n",
      "SubSGD iter. 109/499: loss=7.589286968400929, w0=67.20000000000012, w1=14.017386479036569\n",
      "SubSGD iter. 110/499: loss=7.056607011845289, w0=67.90000000000012, w1=15.02982481690299\n",
      "SubSGD iter. 111/499: loss=7.508965066992657, w0=67.20000000000012, w1=15.314999311452283\n",
      "SubSGD iter. 112/499: loss=7.101125067334986, w0=67.90000000000012, w1=14.496340941646654\n",
      "SubSGD iter. 113/499: loss=6.620096476865434, w0=68.60000000000012, w1=15.24232659523405\n",
      "SubSGD iter. 114/499: loss=6.289555796794467, w0=69.30000000000013, w1=14.811315910485437\n",
      "SubSGD iter. 115/499: loss=5.881633127647935, w0=70.00000000000013, w1=15.434804032771615\n",
      "SubSGD iter. 116/499: loss=5.687215982156952, w0=70.70000000000013, w1=15.004472444310878\n",
      "SubSGD iter. 117/499: loss=5.537948062058692, w0=71.40000000000013, w1=14.749040563209308\n",
      "SubSGD iter. 118/499: loss=5.689380488304718, w0=70.70000000000013, w1=14.993067387918458\n",
      "SubSGD iter. 119/499: loss=5.6736534422074, w0=71.40000000000013, w1=14.193236653855749\n",
      "SubSGD iter. 120/499: loss=5.783586355481282, w0=72.10000000000014, w1=13.33939038343048\n",
      "SubSGD iter. 121/499: loss=5.689425770094206, w0=71.40000000000013, w1=14.13375972792126\n",
      "SubSGD iter. 122/499: loss=5.404297659909046, w0=72.10000000000014, w1=15.012932040152068\n",
      "SubSGD iter. 123/499: loss=5.543160783561759, w0=71.40000000000013, w1=14.724434775594652\n",
      "SubSGD iter. 124/499: loss=5.3744473048001495, w0=72.10000000000014, w1=15.24861532704637\n",
      "SubSGD iter. 125/499: loss=5.332878860311788, w0=72.80000000000014, w1=15.414068118978996\n",
      "SubSGD iter. 126/499: loss=5.35969646136682, w0=73.50000000000014, w1=16.046342547460686\n",
      "SubSGD iter. 127/499: loss=5.553680978661278, w0=74.20000000000014, w1=16.902564599965224\n",
      "SubSGD iter. 128/499: loss=5.455344491486251, w0=73.50000000000014, w1=17.023069151444265\n",
      "SubSGD iter. 129/499: loss=5.650313582108117, w0=74.20000000000014, w1=17.47324889765056\n",
      "SubSGD iter. 130/499: loss=5.876690625653273, w0=74.90000000000015, w1=17.722847360337738\n",
      "SubSGD iter. 131/499: loss=5.583127391559469, w0=74.20000000000014, w1=17.10646041150049\n",
      "SubSGD iter. 132/499: loss=5.7634139983965635, w0=74.90000000000015, w1=17.121768078212625\n",
      "SubSGD iter. 133/499: loss=5.508794085380061, w0=74.20000000000014, w1=16.46458537501764\n",
      "SubSGD iter. 134/499: loss=5.756338736087821, w0=74.90000000000015, w1=17.0775778722012\n",
      "SubSGD iter. 135/499: loss=5.958834287042214, w0=75.60000000000015, w1=16.727639940720884\n",
      "SubSGD iter. 136/499: loss=6.312463857060955, w0=76.30000000000015, w1=16.924160023499386\n",
      "SubSGD iter. 137/499: loss=6.007953681302756, w0=75.60000000000015, w1=17.044051836987716\n",
      "SubSGD iter. 138/499: loss=6.2637851829236, w0=76.30000000000015, w1=16.644594064601137\n",
      "SubSGD iter. 139/499: loss=5.915889167561159, w0=75.60000000000015, w1=16.374455614311465\n",
      "SubSGD iter. 140/499: loss=6.324183340682332, w0=76.30000000000015, w1=16.987448111495024\n",
      "SubSGD iter. 141/499: loss=6.5182328347688845, w0=77.00000000000016, w1=15.776025223568844\n",
      "SubSGD iter. 142/499: loss=6.165044576691254, w0=76.30000000000015, w1=15.799991063371936\n",
      "SubSGD iter. 143/499: loss=5.85364251644433, w0=75.60000000000015, w1=15.63418480380444\n",
      "SubSGD iter. 144/499: loss=6.14179581599589, w0=76.30000000000015, w1=15.297821846128363\n",
      "SubSGD iter. 145/499: loss=6.52696574848292, w0=77.00000000000016, w1=15.851027762204476\n",
      "SubSGD iter. 146/499: loss=6.1442532071763445, w0=76.30000000000015, w1=15.355401501735644\n",
      "SubSGD iter. 147/499: loss=6.463248367233657, w0=77.00000000000016, w1=14.925069913274907\n",
      "SubSGD iter. 148/499: loss=6.176475558129882, w0=76.30000000000015, w1=14.311155499897325\n",
      "SubSGD iter. 149/499: loss=5.876961047916704, w0=75.60000000000015, w1=14.751350754604296\n",
      "SubSGD iter. 150/499: loss=6.155998105806703, w0=76.30000000000015, w1=15.630598720500467\n",
      "SubSGD iter. 151/499: loss=5.914875454273935, w0=75.60000000000015, w1=16.364713043501457\n",
      "SubSGD iter. 152/499: loss=6.158249946982735, w0=76.30000000000015, w1=15.67801840799532\n",
      "SubSGD iter. 153/499: loss=6.4674908454057105, w0=77.00000000000016, w1=14.466595520069141\n",
      "SubSGD iter. 154/499: loss=6.21383524313158, w0=76.30000000000015, w1=14.016415773862848\n",
      "SubSGD iter. 155/499: loss=5.852822271283416, w0=75.60000000000015, w1=15.106210796936068\n",
      "SubSGD iter. 156/499: loss=5.623551314850729, w0=74.90000000000015, w1=15.653373099169938\n",
      "SubSGD iter. 157/499: loss=5.478038361400489, w0=74.20000000000014, w1=16.038657239498065\n",
      "SubSGD iter. 158/499: loss=5.37323612387221, w0=73.50000000000014, w1=16.32120505858429\n",
      "SubSGD iter. 159/499: loss=5.3133789034051615, w0=72.80000000000014, w1=16.060492837202425\n",
      "SubSGD iter. 160/499: loss=5.366376907420838, w0=72.10000000000014, w1=15.335472647121621\n",
      "SubSGD iter. 161/499: loss=5.362267285300765, w0=72.80000000000014, w1=14.985534715641304\n",
      "SubSGD iter. 162/499: loss=5.354700978027483, w0=72.10000000000014, w1=15.494035833381137\n",
      "SubSGD iter. 163/499: loss=5.384144892449435, w0=72.80000000000014, w1=14.797451082140373\n",
      "SubSGD iter. 164/499: loss=5.35980356277161, w0=73.50000000000014, w1=16.036767383118374\n",
      "SubSGD iter. 165/499: loss=5.476192806811572, w0=74.20000000000014, w1=16.002076523776903\n",
      "SubSGD iter. 166/499: loss=5.36640130741904, w0=73.50000000000014, w1=16.219333527956653\n",
      "SubSGD iter. 167/499: loss=5.377783290443609, w0=72.80000000000014, w1=16.87621449570962\n",
      "SubSGD iter. 168/499: loss=5.339115610805698, w0=72.10000000000014, w1=16.13022884212222\n",
      "SubSGD iter. 169/499: loss=5.397617877858805, w0=72.80000000000014, w1=17.009476808018395\n",
      "SubSGD iter. 170/499: loss=5.447790405251743, w0=73.50000000000014, w1=16.977047800251036\n",
      "SubSGD iter. 171/499: loss=5.494561264425656, w0=72.80000000000014, w1=17.601863900061982\n",
      "SubSGD iter. 172/499: loss=5.541000886162255, w0=73.50000000000014, w1=17.501586800821773\n",
      "SubSGD iter. 173/499: loss=5.389240565224003, w0=72.80000000000014, w1=16.95319229706396\n",
      "SubSGD iter. 174/499: loss=5.422061950625141, w0=72.10000000000014, w1=17.13735307538228\n",
      "SubSGD iter. 175/499: loss=5.616563960760138, w0=71.40000000000013, w1=17.68932450731996\n",
      "SubSGD iter. 176/499: loss=5.695930366857893, w0=72.10000000000014, w1=18.33547174070718\n",
      "SubSGD iter. 177/499: loss=5.697404412471988, w0=71.40000000000013, w1=18.000243685403927\n",
      "SubSGD iter. 178/499: loss=5.450113184332425, w0=72.10000000000014, w1=17.303658934163163\n",
      "SubSGD iter. 179/499: loss=5.520570871885175, w0=72.80000000000014, w1=17.73117601938058\n",
      "SubSGD iter. 180/499: loss=5.789739831902467, w0=73.50000000000014, w1=18.58739807188512\n",
      "SubSGD iter. 181/499: loss=5.8258399753974235, w0=72.80000000000014, w1=18.858878253867697\n",
      "SubSGD iter. 182/499: loss=5.667970956246886, w0=73.50000000000014, w1=18.119545454423754\n",
      "SubSGD iter. 183/499: loss=5.659970229203231, w0=72.80000000000014, w1=18.303706232742073\n",
      "SubSGD iter. 184/499: loss=5.59550876134499, w0=73.50000000000014, w1=17.779073308030334\n",
      "SubSGD iter. 185/499: loss=5.539947324104142, w0=74.20000000000014, w1=14.405929630440657\n",
      "SubSGD iter. 186/499: loss=5.586488582100724, w0=73.50000000000014, w1=13.65994397685326\n",
      "SubSGD iter. 187/499: loss=5.614561702567078, w0=74.20000000000014, w1=13.904256700562394\n",
      "SubSGD iter. 188/499: loss=5.390529211783697, w0=73.50000000000014, w1=14.994051723635614\n",
      "SubSGD iter. 189/499: loss=5.466832712809417, w0=74.20000000000014, w1=15.547257639711727\n",
      "SubSGD iter. 190/499: loss=5.757898537701963, w0=74.90000000000015, w1=17.087319978098915\n",
      "SubSGD iter. 191/499: loss=5.98408473837935, w0=75.60000000000015, w1=16.903854199048496\n",
      "SubSGD iter. 192/499: loss=5.8942202144700095, w0=74.90000000000015, w1=17.79268602321499\n",
      "SubSGD iter. 193/499: loss=5.709101408890736, w0=74.20000000000014, w1=17.74052567768528\n",
      "SubSGD iter. 194/499: loss=5.770738600568147, w0=74.90000000000015, w1=17.167515594670657\n",
      "SubSGD iter. 195/499: loss=5.997738210882846, w0=75.60000000000015, w1=16.98404981562024\n",
      "SubSGD iter. 196/499: loss=5.770849876404325, w0=74.90000000000015, w1=17.168210593938557\n",
      "SubSGD iter. 197/499: loss=5.5537880670532775, w0=74.20000000000014, w1=16.90342548474779\n",
      "SubSGD iter. 198/499: loss=5.494735513014985, w0=73.50000000000014, w1=17.248602572472787\n",
      "SubSGD iter. 199/499: loss=5.38036721921618, w0=72.80000000000014, w1=16.89357509494296\n",
      "SubSGD iter. 200/499: loss=5.545617401375387, w0=73.50000000000014, w1=17.52584952342465\n",
      "SubSGD iter. 201/499: loss=5.436582217336187, w0=72.80000000000014, w1=17.265137302042785\n",
      "SubSGD iter. 202/499: loss=5.529611487604171, w0=72.10000000000014, w1=17.705332556749756\n",
      "SubSGD iter. 203/499: loss=5.558786123955778, w0=72.80000000000014, w1=17.901852639528258\n",
      "SubSGD iter. 204/499: loss=5.553159811080472, w0=73.50000000000014, w1=17.56548968185218\n",
      "SubSGD iter. 205/499: loss=5.5829256434353285, w0=72.80000000000014, w1=18.005178086937367\n",
      "SubSGD iter. 206/499: loss=5.395784038806816, w0=72.10000000000014, w1=16.948776855558478\n",
      "SubSGD iter. 207/499: loss=5.430616236859304, w0=72.80000000000014, w1=17.226903824360164\n",
      "SubSGD iter. 208/499: loss=5.446908695651505, w0=73.50000000000014, w1=16.971471943258592\n",
      "SubSGD iter. 209/499: loss=5.448535364937821, w0=72.80000000000014, w1=17.34174003378739\n",
      "SubSGD iter. 210/499: loss=5.542134810346773, w0=73.50000000000014, w1=17.507546293354885\n",
      "SubSGD iter. 211/499: loss=5.583393987136863, w0=74.20000000000014, w1=17.108088520968305\n",
      "SubSGD iter. 212/499: loss=5.63078838282882, w0=73.50000000000014, w1=17.948116092428116\n",
      "SubSGD iter. 213/499: loss=5.630185512385238, w0=74.20000000000014, w1=17.375106009413493\n",
      "SubSGD iter. 214/499: loss=5.688831919728284, w0=74.90000000000015, w1=16.580736664922714\n",
      "SubSGD iter. 215/499: loss=5.528281380562716, w0=74.20000000000014, w1=16.661550970411426\n",
      "SubSGD iter. 216/499: loss=5.717441031351539, w0=74.90000000000015, w1=16.81444472752601\n",
      "SubSGD iter. 217/499: loss=5.6896802292004285, w0=74.20000000000014, w1=17.65447229898582\n",
      "SubSGD iter. 218/499: loss=5.727302485220973, w0=74.90000000000015, w1=16.883649232917634\n",
      "SubSGD iter. 219/499: loss=5.477401113401398, w0=74.20000000000014, w1=16.027427180413095\n",
      "SubSGD iter. 220/499: loss=5.380888405877393, w0=73.50000000000014, w1=15.186237012188453\n",
      "SubSGD iter. 221/499: loss=5.312765811997127, w0=72.80000000000014, w1=15.843117979941416\n",
      "SubSGD iter. 222/499: loss=5.385991313313352, w0=73.50000000000014, w1=16.459504928778667\n",
      "SubSGD iter. 223/499: loss=5.348128247905673, w0=72.80000000000014, w1=16.66433844865389\n",
      "SubSGD iter. 224/499: loss=5.575009141592955, w0=73.50000000000014, w1=17.67677678652031\n",
      "SubSGD iter. 225/499: loss=5.556986791827735, w0=72.80000000000014, w1=17.89403379070006\n",
      "SubSGD iter. 226/499: loss=5.664844324875065, w0=73.50000000000014, w1=18.105198185266264\n",
      "SubSGD iter. 227/499: loss=5.475926914718557, w0=72.80000000000014, w1=17.500666079412106\n",
      "SubSGD iter. 228/499: loss=5.374486076937548, w0=72.10000000000014, w1=16.7756458893313\n",
      "SubSGD iter. 229/499: loss=5.5972693896492025, w0=71.40000000000013, w1=17.611684746604116\n",
      "SubSGD iter. 230/499: loss=5.42985281240504, w0=72.10000000000014, w1=17.185898599313376\n",
      "SubSGD iter. 231/499: loss=5.563434912000092, w0=71.40000000000013, w1=17.46582759174984\n",
      "SubSGD iter. 232/499: loss=5.831914621223848, w0=70.70000000000013, w1=17.876902776292066\n",
      "SubSGD iter. 233/499: loss=5.699828485894736, w0=71.40000000000013, w1=18.009226302645907\n",
      "SubSGD iter. 234/499: loss=5.87099177823741, w0=72.10000000000014, w1=18.88847426854208\n",
      "SubSGD iter. 235/499: loss=6.266387069137161, w0=72.80000000000014, w1=20.013604228672385\n",
      "SubSGD iter. 236/499: loss=6.660951584703261, w0=72.10000000000014, w1=20.8496430859452\n",
      "SubSGD iter. 237/499: loss=6.880985626731967, w0=71.40000000000013, w1=21.134817580494495\n",
      "SubSGD iter. 238/499: loss=6.5406484991254965, w0=72.10000000000014, w1=20.591801119129396\n",
      "SubSGD iter. 239/499: loss=6.077160407363003, w0=71.40000000000013, w1=19.226431666465785\n",
      "SubSGD iter. 240/499: loss=6.326055502724817, w0=72.10000000000014, w1=20.10567963236196\n",
      "SubSGD iter. 241/499: loss=6.161527684890535, w0=71.40000000000013, w1=19.453622734919502\n",
      "SubSGD iter. 242/499: loss=5.9431268756262705, w0=72.10000000000014, w1=19.103684803439187\n",
      "SubSGD iter. 243/499: loss=5.968172643148681, w0=72.80000000000014, w1=19.269491063006683\n",
      "SubSGD iter. 244/499: loss=5.807592981845906, w0=73.50000000000014, w1=18.6503499075923\n",
      "SubSGD iter. 245/499: loss=5.620371367055233, w0=72.80000000000014, w1=18.15472364712347\n",
      "SubSGD iter. 246/499: loss=5.595522222813038, w0=73.50000000000014, w1=17.7791378088369\n",
      "SubSGD iter. 247/499: loss=5.521710406780447, w0=74.20000000000014, w1=16.59513560844301\n",
      "SubSGD iter. 248/499: loss=5.66635392742464, w0=74.90000000000015, w1=16.35415569377636\n",
      "SubSGD iter. 249/499: loss=5.47416227789437, w0=74.20000000000014, w1=15.261531179450564\n",
      "SubSGD iter. 250/499: loss=5.4108801885735005, w0=73.50000000000014, w1=14.731830205388448\n",
      "SubSGD iter. 251/499: loss=5.506521624999292, w0=74.20000000000014, w1=14.707864365585356\n",
      "SubSGD iter. 252/499: loss=5.763275553182569, w0=74.90000000000015, w1=13.968531566141412\n",
      "SubSGD iter. 253/499: loss=5.9253879204251385, w0=75.60000000000015, w1=14.31299981301188\n",
      "SubSGD iter. 254/499: loss=5.6322963157283406, w0=74.90000000000015, w1=15.15302738447169\n",
      "SubSGD iter. 255/499: loss=5.473839240094947, w0=74.20000000000014, w1=15.272919197960022\n",
      "SubSGD iter. 256/499: loss=5.363429397961625, w0=73.50000000000014, w1=15.712607603045207\n",
      "SubSGD iter. 257/499: loss=5.475101261762221, w0=74.20000000000014, w1=15.978091305269437\n",
      "SubSGD iter. 258/499: loss=5.36193995700917, w0=73.50000000000014, w1=15.845767778915599\n",
      "SubSGD iter. 259/499: loss=5.501399212960439, w0=74.20000000000014, w1=16.38672483088674\n",
      "SubSGD iter. 260/499: loss=5.683570952925592, w0=74.90000000000015, w1=16.530055421481258\n",
      "SubSGD iter. 261/499: loss=5.477170342313343, w0=74.20000000000014, w1=15.16730402608951\n",
      "SubSGD iter. 262/499: loss=5.626986271252402, w0=74.90000000000015, w1=15.780296523273071\n",
      "SubSGD iter. 263/499: loss=5.883159973238795, w0=75.60000000000015, w1=16.0457802254973\n",
      "SubSGD iter. 264/499: loss=5.649766972573429, w0=74.90000000000015, w1=16.155954342378042\n",
      "SubSGD iter. 265/499: loss=5.499020089007363, w0=74.20000000000014, w1=14.793202946986295\n",
      "SubSGD iter. 266/499: loss=5.6482443240434215, w0=74.90000000000015, w1=14.958655738918921\n",
      "SubSGD iter. 267/499: loss=5.566738300436364, w0=74.20000000000014, w1=14.197298788299852\n",
      "SubSGD iter. 268/499: loss=5.504954286591677, w0=73.50000000000014, w1=14.018946666920037\n",
      "SubSGD iter. 269/499: loss=5.6155890851816395, w0=72.80000000000014, w1=13.595405413866022\n",
      "SubSGD iter. 270/499: loss=5.5000436935333745, w0=73.50000000000014, w1=14.045585160072315\n",
      "SubSGD iter. 271/499: loss=5.558204315465234, w0=72.80000000000014, w1=13.816463467785557\n",
      "SubSGD iter. 272/499: loss=5.465494814806381, w0=73.50000000000014, w1=14.256387910669739\n",
      "SubSGD iter. 273/499: loss=5.434037842814987, w0=72.80000000000014, w1=14.438251754244908\n",
      "SubSGD iter. 274/499: loss=5.545138702907327, w0=72.10000000000014, w1=14.177539532863044\n",
      "SubSGD iter. 275/499: loss=5.6927484749954305, w0=71.40000000000013, w1=14.12122991781774\n",
      "SubSGD iter. 276/499: loss=5.439649882981129, w0=72.10000000000014, w1=14.764159616581535\n",
      "SubSGD iter. 277/499: loss=5.696528143425309, w0=71.40000000000013, w1=14.10697691338655\n",
      "SubSGD iter. 278/499: loss=5.900255705434466, w0=70.70000000000013, w1=14.050667298341246\n",
      "SubSGD iter. 279/499: loss=6.158185305398618, w0=70.00000000000013, w1=13.994357683295942\n",
      "SubSGD iter. 280/499: loss=6.133319601655065, w0=70.70000000000013, w1=13.297772932055178\n",
      "SubSGD iter. 281/499: loss=5.746806448204155, w0=71.40000000000013, w1=13.930047360536868\n",
      "SubSGD iter. 282/499: loss=5.813115731331923, w0=72.10000000000014, w1=13.251700364562325\n",
      "SubSGD iter. 283/499: loss=5.487960115473369, w0=72.80000000000014, w1=14.131430494394802\n",
      "SubSGD iter. 284/499: loss=5.44085524951971, w0=72.10000000000014, w1=14.756246594205749\n",
      "SubSGD iter. 285/499: loss=5.50210230227853, w0=72.80000000000014, w1=14.059661842964985\n",
      "SubSGD iter. 286/499: loss=5.837283629910565, w0=72.10000000000014, w1=13.179931713132508\n",
      "SubSGD iter. 287/499: loss=5.502222018584035, w0=72.80000000000014, w1=14.059104025363315\n",
      "SubSGD iter. 288/499: loss=5.503907480083438, w0=73.50000000000014, w1=14.024413166021844\n",
      "SubSGD iter. 289/499: loss=5.540299199641973, w0=74.20000000000014, w1=14.402952949378577\n",
      "SubSGD iter. 290/499: loss=5.371954010637169, w0=73.50000000000014, w1=15.379750428320495\n",
      "SubSGD iter. 291/499: loss=5.501861556719089, w0=74.20000000000014, w1=16.392188766186916\n",
      "SubSGD iter. 292/499: loss=5.3619620314488925, w0=73.50000000000014, w1=15.843794262429105\n",
      "SubSGD iter. 293/499: loss=5.484460681330268, w0=74.20000000000014, w1=14.989947992003836\n",
      "SubSGD iter. 294/499: loss=5.554891524026483, w0=73.50000000000014, w1=13.783931702233389\n",
      "SubSGD iter. 295/499: loss=5.577408842951078, w0=74.20000000000014, w1=14.128399949103857\n",
      "SubSGD iter. 296/499: loss=5.483177242994986, w0=73.50000000000014, w1=14.142155145079347\n",
      "SubSGD iter. 297/499: loss=5.787489619023242, w0=72.80000000000014, w1=13.058279748388046\n",
      "SubSGD iter. 298/499: loss=6.083167617368375, w0=72.10000000000014, w1=12.509885244630235\n",
      "SubSGD iter. 299/499: loss=6.197602371296005, w0=71.40000000000013, w1=12.590699550118947\n",
      "SubSGD iter. 300/499: loss=5.828139329486396, w0=72.10000000000014, w1=13.207086498956196\n",
      "SubSGD iter. 301/499: loss=5.849805211034218, w0=71.40000000000013, w1=13.592370639284324\n",
      "SubSGD iter. 302/499: loss=5.63173372846356, w0=72.10000000000014, w1=13.842651755746381\n",
      "SubSGD iter. 303/499: loss=5.654386625723743, w0=71.40000000000013, w1=14.265891180518896\n",
      "SubSGD iter. 304/499: loss=5.3712921130709015, w0=72.10000000000014, w1=15.278569521871663\n",
      "SubSGD iter. 305/499: loss=5.315735675520553, w0=72.80000000000014, w1=15.745151647838238\n",
      "SubSGD iter. 306/499: loss=5.371240080152136, w0=73.50000000000014, w1=15.39521371635792\n",
      "SubSGD iter. 307/499: loss=5.397532593070567, w0=72.80000000000014, w1=14.694279718923168\n",
      "SubSGD iter. 308/499: loss=5.480571110271562, w0=72.10000000000014, w1=14.495518853104643\n",
      "SubSGD iter. 309/499: loss=5.458557185405307, w0=71.40000000000013, w1=15.439025336641764\n",
      "SubSGD iter. 310/499: loss=5.343806586622759, w0=72.10000000000014, w1=16.2952473891463\n",
      "SubSGD iter. 311/499: loss=5.336285032929143, w0=72.80000000000014, w1=15.35174090560918\n",
      "SubSGD iter. 312/499: loss=5.340709360282981, w0=72.10000000000014, w1=15.878697941378029\n",
      "SubSGD iter. 313/499: loss=5.590039330371808, w0=71.40000000000013, w1=14.515946545986282\n",
      "SubSGD iter. 314/499: loss=5.785592516570067, w0=70.70000000000013, w1=14.509518665107935\n",
      "SubSGD iter. 315/499: loss=6.1144091450026155, w0=70.00000000000013, w1=14.17429060980468\n",
      "SubSGD iter. 316/499: loss=5.708475390812565, w0=70.70000000000013, w1=14.892453933835869\n",
      "SubSGD iter. 317/499: loss=5.894829472298191, w0=70.00000000000013, w1=15.33264918854284\n",
      "SubSGD iter. 318/499: loss=5.610943075852231, w0=70.70000000000013, w1=16.21237931837532\n",
      "SubSGD iter. 319/499: loss=5.864417173708013, w0=70.00000000000013, w1=15.581213978380466\n",
      "SubSGD iter. 320/499: loss=5.677811596691909, w0=70.70000000000013, w1=15.05402533963324\n",
      "SubSGD iter. 321/499: loss=5.434237891653185, w0=71.40000000000013, w1=16.066463677499662\n",
      "SubSGD iter. 322/499: loss=5.352628106384676, w0=72.10000000000014, w1=15.523447216134564\n",
      "SubSGD iter. 323/499: loss=5.438971188379274, w0=71.40000000000013, w1=16.331560957176496\n",
      "SubSGD iter. 324/499: loss=5.400420981300408, w0=72.10000000000014, w1=16.983617854618952\n",
      "SubSGD iter. 325/499: loss=5.3396251068629725, w0=72.80000000000014, w1=16.58764161139712\n",
      "SubSGD iter. 326/499: loss=5.352185565329443, w0=72.10000000000014, w1=15.529726304569563\n",
      "SubSGD iter. 327/499: loss=5.440290000011861, w0=71.40000000000013, w1=16.36576516184238\n",
      "SubSGD iter. 328/499: loss=5.647191003235591, w0=70.70000000000013, w1=15.273140647516584\n",
      "SubSGD iter. 329/499: loss=5.701404221168823, w0=71.40000000000013, w1=14.089138447122693\n",
      "SubSGD iter. 330/499: loss=5.4490985212437995, w0=72.10000000000014, w1=14.702130944306253\n",
      "SubSGD iter. 331/499: loss=5.410986717664905, w0=72.80000000000014, w1=14.591956827425513\n",
      "SubSGD iter. 332/499: loss=5.37906728363481, w0=72.10000000000014, w1=15.204754998426925\n",
      "SubSGD iter. 333/499: loss=5.50263745620238, w0=71.40000000000013, w1=15.0059941326084\n",
      "SubSGD iter. 334/499: loss=5.338082400177756, w0=72.10000000000014, w1=16.018672473961168\n",
      "SubSGD iter. 335/499: loss=5.441183574088499, w0=71.40000000000013, w1=16.388940564489964\n",
      "SubSGD iter. 336/499: loss=5.6434271617731895, w0=70.70000000000013, w1=16.80001574903219\n",
      "SubSGD iter. 337/499: loss=5.534734052205155, w0=71.40000000000013, w1=17.32807156454284\n",
      "SubSGD iter. 338/499: loss=5.343633200317867, w0=72.10000000000014, w1=16.2905573707293\n",
      "SubSGD iter. 339/499: loss=5.345173659324282, w0=72.80000000000014, w1=16.638524417119715\n",
      "SubSGD iter. 340/499: loss=5.40365971033344, w0=73.50000000000014, w1=16.619944958220838\n",
      "SubSGD iter. 341/499: loss=5.3128780541320575, w0=72.80000000000014, w1=15.919010960786085\n",
      "SubSGD iter. 342/499: loss=5.423482434381623, w0=73.50000000000014, w1=16.79825892668226\n",
      "SubSGD iter. 343/499: loss=5.523655275720485, w0=74.20000000000014, w1=16.61479314763184\n",
      "SubSGD iter. 344/499: loss=5.910837476772746, w0=74.90000000000015, w1=17.85410944860984\n",
      "SubSGD iter. 345/499: loss=6.079260833393021, w0=75.60000000000015, w1=17.423777860149105\n",
      "SubSGD iter. 346/499: loss=6.251503269154178, w0=76.30000000000015, w1=16.569931589723836\n",
      "SubSGD iter. 347/499: loss=6.566312028232051, w0=77.00000000000016, w1=16.173955346502005\n",
      "SubSGD iter. 348/499: loss=6.276022708602341, w0=76.30000000000015, w1=16.716971807867104\n",
      "SubSGD iter. 349/499: loss=6.556675189647334, w0=77.00000000000016, w1=16.09783065245272\n",
      "SubSGD iter. 350/499: loss=6.172720706363545, w0=76.30000000000015, w1=15.932024392885225\n",
      "SubSGD iter. 351/499: loss=6.480758418786737, w0=77.00000000000016, w1=15.377917318888644\n",
      "SubSGD iter. 352/499: loss=6.1533130399810005, w0=76.30000000000015, w1=14.521695266384105\n",
      "SubSGD iter. 353/499: loss=6.503310912518652, w0=77.00000000000016, w1=14.122237493997526\n",
      "SubSGD iter. 354/499: loss=6.381847481342136, w0=76.30000000000015, w1=13.02961297967173\n",
      "SubSGD iter. 355/499: loss=6.356623035946363, w0=75.60000000000015, w1=12.188422811447088\n",
      "SubSGD iter. 356/499: loss=8.09756141389952, w0=76.30000000000015, w1=8.81527913385741\n",
      "SubSGD iter. 357/499: loss=7.84927785222209, w0=75.60000000000015, w1=9.020112653732633\n",
      "SubSGD iter. 358/499: loss=7.667319438233928, w0=76.30000000000015, w1=9.612430615000811\n",
      "SubSGD iter. 359/499: loss=6.926622072398795, w0=75.60000000000015, w1=10.796432815394702\n",
      "SubSGD iter. 360/499: loss=6.273509086398008, w0=74.90000000000015, w1=12.007855703320882\n",
      "SubSGD iter. 361/499: loss=5.96515203059681, w0=74.20000000000014, w1=12.628907276891475\n",
      "SubSGD iter. 362/499: loss=5.912648966658915, w0=74.90000000000015, w1=13.156963092402124\n",
      "SubSGD iter. 363/499: loss=5.922622822754679, w0=75.60000000000015, w1=14.33776779310202\n",
      "SubSGD iter. 364/499: loss=5.6540474848229705, w0=74.90000000000015, w1=14.8897392250397\n",
      "SubSGD iter. 365/499: loss=5.863456591181587, w0=75.60000000000015, w1=14.905046891751836\n",
      "SubSGD iter. 366/499: loss=6.135455262072859, w0=76.30000000000015, w1=15.101391485269064\n",
      "SubSGD iter. 367/499: loss=5.8650969127111185, w0=75.60000000000015, w1=15.835505808270053\n",
      "SubSGD iter. 368/499: loss=5.6650666260943625, w0=74.90000000000015, w1=16.340523540094374\n",
      "SubSGD iter. 369/499: loss=5.916238279597529, w0=75.60000000000015, w1=16.377810851712244\n",
      "SubSGD iter. 370/499: loss=6.347093949453117, w0=76.30000000000015, w1=17.109775445219878\n",
      "SubSGD iter. 371/499: loss=6.920084069808176, w0=77.00000000000016, w1=18.122453786572645\n",
      "SubSGD iter. 372/499: loss=6.605489066526598, w0=76.30000000000015, w1=18.304317630147814\n",
      "SubSGD iter. 373/499: loss=6.172532399006088, w0=75.60000000000015, w1=17.83773550418124\n",
      "SubSGD iter. 374/499: loss=5.846668465020664, w0=74.90000000000015, w1=17.587454387719184\n",
      "SubSGD iter. 375/499: loss=5.752726193093229, w0=74.20000000000014, w1=17.932631475444182\n",
      "SubSGD iter. 376/499: loss=5.466925160549062, w0=73.50000000000014, w1=17.09144130721954\n",
      "SubSGD iter. 377/499: loss=5.5656016350095445, w0=72.80000000000014, w1=17.93146887867935\n",
      "SubSGD iter. 378/499: loss=5.848657196509138, w0=73.50000000000014, w1=18.78769093118389\n",
      "SubSGD iter. 379/499: loss=5.874052122682833, w0=74.20000000000014, w1=18.42177020045349\n",
      "SubSGD iter. 380/499: loss=5.658929703128841, w0=73.50000000000014, w1=18.0780575792701\n",
      "SubSGD iter. 381/499: loss=5.553595385093921, w0=72.80000000000014, w1=17.879296713451577\n",
      "SubSGD iter. 382/499: loss=5.686207141312468, w0=72.10000000000014, w1=18.30253613822409\n",
      "SubSGD iter. 383/499: loss=5.78965109783009, w0=71.40000000000013, w1=18.327833903612934\n",
      "SubSGD iter. 384/499: loss=5.42310167766715, w0=72.10000000000014, w1=17.143831703219043\n",
      "SubSGD iter. 385/499: loss=5.534257383111002, w0=71.40000000000013, w1=17.32569554679421\n",
      "SubSGD iter. 386/499: loss=5.904238997482163, w0=70.70000000000013, w1=18.161734404067026\n",
      "SubSGD iter. 387/499: loss=5.59897509338318, w0=71.40000000000013, w1=17.618717942701927\n",
      "SubSGD iter. 388/499: loss=5.463806298822858, w0=72.10000000000014, w1=17.377738028035278\n",
      "SubSGD iter. 389/499: loss=5.644542349024719, w0=71.40000000000013, w1=17.800977452807793\n",
      "SubSGD iter. 390/499: loss=5.4066144668236005, w0=72.10000000000014, w1=17.030154386739603\n",
      "SubSGD iter. 391/499: loss=5.430553064682377, w0=72.80000000000014, w1=17.22649898025683\n",
      "SubSGD iter. 392/499: loss=5.362408404539758, w0=73.50000000000014, w1=16.136703957183613\n",
      "SubSGD iter. 393/499: loss=5.400008074342924, w0=72.80000000000014, w1=17.025535781350108\n",
      "SubSGD iter. 394/499: loss=5.370382719877886, w0=72.10000000000014, w1=16.73703851679269\n",
      "SubSGD iter. 395/499: loss=5.525932843318073, w0=71.40000000000013, w1=17.284200819026562\n",
      "SubSGD iter. 396/499: loss=5.785747117121598, w0=70.70000000000013, w1=17.66948495935469\n",
      "SubSGD iter. 397/499: loss=6.06165000138757, w0=70.00000000000013, w1=17.81994163580582\n",
      "SubSGD iter. 398/499: loss=6.402465474256493, w0=69.30000000000013, w1=18.001805479380987\n",
      "SubSGD iter. 399/499: loss=6.886918530602712, w0=68.60000000000012, w1=18.559929183130027\n",
      "SubSGD iter. 400/499: loss=6.486440305720058, w0=69.30000000000013, w1=18.359179646080776\n",
      "SubSGD iter. 401/499: loss=6.3768932454802885, w0=70.00000000000013, w1=18.975566594918025\n",
      "SubSGD iter. 402/499: loss=6.492743120645749, w0=69.30000000000013, w1=18.383248633649846\n",
      "SubSGD iter. 403/499: loss=6.0603139555097965, w0=70.00000000000013, w1=17.81328850911386\n",
      "SubSGD iter. 404/499: loss=5.70373604932068, w0=70.70000000000013, w1=17.22897597156797\n",
      "SubSGD iter. 405/499: loss=5.881046177804135, w0=70.00000000000013, w1=16.387785803343327\n",
      "SubSGD iter. 406/499: loss=5.611983977556991, w0=70.70000000000013, w1=16.355356795575968\n",
      "SubSGD iter. 407/499: loss=5.438606770654215, w0=71.40000000000013, w1=16.320665936234498\n",
      "SubSGD iter. 408/499: loss=5.3515491903670025, w0=72.10000000000014, w1=16.463996526829014\n",
      "SubSGD iter. 409/499: loss=5.492215824488368, w0=72.80000000000014, w1=17.589126486959323\n",
      "SubSGD iter. 410/499: loss=5.522069094272516, w0=72.10000000000014, w1=17.669940792448035\n",
      "SubSGD iter. 411/499: loss=5.441971146984036, w0=72.80000000000014, w1=17.29967270191924\n",
      "SubSGD iter. 412/499: loss=5.563722308916846, w0=72.10000000000014, w1=17.85164413385692\n",
      "SubSGD iter. 413/499: loss=5.425600984486437, w0=72.80000000000014, w1=17.194763166103954\n",
      "SubSGD iter. 414/499: loss=5.514505931738806, w0=72.10000000000014, w1=17.63445157118914\n",
      "SubSGD iter. 415/499: loss=5.751187814319183, w0=71.40000000000013, w1=18.19257527493818\n",
      "SubSGD iter. 416/499: loss=5.763582935547827, w0=70.70000000000013, w1=17.561409934943327\n",
      "SubSGD iter. 417/499: loss=5.700055891395749, w0=71.40000000000013, w1=18.01006897372804\n",
      "SubSGD iter. 418/499: loss=5.849934183236852, w0=70.70000000000013, w1=17.953759358682735\n",
      "SubSGD iter. 419/499: loss=5.86243234652931, w0=71.40000000000013, w1=18.568420339813123\n",
      "SubSGD iter. 420/499: loss=5.845976632776212, w0=72.10000000000014, w1=18.81273306352226\n",
      "SubSGD iter. 421/499: loss=5.655789480394123, w0=72.80000000000014, w1=18.28810013881052\n",
      "SubSGD iter. 422/499: loss=5.623358206219985, w0=73.50000000000014, w1=17.912514300523952\n",
      "SubSGD iter. 423/499: loss=5.406823152317824, w0=72.80000000000014, w1=17.07132413229931\n",
      "SubSGD iter. 424/499: loss=5.447326136068735, w0=72.10000000000014, w1=17.28858113647906\n",
      "SubSGD iter. 425/499: loss=5.501832179368022, w0=71.40000000000013, w1=17.15625761012522\n",
      "SubSGD iter. 426/499: loss=5.524306204755136, w0=72.10000000000014, w1=17.680438161576937\n",
      "SubSGD iter. 427/499: loss=5.707482103122351, w0=71.40000000000013, w1=18.037163679932775\n",
      "SubSGD iter. 428/499: loss=6.052637040011926, w0=70.70000000000013, w1=18.66197977974372\n",
      "SubSGD iter. 429/499: loss=6.180720669521671, w0=70.00000000000013, w1=18.306952302213894\n",
      "SubSGD iter. 430/499: loss=5.916547163701954, w0=70.70000000000013, w1=18.206675202973685\n",
      "SubSGD iter. 431/499: loss=6.018785639680821, w0=70.00000000000013, w1=17.58754180702902\n",
      "SubSGD iter. 432/499: loss=5.693493425479854, w0=70.70000000000013, w1=17.16175565973828\n",
      "SubSGD iter. 433/499: loss=5.487889869415972, w0=71.40000000000013, w1=17.06147856049807\n",
      "SubSGD iter. 434/499: loss=5.432887685307074, w0=72.10000000000014, w1=17.204809151092586\n",
      "SubSGD iter. 435/499: loss=5.34778036902693, w0=72.80000000000014, w1=16.661792689727488\n",
      "SubSGD iter. 436/499: loss=5.4042155748649785, w0=72.10000000000014, w1=17.01212962309588\n",
      "SubSGD iter. 437/499: loss=5.468243491331953, w0=72.80000000000014, w1=14.241829926206892\n",
      "SubSGD iter. 438/499: loss=5.5268119146445125, w0=72.10000000000014, w1=14.255585122182381\n",
      "SubSGD iter. 439/499: loss=5.609223155038605, w0=71.40000000000013, w1=14.43744896575755\n",
      "SubSGD iter. 440/499: loss=5.436411961144207, w0=72.10000000000014, w1=14.785416012147964\n",
      "SubSGD iter. 441/499: loss=5.44259463517782, w0=72.80000000000014, w1=14.385958239761385\n",
      "SubSGD iter. 442/499: loss=5.424754602091773, w0=72.10000000000014, w1=14.864006442187508\n",
      "SubSGD iter. 443/499: loss=5.360202867853924, w0=72.80000000000014, w1=15.007337032782024\n",
      "SubSGD iter. 444/499: loss=5.442320906317399, w0=72.10000000000014, w1=14.74662481140016\n",
      "SubSGD iter. 445/499: loss=5.388690721241663, w0=72.80000000000014, w1=14.761932478112296\n",
      "SubSGD iter. 446/499: loss=5.410639189291996, w0=72.10000000000014, w1=14.966765997987519\n",
      "SubSGD iter. 447/499: loss=5.523265101266933, w0=71.40000000000013, w1=14.83444247163368\n",
      "SubSGD iter. 448/499: loss=5.667113705098585, w0=70.70000000000013, w1=15.114371464070144\n",
      "SubSGD iter. 449/499: loss=5.9535512522672125, w0=70.00000000000013, w1=14.91561059825162\n",
      "SubSGD iter. 450/499: loss=6.206731256248965, w0=69.30000000000013, w1=15.46277290048549\n",
      "SubSGD iter. 451/499: loss=6.03083231617842, w0=70.00000000000013, w1=14.51926641694837\n",
      "SubSGD iter. 452/499: loss=6.2644279079503695, w0=69.30000000000013, w1=14.95946167165534\n",
      "SubSGD iter. 453/499: loss=5.877261232541432, w0=70.00000000000013, w1=15.468647515601592\n",
      "SubSGD iter. 454/499: loss=6.190482656424965, w0=69.30000000000013, w1=15.891886940374107\n",
      "SubSGD iter. 455/499: loss=5.918170551472333, w0=70.00000000000013, w1=16.79706204356117\n",
      "SubSGD iter. 456/499: loss=6.195455051085615, w0=69.30000000000013, w1=16.301435783092337\n",
      "SubSGD iter. 457/499: loss=6.5766313751319005, w0=68.60000000000012, w1=16.54546260780149\n",
      "SubSGD iter. 458/499: loss=6.4207993907078516, w0=69.30000000000013, w1=18.085524946188677\n",
      "SubSGD iter. 459/499: loss=6.7790711845355665, w0=68.60000000000012, w1=18.09928014216417\n",
      "SubSGD iter. 460/499: loss=6.665039058015768, w0=69.30000000000013, w1=18.978452454394976\n",
      "SubSGD iter. 461/499: loss=6.5543266372315125, w0=70.00000000000013, w1=19.487638298341228\n",
      "SubSGD iter. 462/499: loss=6.1502197325362635, w0=70.70000000000013, w1=18.96300537362949\n",
      "SubSGD iter. 463/499: loss=6.1684499539004145, w0=71.40000000000013, w1=19.47219121757574\n",
      "SubSGD iter. 464/499: loss=5.810023024136191, w0=72.10000000000014, w1=18.701368151507552\n",
      "SubSGD iter. 465/499: loss=5.791970402019374, w0=72.80000000000014, w1=18.753528497037262\n",
      "SubSGD iter. 466/499: loss=5.865432942175473, w0=73.50000000000014, w1=18.842056010526388\n",
      "SubSGD iter. 467/499: loss=5.6848550275892915, w0=72.80000000000014, w1=18.393396971741677\n",
      "SubSGD iter. 468/499: loss=5.559541140895948, w0=73.50000000000014, w1=17.599027627250898\n",
      "SubSGD iter. 469/499: loss=5.501473668486014, w0=74.20000000000014, w1=16.38760473932472\n",
      "SubSGD iter. 470/499: loss=5.8097850417755845, w0=74.90000000000015, w1=17.400283080677486\n",
      "SubSGD iter. 471/499: loss=5.58399940253138, w0=74.20000000000014, w1=17.11178581612007\n",
      "SubSGD iter. 472/499: loss=5.488225560231042, w0=73.50000000000014, w1=17.212062915360278\n",
      "SubSGD iter. 473/499: loss=5.548650556998195, w0=74.20000000000014, w1=16.862124983879962\n",
      "SubSGD iter. 474/499: loss=5.392478471995103, w0=73.50000000000014, w1=16.518412362696573\n",
      "SubSGD iter. 475/499: loss=5.587767758541177, w0=74.20000000000014, w1=17.134799311533822\n",
      "SubSGD iter. 476/499: loss=5.76972292609944, w0=74.90000000000015, w1=17.161171962158644\n",
      "SubSGD iter. 477/499: loss=5.491757265204423, w0=74.20000000000014, w1=16.25599685897158\n",
      "SubSGD iter. 478/499: loss=5.387505409464692, w0=73.50000000000014, w1=16.47325386315133\n",
      "SubSGD iter. 479/499: loss=5.3188333085192365, w0=72.80000000000014, w1=16.34093033679749\n",
      "SubSGD iter. 480/499: loss=5.4112530698214165, w0=73.50000000000014, w1=16.688897383187907\n",
      "SubSGD iter. 481/499: loss=5.407248442671006, w0=72.80000000000014, w1=17.074181523516035\n",
      "SubSGD iter. 482/499: loss=5.340459765118999, w0=72.10000000000014, w1=16.195009211285228\n",
      "SubSGD iter. 483/499: loss=5.313686070349609, w0=72.80000000000014, w1=16.09473211204502\n",
      "SubSGD iter. 484/499: loss=5.33873605014405, w0=72.10000000000014, w1=15.96240858569118\n",
      "SubSGD iter. 485/499: loss=5.529565640548272, w0=71.40000000000013, w1=14.794077785173712\n",
      "SubSGD iter. 486/499: loss=5.422199797969575, w0=72.10000000000014, w1=14.882605298662838\n",
      "SubSGD iter. 487/499: loss=5.3575362626837135, w0=72.80000000000014, w1=15.03549905577742\n",
      "SubSGD iter. 488/499: loss=5.364029160287963, w0=73.50000000000014, w1=15.658987178063597\n",
      "SubSGD iter. 489/499: loss=5.529282089349977, w0=74.20000000000014, w1=16.671665519416365\n",
      "SubSGD iter. 490/499: loss=5.628401144844103, w0=74.90000000000015, w1=15.817819248991096\n",
      "SubSGD iter. 491/499: loss=5.512141444916211, w0=74.20000000000014, w1=16.498418426647177\n",
      "SubSGD iter. 492/499: loss=5.4128338606310775, w0=73.50000000000014, w1=16.7032519465224\n",
      "SubSGD iter. 493/499: loss=5.562463978622876, w0=74.20000000000014, w1=16.96873564874663\n",
      "SubSGD iter. 494/499: loss=5.362868881268137, w0=73.50000000000014, w1=15.762719358976183\n",
      "SubSGD iter. 495/499: loss=5.3389420572392545, w0=72.80000000000014, w1=16.58137772878181\n",
      "SubSGD iter. 496/499: loss=5.3615034471545515, w0=73.50000000000014, w1=15.884792977541046\n",
      "SubSGD iter. 497/499: loss=6.008986047409778, w0=74.20000000000014, w1=12.511649299951369\n",
      "SubSGD iter. 498/499: loss=5.939485425326122, w0=74.90000000000015, w1=13.039705115462018\n",
      "SubSGD iter. 499/499: loss=5.687884801202448, w0=74.20000000000014, w1=13.54472284728634\n",
      "SubSGD: execution time=0.071 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be8a70e71794efa97a85b4e6417502b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "50bb0dcfe1e3ff0df9b3736d3a44cf118a876b258b029b036df7943fef90590f"
   }
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}