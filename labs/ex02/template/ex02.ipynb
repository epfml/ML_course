{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694.4833658870843\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    error = y - tx.dot(w)\n",
    "    loss = 1/(2*y.shape[0]) * np.sum(error**2)\n",
    "    return loss\n",
    "\n",
    "print(compute_loss(y, tx, np.array([1, 2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            losses[i, j] = compute_loss(y, tx, np.array([w0, w1]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.005 seconds\n"
     ]
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    error = y - tx.dot(w)\n",
    "    gradient = -1/(y.shape[0]) * tx.T.dot(error)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=1062606.4462798769, w0=-892.6706077997894, w1=901.3479712434988\n",
      "GD iter. 1/49: loss=860714.1448053952, w0=-796.0741548195997, w1=812.5611453626477\n",
      "GD iter. 2/49: loss=697181.3806110647, w0=-709.1373471374292, w1=732.6530020698816\n",
      "GD iter. 3/49: loss=564719.8416136572, w0=-630.8942202234757, w1=660.7356731063921\n",
      "GD iter. 4/49: loss=457425.99502575735, w0=-560.4754060009176, w1=596.0100770392519\n",
      "GD iter. 5/49: loss=370517.9792895585, w0=-497.0984732006152, w1=537.7570405788256\n",
      "GD iter. 6/49: loss=300122.4865432374, w0=-440.05923368034314, w1=485.32930776444186\n",
      "GD iter. 7/49: loss=243102.1374187173, w0=-388.7239181120982, w1=438.14434823149645\n",
      "GD iter. 8/49: loss=196915.65462785604, w0=-342.52213410067776, w1=395.67788465184566\n",
      "GD iter. 9/49: loss=159504.60356725843, w0=-300.94052849039946, w1=357.45806743016004\n",
      "GD iter. 10/49: loss=129201.65220817438, w0=-263.517083441149, w1=323.0602319306429\n",
      "GD iter. 11/49: loss=104656.26160731632, w0=-229.8359828968235, w1=292.1021799810775\n",
      "GD iter. 12/49: loss=84774.49522062126, w0=-199.5229924069306, w1=264.2399332264686\n",
      "GD iter. 13/49: loss=68670.26444739828, w0=-172.24130096602696, w1=239.1639111473206\n",
      "GD iter. 14/49: loss=55625.83752108766, w0=-147.68777866921369, w1=216.5954912760874\n",
      "GD iter. 15/49: loss=45059.851710776056, w0=-125.58960860208177, w1=196.2839133919775\n",
      "GD iter. 16/49: loss=36501.403204423674, w0=-105.70125554166304, w1=178.00349329627863\n",
      "GD iter. 17/49: loss=29569.059914278245, w0=-87.8017377872862, w1=161.55111521014965\n",
      "GD iter. 18/49: loss=23953.861849260444, w0=-71.69217180834706, w1=146.74397493263356\n",
      "GD iter. 19/49: loss=19405.55141659603, w0=-57.19356242730183, w1=133.4175486828691\n",
      "GD iter. 20/49: loss=15721.419966137863, w0=-44.14481398436112, w1=121.42376505808105\n",
      "GD iter. 21/49: loss=12737.27349126674, w0=-32.400940385714485, w1=110.62935979577182\n",
      "GD iter. 22/49: loss=10320.114846621134, w0=-21.8314541469325, w1=100.91439505969352\n",
      "GD iter. 23/49: loss=8362.216344458193, w0=-12.318916532028727, w1=92.17092679722306\n",
      "GD iter. 24/49: loss=6776.318557706213, w0=-3.7576326786153196, w1=84.30180536099965\n",
      "GD iter. 25/49: loss=5491.741350437108, w0=3.9475227894567535, w1=77.21959606839857\n",
      "GD iter. 26/49: loss=4451.233812549132, w0=10.882162710721607, w1=70.84560770505762\n",
      "GD iter. 27/49: loss=3608.4227068598743, w0=17.123338639859973, w1=65.10901817805075\n",
      "GD iter. 28/49: loss=2925.7457112515744, w0=22.740396976084508, w1=59.946087603744566\n",
      "GD iter. 29/49: loss=2372.7773448088515, w0=27.79574947868658, w1=55.29945008686901\n",
      "GD iter. 30/49: loss=1924.8729679902472, w0=32.345566731028455, w1=51.117476321681\n",
      "GD iter. 31/49: loss=1562.070422767177, w0=36.44040225813613, w1=47.3536999330118\n",
      "GD iter. 32/49: loss=1268.20036113649, w0=40.12575423253304, w1=43.96630118320951\n",
      "GD iter. 33/49: loss=1030.1656112156343, w0=43.44257100949026, w1=40.91764230838746\n",
      "GD iter. 34/49: loss=837.357463779741, w0=46.42770610875176, w1=38.17384932104761\n",
      "GD iter. 35/49: loss=681.1828643566676, w0=49.1143276980871, w1=35.70443563244175\n",
      "GD iter. 36/49: loss=554.6814388239782, w0=51.53228712848891, w1=33.48196331269648\n",
      "GD iter. 37/49: loss=452.2152841424999, w0=53.70845061585054, w1=31.48173822492573\n",
      "GD iter. 38/49: loss=369.2176988505024, w0=55.66699775447601, w1=29.681535645932062\n",
      "GD iter. 39/49: loss=301.9896547639845, w0=57.429690179238925, w1=28.061353324837757\n",
      "GD iter. 40/49: loss=247.53493905390496, w0=59.016113361525555, w1=26.603189235852884\n",
      "GD iter. 41/49: loss=203.42661932874051, w0=60.44389422558352, w1=25.290841555766498\n",
      "GD iter. 42/49: loss=167.69888035135736, w0=61.72889700323569, w1=24.10972864368875\n",
      "GD iter. 43/49: loss=138.75941177967704, w0=62.88539950312264, w1=23.04672702281878\n",
      "GD iter. 44/49: loss=115.31844223661591, w0=63.9262517530209, w1=22.090025564035805\n",
      "GD iter. 45/49: loss=96.33125690673644, w0=64.86301877792933, w1=21.22899425113113\n",
      "GD iter. 46/49: loss=80.95163678953413, w0=65.70610910034691, w1=20.454066069516923\n",
      "GD iter. 47/49: loss=68.49414449460028, w0=66.46489039052274, w1=19.756630706064136\n",
      "GD iter. 48/49: loss=58.40357573570379, w0=67.14779355168099, w1=19.128938878956625\n",
      "GD iter. 49/49: loss=50.2302150409976, w0=67.7624063967234, w1=18.564016234559865\n",
      "GD: execution time=0.008 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Define arbitrary arrays for y and tx (a enlever plus tard)\n",
    "\n",
    "\n",
    "# Run gradient descent with the arbitrary arrays\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Time Visualization\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntSlider, interact\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_figure\u001b[39m(n_iter):\n\u001b[0;32m      6\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gradient_descent_visualization(\n\u001b[0;32m      7\u001b[0m         gd_losses,\n\u001b[0;32m      8\u001b[0m         gd_ws,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         n_iter,\n\u001b[0;32m     17\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    gradient = compute_gradient(y, tx, w)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        for batch_y, batch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(batch_y, batch_tx, w)\n",
    "            loss = compute_loss(batch_y, batch_tx, w)\n",
    "            w = w - gamma * gradient\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=1478.4168761832668, w0=5.437677585483102, w1=-7.702164219443922\n",
      "SGD iter. 1/49: loss=1427.622276203264, w0=10.781126427201837, w1=-10.395559374320449\n",
      "SGD iter. 2/49: loss=2659.576635999685, w0=18.074378974590672, w1=-5.670561078765635\n",
      "SGD iter. 3/49: loss=656.0256806325144, w0=21.69660392793776, w1=-9.303152348263495\n",
      "SGD iter. 4/49: loss=4326.483003179391, w0=30.99873594503942, w1=9.287077768517062\n",
      "SGD iter. 5/49: loss=1152.6168758970034, w0=35.80002093115929, w1=11.766592404618962\n",
      "SGD iter. 6/49: loss=907.4372338482192, w0=40.06015528079715, w1=14.696245242535664\n",
      "SGD iter. 7/49: loss=507.5511792693538, w0=43.24622238388633, w1=18.594434227632583\n",
      "SGD iter. 8/49: loss=251.9614360200638, w0=45.49104503175882, w1=17.587065824458104\n",
      "SGD iter. 9/49: loss=400.98342736071186, w0=48.32294696283169, w1=20.083867193189075\n",
      "SGD iter. 10/49: loss=112.34489498192029, w0=49.821912572723534, w1=19.66318421621703\n",
      "SGD iter. 11/49: loss=234.37795943765363, w0=51.986989751198486, w1=20.55922048928062\n",
      "SGD iter. 12/49: loss=168.6843112658599, w0=53.82374945930328, w1=20.295958525878934\n",
      "SGD iter. 13/49: loss=386.41804594315715, w0=56.60374243031294, w1=20.506375285269755\n",
      "SGD iter. 14/49: loss=419.32652777600606, w0=59.49969314735219, w1=19.27169332001143\n",
      "SGD iter. 15/49: loss=135.351874815414, w0=61.145000868208164, w1=17.82308553000498\n",
      "SGD iter. 16/49: loss=5.465098364765607, w0=61.475609348582324, w1=17.977909550676117\n",
      "SGD iter. 17/49: loss=170.27729400137247, w0=63.321021465164264, w1=16.140651635074363\n",
      "SGD iter. 18/49: loss=88.94714573190613, w0=64.65479165325445, w1=14.734233989352141\n",
      "SGD iter. 19/49: loss=3.4211362563385856, w0=64.39321427440305, w1=14.893521732167233\n",
      "SGD iter. 20/49: loss=135.1870665643636, w0=66.03752000366609, w1=13.047529204336051\n",
      "SGD iter. 21/49: loss=4.541497121847177, w0=65.73613994056147, w1=12.891025885509698\n",
      "SGD iter. 22/49: loss=51.455144703077295, w0=66.75058702841847, w1=13.928995109474196\n",
      "SGD iter. 23/49: loss=0.011116384137777104, w0=66.76549768510272, w1=13.932573105893537\n",
      "SGD iter. 24/49: loss=6.919715863182121, w0=66.39348381807133, w1=13.045922854812927\n",
      "SGD iter. 25/49: loss=1.0183478883506623, w0=66.5361966688137, w1=12.808126040823153\n",
      "SGD iter. 26/49: loss=72.19169045143038, w0=67.73779302742528, w1=11.722180103602636\n",
      "SGD iter. 27/49: loss=23.117670914365785, w0=68.4177587753033, w1=12.534295768006228\n",
      "SGD iter. 28/49: loss=10.19362340724624, w0=68.86928116472039, w1=13.071718513671335\n",
      "SGD iter. 29/49: loss=2.4954417258075754, w0=68.64587831219573, w1=13.283442629870757\n",
      "SGD iter. 30/49: loss=24.237114053365833, w0=69.34211267005759, w1=12.15360504327624\n",
      "SGD iter. 31/49: loss=0.7718937207971756, w0=69.46636191317779, w1=12.312904759736309\n",
      "SGD iter. 32/49: loss=43.498576629182416, w0=70.39908455822489, w1=11.417485432368832\n",
      "SGD iter. 33/49: loss=4.808236433516665, w0=70.70918894173201, w1=11.425949308764647\n",
      "SGD iter. 34/49: loss=1.3973859468059022, w0=70.87636465448993, w1=11.750961194739123\n",
      "SGD iter. 35/49: loss=21.94974042783874, w0=71.53893148776547, w1=12.568741668436909\n",
      "SGD iter. 36/49: loss=15.274853562738253, w0=72.09164938258593, w1=12.740893021435518\n",
      "SGD iter. 37/49: loss=21.463071658543257, w0=71.43646892410837, w1=12.876804885363589\n",
      "SGD iter. 38/49: loss=26.131419547951527, w0=72.15939934327679, w1=12.524641472191009\n",
      "SGD iter. 39/49: loss=67.37787455052899, w0=70.99855590378523, w1=13.60507216622505\n",
      "SGD iter. 40/49: loss=14.070912511293997, w0=70.46806721377773, w1=13.561646432692568\n",
      "SGD iter. 41/49: loss=16.100066029620354, w0=69.90061561179532, w1=13.73647189218233\n",
      "SGD iter. 42/49: loss=11.772192852162496, w0=70.38584118515605, w1=14.302779796755985\n",
      "SGD iter. 43/49: loss=30.74695182983885, w0=71.17002167241442, w1=13.473577880535993\n",
      "SGD iter. 44/49: loss=8.447431990976803, w0=71.58105529612365, w1=13.753624238384013\n",
      "SGD iter. 45/49: loss=5.837794061799197, w0=71.92275089345551, w1=14.103242782554592\n",
      "SGD iter. 46/49: loss=0.2891485191178678, w0=71.99879673740742, w1=13.917975466763355\n",
      "SGD iter. 47/49: loss=0.6056450503539023, w0=71.88873811208856, w1=14.040797290539622\n",
      "SGD iter. 48/49: loss=0.38328839207477877, w0=71.80118374131533, w1=13.996916542365003\n",
      "SGD iter. 49/49: loss=1.7379794457425286, w0=71.61474450479348, w1=13.93977324046077\n",
      "SGD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Time Visualization\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntSlider, interact\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_figure\u001b[39m(n_iter):\n\u001b[0;32m      6\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gradient_descent_visualization(\n\u001b[0;32m      7\u001b[0m         sgd_losses,\n\u001b[0;32m      8\u001b[0m         sgd_ws,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         n_iter,\n\u001b[0;32m     17\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=7.4067805854926325, w1=1.103489486598917\n",
      "GD iter. 1/49: loss=2337.093281493536, w0=14.072883112436008, w1=2.0966300245379443\n",
      "GD iter. 2/49: loss=1905.5723967292586, w0=20.07237538668505, w1=2.9904565086830646\n",
      "GD iter. 3/49: loss=1556.0404800701936, w0=25.47191843350918, w1=3.7949003444136746\n",
      "GD iter. 4/49: loss=1272.9196275763513, w0=30.3315071756509, w1=4.5188997965712225\n",
      "GD iter. 5/49: loss=1043.591737056339, w0=34.705137043578446, w1=5.170499303513015\n",
      "GD iter. 6/49: loss=857.8361457351294, w0=38.64140392471324, w1=5.756938859760628\n",
      "GD iter. 7/49: loss=707.3741167649493, w0=42.18404411773455, w1=6.28473446038348\n",
      "GD iter. 8/49: loss=585.4998732991037, w0=45.37242029145373, w1=6.759750500944047\n",
      "GD iter. 9/49: loss=486.7817360917687, w0=48.241958847800994, w1=7.187264937448555\n",
      "GD iter. 10/49: loss=406.8200449538272, w0=50.82454354851353, w1=7.572027930302614\n",
      "GD iter. 11/49: loss=342.0510751320947, w0=53.148869779154815, w1=7.918314623871265\n",
      "GD iter. 12/49: loss=289.5882095764913, w0=55.24076338673197, w1=8.229972648083052\n",
      "GD iter. 13/49: loss=247.09328847645259, w0=57.12346763355141, w1=8.51046486987366\n",
      "GD iter. 14/49: loss=212.67240238542126, w0=58.817901455688904, w1=8.762907869485206\n",
      "GD iter. 15/49: loss=184.79148465168583, w0=60.34289189561265, w1=8.990106569135598\n",
      "GD iter. 16/49: loss=162.20794128736017, w0=61.71538329154402, w1=9.19458539882095\n",
      "GD iter. 17/49: loss=143.9152711622564, w0=62.950625547882254, w1=9.378616345537766\n",
      "GD iter. 18/49: loss=129.0982083609223, w0=64.06234357858666, w1=9.5442441975829\n",
      "GD iter. 19/49: loss=117.09638749184177, w0=65.06288980622064, w1=9.69330926442352\n",
      "GD iter. 20/49: loss=107.3749125878864, w0=65.9633814110912, w1=9.82746782458008\n",
      "GD iter. 21/49: loss=99.50051791568269, w0=66.77382385547472, w1=9.948210528720981\n",
      "GD iter. 22/49: loss=93.12225823119766, w0=67.50322205541988, w1=10.056878962447794\n",
      "GD iter. 23/49: loss=87.95586788676472, w0=68.15968043537053, w1=10.154680552801926\n",
      "GD iter. 24/49: loss=83.77109170777406, w0=68.75049297732612, w1=10.242701984120645\n",
      "GD iter. 25/49: loss=80.3814230027916, w0=69.28222426508614, w1=10.321921272307492\n",
      "GD iter. 26/49: loss=77.63579135175586, w0=69.76078242407017, w1=10.393218631675653\n",
      "GD iter. 27/49: loss=75.41182971441685, w0=70.19148476715579, w1=10.457386255106998\n",
      "GD iter. 28/49: loss=73.6104207881723, w0=70.57911687593284, w1=10.51513711619521\n",
      "GD iter. 29/49: loss=72.15127955791424, w0=70.92798577383219, w1=10.567112891174599\n",
      "GD iter. 30/49: loss=70.96937516140518, w0=71.24196778194161, w1=10.61389108865605\n",
      "GD iter. 31/49: loss=70.01203260023283, w0=71.52455158924009, w1=10.655991466389356\n",
      "GD iter. 32/49: loss=69.2365851256832, w0=71.77887701580872, w1=10.69388180634933\n",
      "GD iter. 33/49: loss=68.60847267129806, w0=72.00776989972049, w1=10.727983112313307\n",
      "GD iter. 34/49: loss=68.09970158324606, w0=72.21377349524107, w1=10.758674287680886\n",
      "GD iter. 35/49: loss=67.68759700192395, w0=72.39917673120961, w1=10.786296345511708\n",
      "GD iter. 36/49: loss=67.35379229105303, w0=72.56603964358128, w1=10.811156197559448\n",
      "GD iter. 37/49: loss=67.08341047524759, w0=72.71621626471578, w1=10.833530064402414\n",
      "GD iter. 38/49: loss=66.86440120444522, w0=72.85137522373684, w1=10.853666544561083\n",
      "GD iter. 39/49: loss=66.68700369509526, w0=72.97301828685579, w1=10.871789376703886\n",
      "GD iter. 40/49: loss=66.5433117125218, w0=73.08249704366285, w1=10.888099925632407\n",
      "GD iter. 41/49: loss=66.4269212066373, w0=73.1810279247892, w1=10.902779419668077\n",
      "GD iter. 42/49: loss=66.33264489687086, w0=73.26970571780292, w1=10.91599096430018\n",
      "GD iter. 43/49: loss=66.25628108596004, w0=73.34951573151527, w1=10.927881354469072\n",
      "GD iter. 44/49: loss=66.19442639912226, w0=73.42134474385638, w1=10.938582705621075\n",
      "GD iter. 45/49: loss=66.14432410278367, w0=73.48599085496338, w1=10.948213921657878\n",
      "GD iter. 46/49: loss=66.10374124274942, w0=73.54417235495967, w1=10.956882016091\n",
      "GD iter. 47/49: loss=66.07086912612168, w0=73.59653570495634, w1=10.964683301080811\n",
      "GD iter. 48/49: loss=66.04424271165321, w0=73.64366271995334, w1=10.97170445757164\n",
      "GD iter. 49/49: loss=66.02267531593372, w0=73.68607703345064, w1=10.978023498413386\n",
      "GD: execution time=0.002 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "# Visualize the cloud of points and the model fit\n",
    "plt.scatter(tx[:, 1], y, color='blue', label='Data points')\n",
    "plt.plot(tx[:, 1], tx.dot(gd_ws[-1]), color='red', label='Model fit')\n",
    "plt.xlabel('Standardized Height')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Model Fit on Subsampled Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    error = y - tx.dot(w)\n",
    "    #Using chain rule for the subgradient of the MAE\n",
    "    subgradient = -np.sign(error).dot(tx) / y.shape[0]\n",
    "    return subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * subgradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=2869.8351145358524, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=2818.2326504374046, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=2767.120186338956, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=2716.4977222405073, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=2666.365258142059, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=2616.72279404361, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=2567.570329945162, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=2518.9078658467133, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=2470.735401748265, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=2423.052937649817, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=2375.8604735513677, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=2329.1580094529195, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=2282.945545354471, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=2237.223081256023, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=2191.9906171575744, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=2147.248153059126, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=2102.995688960677, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=2059.2332248622292, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=2015.9607607637806, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=1973.1782966653323, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=1930.885832566884, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=1889.0833684684355, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=1847.7709043699867, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=1806.9484402715386, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=1766.61597617309, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=1726.7735120746415, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=1687.421047976193, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=1648.5585838777447, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=1610.1861197792962, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=1572.3036556808477, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=1534.9111915823994, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=1498.0087274839507, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=1461.5962633855024, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=1425.6737992870537, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=1390.2413351886057, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=1355.2988710901573, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=1320.8464069917086, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=1286.8839428932602, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=1253.4114787948117, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=1220.4290146963633, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=1187.936550597915, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=1155.9340864994663, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=1124.4216224010179, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=1093.3991583025695, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=1062.8666942041211, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=1032.8242301056725, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=1003.271766007224, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=974.2093019087753, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=945.6368378103266, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=917.554373711878, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=889.9619096134297, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=862.859445514981, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=836.2469814165324, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=810.1245173180838, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=784.4920532196352, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=759.3495891211867, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=734.6971250227382, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=710.5346609242895, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=686.862196825841, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=663.6797327273924, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=640.9872686289439, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=618.7848045304954, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=597.0723404320468, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=575.8498763335982, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=555.1174122351497, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=534.8749481367012, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=515.1224840382527, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=495.86001993980415, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=477.14806692939743, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=458.97652381717785, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=441.27624817364483, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=424.24408268143, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=407.6944527790817, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=391.5821885242349, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=375.99555288487306, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=360.95695350929594, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=346.3748247543328, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=332.3117701076255, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=318.6833724768496, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=305.5086034302328, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=292.7666734173505, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=280.53153011615814, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=268.89668417535404, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=257.733920052546, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=246.93766902970762, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=236.64352344592248, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=226.75154983045002, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=217.3573889641131, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=208.28322256993167, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=199.60239149197503, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=191.51606823720067, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=183.75485658516766, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=176.30486084041354, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=169.1001222646711, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=162.25177552382956, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=155.69742299714355, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=149.52752679496214, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=143.6406908762162, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=138.01748857628561, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=132.61620926126318, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=127.5497244247717, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=122.74087338718583, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=118.14592856152613, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=113.76488994779264, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=109.59775754598533, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=105.79833542751531, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=102.29523108809988, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=98.99125186585272, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=95.97103181808458, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=93.14678297619844, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=90.61539672531055, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=88.27117995547904, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=86.11413266670397, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=84.16459694644126, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=82.46374060728385, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=80.92130656136143, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=79.52815785669323, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=78.31663397216153, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=77.3176356885103, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=76.5086323125277, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=75.84369059931619, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=75.2972811232521, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=74.7958198200142, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=74.43521733660019, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=74.19719822092475, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=74.0683789939549, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=74.03676697743114, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=74.08918974672679, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=74.22098311708994, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=74.41647628166011, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=74.67096152833798, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=74.95294838238374, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=75.1777967088681, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=75.42154902891534, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=75.65853052170131, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=75.90956114411335, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=76.17464089615153, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=76.41613751021124, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=76.65285618006445, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=76.89760893143615, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=77.10246868795754, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=77.27462217352495, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=77.42392987125322, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=77.56681600428621, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=77.6575549725316, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=77.697735657651, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=77.77686805360916, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=77.80488113813014, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=77.83348882035091, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=77.86269110027146, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=77.94417771357972, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=77.97453880885682, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=78.01721470220237, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=78.06006252205748, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=78.10308226842213, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=78.12180591760088, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=78.14054518714461, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=78.15930007705333, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=78.17807058732701, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=78.19685671796569, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=78.21565846896934, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=78.23447584033798, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=78.25330883207157, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=78.27215744417016, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=78.29102167663372, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=78.30990152946228, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=78.32879700265579, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=78.3477080962143, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=78.36663481013778, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=78.38557714442624, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=78.40453509907967, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=78.42350867409807, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=78.44249786948149, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=78.46150268522987, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=78.48052312134321, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=78.49955917782155, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=78.51861085466484, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=78.53767815187314, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=78.55676106944641, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=78.57585960738466, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=78.59497376568787, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=78.61410354435608, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=78.63324894338928, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=78.65240996278742, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=78.67158660255058, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=78.69077886267868, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=78.70998674317177, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=78.72921024402986, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=78.7484493652529, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=78.76770410684094, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=78.78697446879396, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=78.78623350545425, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=78.80551108487151, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=78.82480428465377, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=78.82409907031933, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=78.84339948756585, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=78.86271552517735, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=78.88204718315382, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=78.88136931492396, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=78.90070819036468, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=78.9200626861704, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=78.91942056694582, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=78.93878228021579, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=78.95815961385075, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=78.97755256785065, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=78.97693779473066, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=78.99633796619483, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=79.015753758024, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=79.01517473390928, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=79.0345977432027, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=79.0540363728611, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=79.07349062288448, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=79.07293894487434, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=79.09240041236197, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=79.11187750021459, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=79.11136157120973, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=79.12342941191513, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=79.12290850230885, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=79.13498681139052, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=79.13446092118284, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=79.14654969864078, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=79.1460188278317, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=79.15811807366592, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=79.15758222225541, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=79.14963612667158, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=79.16173864779152, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=79.16120123807893, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=79.15325396271149, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=79.16535975911714, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=79.16482079110246, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=79.15687233595143, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=79.14892647180801, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=79.16102835040883, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=79.16049124639137, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=79.15254420246437, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=79.16464935635088, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=79.16411069403135, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=79.15616247032072, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=79.16827089949295, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=79.16773067887131, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=79.1597812753771, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=79.15183446289053, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=79.16393897425792, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=79.1634006176335, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=79.15545262536332, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=79.16756041201641, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=79.16702049708994, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=79.15907132503614, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=79.15112474399001, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=79.16322861283824, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=79.16269056190897, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=79.15474280107924, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=79.16684994521319, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=79.16631033598182, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=79.15836139536849, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=79.15041504576276, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=79.16251827209189, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=79.16198052685775, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=79.15403299746843, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=79.16613949908326, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=79.16560019554703, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=79.15765148637412, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=79.14970536820883, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=79.16180795201879, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=79.1612705124798, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=79.15332321453093, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=79.16542907362661, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=79.16489007578554, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=79.15694159805305, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=79.1489957113282, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=79.16109765261903, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=79.16056051877517, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=79.15261345226673, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=79.16471866884328, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=79.16417997669731, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=79.15623173040527, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=79.16834022226755, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=79.16779997181949, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=79.15985054574384, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=79.15190371067581, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=79.16400828473323, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=79.16346989828241, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=79.15552188343078, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=79.16762973277392, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=79.16708978802103, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=79.1591405933858, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=79.1511939897582, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=79.1632979212965, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=79.1627598405408, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=79.15481205712962, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=79.1669192639536, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=79.16637962489584, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=79.15843066170106, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=79.1504842895139, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=79.16258757853305, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=79.1620498034725, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=79.15410225150173, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=79.1662088158066, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=79.16566948244396, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=79.15772075068959, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=79.14977460994287, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=79.1618772564429, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=79.1613397870775, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=79.15339246654716, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=79.16549838833289, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=79.16495936066539, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=79.15701086035145, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=79.14906495104516, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=79.16116695502608, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=79.16062979135577, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=79.15268270226586, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=79.16478798153247, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=79.16424925956011, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=79.1563009906866, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=79.16840954523893, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=79.16786926496447, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=79.15991981630737, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=79.15197295865788, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=79.16407759540536, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=79.16353917912812, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=79.15559114169506, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=79.16769905372824, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=79.16715907914892, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=79.15920986193224, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=79.1512632357232, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=79.16336722995155, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=79.16282911936945, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=79.15488131337679, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=79.16698858289085, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=79.16644891400665, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=79.15849992823041, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=79.1505535334618, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=79.16265688517103, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=79.16211908028406, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=79.15417150573185, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=79.16627813272679, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=79.16573876953771, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=79.15779001520188, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=79.14984385187371, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=79.1619465610638, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=79.16140906187198, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=79.15346171876018, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=79.16556770323598, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=79.16502864574207, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=79.15708012284666, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=79.14913419095893, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=79.1612362576299, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=79.16069906413318, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=79.15275195246184, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=79.1648572944185, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=79.16431854261971, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=79.15637025116476, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=79.16847886840712, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=79.16793855830623, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=79.1599890870677, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=79.15204220683677, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=79.16414690627431, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=79.16360846017065, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=79.15566040015614, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=79.16776837487937, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=79.16722837047361, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=79.15927913067549, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=79.151332481885, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=79.16343653880342, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=79.16289839839489, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=79.15495056982078, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=79.1670579020249, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=79.1665182033143, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=79.1585691949566, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=79.15062277760653, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=79.1627261920058, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=79.16218835729242, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=79.15424076015876, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=79.16634744984376, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=79.16580805682827, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=79.15785927991101, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=79.14991309400138, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=79.16201586588151, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=79.16147833686325, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=79.15353097117003, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=79.16563701833587, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=79.16509793101554, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=79.15714938553872, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=79.14920343106952, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=79.16130556043052, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=79.1607683371074, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=79.1528212028546, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=79.1649266075013, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=79.1643878258761, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=79.15643951183971, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=79.16854819177212, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=79.16800785184483, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=79.16005835802483, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=79.15211145521246, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=79.16421621734004, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=79.16367774140997, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=79.155729658814, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=79.1678376962273, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=79.16729766199514, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=79.15934839961555, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=79.15140172824361, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=79.16350584785206, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=79.16296767761713, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=79.1550198264616, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=79.16712722135576, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=79.16658749281873, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=79.15863846187959, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=79.15069202194807, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=79.1627954990374, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=79.16225763449759, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=79.1543100147825, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=79.16641676715751, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=79.16587734431563, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=79.15792854481691, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=79.14998233632583, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=79.16208517089602, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=79.16154761205135, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=79.15360022377668, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=79.16570633363258, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=79.16516721648581, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=79.15721864842753, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=79.14927267137689, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=79.16137486342795, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=79.1608376102784, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=79.15289045344416, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=79.16499592078094, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=79.16445710932929, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=79.15650877271145, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=79.16861751533395, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=79.16807714558023, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=79.16012762917877, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=79.15218070378494, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=79.16428552860259, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=79.16374702284608, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=79.15579891766866, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=79.16790701777202, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=79.16736695371344, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=79.15941766875241, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=79.15147097479904, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=79.16357515709753, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=79.16303695703617, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=79.15508908329919, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=79.16719654088341, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=79.16665678251996, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=79.15870772899937, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=79.15076126648641, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=79.16286480626579, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=79.16232691189957, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=79.154379269603, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=79.1664860846681, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=79.16594663199977, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=79.15799780991962, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=79.1500515788471, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=79.16215447610735, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=79.16161688743624, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=79.15366947658012, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=79.16577564912609, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=79.16523650215288, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=79.15728791151318, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=79.14934191188107, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=79.1614441666222, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=79.16090688364623, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=79.15295970423053, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=79.16506523425736, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=79.16452639297931, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=79.15657803378002, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=79.16868683909256, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=79.16814643951243, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=79.16019690052951, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=79.15224995255426, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=79.16435484006193, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=79.16381630447901, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=79.15586817672016, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=79.16797633951357, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=79.16743624562855, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=79.15948693808609, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=79.15154022155124, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=79.16364446653981, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=79.16310623665203, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=79.15515834033359, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=79.16726586060787, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=79.166726072418, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=79.15877699631596, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=79.15083051122156, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=79.16293411369098, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=79.16239618949834, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=79.15444852462034, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=79.16655540237548, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=79.16601591988075, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=79.15806707521914, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=79.15012082156515, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=79.16222378151546, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=79.16168616301796, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=79.1537387295804, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=79.16584496481637, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=79.16530578801678, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=79.1573571747956, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=79.14941115258208, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=79.16151347001323, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=79.16097615721087, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.025 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntSlider, interact\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_figure\u001b[39m(n_iter):\n\u001b[0;32m      5\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gradient_descent_visualization(\n\u001b[0;32m      6\u001b[0m         subgd_losses,\n\u001b[0;32m      7\u001b[0m         subgd_ws,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m         n_iter,\n\u001b[0;32m     16\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        \n",
    "        for batch_y, batch_tx in batch_iter(y, tx, batch_size):\n",
    "            subgradient = compute_subgradient_mae(batch_y, batch_tx, w)\n",
    "            loss = compute_loss(batch_y, batch_tx, w)\n",
    "            w = w - gamma * subgradient\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=1446.407839559022, w0=0.7, w1=-0.9767974789419188\n",
      "SubSGD iter. 1/499: loss=3789.823348670773, w0=1.4, w1=-0.698670510140231\n",
      "SubSGD iter. 2/499: loss=3774.0394767420953, w0=2.0999999999999996, w1=-0.4543577864310967\n",
      "SubSGD iter. 3/499: loss=4989.309805447026, w0=2.8, w1=0.5583205549216702\n",
      "SubSGD iter. 4/499: loss=2277.1316821291607, w0=3.5, w1=0.7570814207401947\n",
      "SubSGD iter. 5/499: loss=5359.586331130035, w0=4.2, w1=1.9378861214400906\n",
      "SubSGD iter. 6/499: loss=2522.8725632219125, w0=4.9, w1=1.905457113672731\n",
      "SubSGD iter. 7/499: loss=1887.1147534685576, w0=5.6000000000000005, w1=1.3513500396761502\n",
      "SubSGD iter. 8/499: loss=3158.0777224495478, w0=6.300000000000001, w1=1.5476946331933783\n",
      "SubSGD iter. 9/499: loss=3156.3187228847382, w0=7.000000000000001, w1=1.7588590277595821\n",
      "SubSGD iter. 10/499: loss=1179.6474507062592, w0=7.700000000000001, w1=1.0553541093687195\n",
      "SubSGD iter. 11/499: loss=2444.0709884087305, w0=8.4, w1=1.3438513739261353\n",
      "SubSGD iter. 12/499: loss=1442.527278674488, w0=9.1, w1=1.0998245492169845\n",
      "SubSGD iter. 13/499: loss=3805.514986322662, w0=9.799999999999999, w1=2.1122628870834053\n",
      "SubSGD iter. 14/499: loss=1835.590446980825, w0=10.499999999999998, w1=1.5876299623716656\n",
      "SubSGD iter. 15/499: loss=3096.0560416821936, w0=11.199999999999998, w1=2.2111180846578433\n",
      "SubSGD iter. 16/499: loss=1981.4735954377513, w0=11.899999999999997, w1=2.8165212361018277\n",
      "SubSGD iter. 17/499: loss=1843.8464954539015, w0=12.599999999999996, w1=2.4205449928799956\n",
      "SubSGD iter. 18/499: loss=865.2018186226627, w0=13.299999999999995, w1=1.2091221049538154\n",
      "SubSGD iter. 19/499: loss=3044.253137424294, w0=13.999999999999995, w1=1.6490465478379974\n",
      "SubSGD iter. 20/499: loss=2390.0953014973115, w0=14.699999999999994, w1=2.34998054527275\n",
      "SubSGD iter. 21/499: loss=649.4840564207689, w0=15.399999999999993, w1=1.6693813676166673\n",
      "SubSGD iter. 22/499: loss=1545.60850475805, w0=16.099999999999994, w1=1.2390497791559314\n",
      "SubSGD iter. 23/499: loss=2057.1351313860946, w0=16.799999999999994, w1=1.582762400339322\n",
      "SubSGD iter. 24/499: loss=2036.5276980260821, w0=17.499999999999993, w1=1.7792824831178247\n",
      "SubSGD iter. 25/499: loss=945.10809760268, w0=18.199999999999992, w1=1.432198497497834\n",
      "SubSGD iter. 26/499: loss=500.8250088095323, w0=18.89999999999999, w1=0.7515993198417512\n",
      "SubSGD iter. 27/499: loss=769.7698658237503, w0=19.59999999999999, w1=-0.2859148739717894\n",
      "SubSGD iter. 28/499: loss=722.6459656020058, w0=20.29999999999999, w1=-0.7261101286787606\n",
      "SubSGD iter. 29/499: loss=635.3458374116155, w0=20.99999999999999, w1=-1.7636243224923012\n",
      "SubSGD iter. 30/499: loss=2883.7593511788286, w0=21.69999999999999, w1=-0.9074022699877631\n",
      "SubSGD iter. 31/499: loss=2416.9394542997243, w0=22.399999999999988, w1=-0.2927412888573764\n",
      "SubSGD iter. 32/499: loss=1151.1275799262369, w0=23.099999999999987, w1=-0.03202906747551254\n",
      "SubSGD iter. 33/499: loss=1875.4950911302494, w0=23.799999999999986, w1=0.4960267480351362\n",
      "SubSGD iter. 34/499: loss=2310.4386727180276, w0=24.499999999999986, w1=0.9359511909193182\n",
      "SubSGD iter. 35/499: loss=1785.9468460074065, w0=25.199999999999985, w1=1.8828634526617485\n",
      "SubSGD iter. 36/499: loss=2263.561003327019, w0=25.899999999999984, w1=2.739085505166287\n",
      "SubSGD iter. 37/499: loss=385.7174338500195, w0=26.599999999999984, w1=1.8502536809997934\n",
      "SubSGD iter. 38/499: loss=1701.4455868838172, w0=27.299999999999983, w1=2.0465982745170215\n",
      "SubSGD iter. 39/499: loss=786.4078753781444, w0=27.999999999999982, w1=1.9657839690283105\n",
      "SubSGD iter. 40/499: loss=1805.2926788137233, w0=28.69999999999998, w1=2.6119312024155312\n",
      "SubSGD iter. 41/499: loss=403.15569207248274, w0=29.39999999999998, w1=2.103430084675698\n",
      "SubSGD iter. 42/499: loss=345.9880221082552, w0=30.09999999999998, w1=1.369315761674709\n",
      "SubSGD iter. 43/499: loss=520.9702788705166, w0=30.79999999999998, w1=1.4256253767200124\n",
      "SubSGD iter. 44/499: loss=1333.3651095051475, w0=31.49999999999998, w1=2.0179433379881906\n",
      "SubSGD iter. 45/499: loss=984.8973589416917, w0=32.19999999999998, w1=1.7769634233215426\n",
      "SubSGD iter. 46/499: loss=895.8055474523579, w0=32.899999999999984, w1=2.382366574765527\n",
      "SubSGD iter. 47/499: loss=1153.3896936296458, w0=33.59999999999999, w1=2.831025613550237\n",
      "SubSGD iter. 48/499: loss=1486.3604001739238, w0=34.29999999999999, w1=3.887426844929127\n",
      "SubSGD iter. 49/499: loss=420.45816558305165, w0=34.99999999999999, w1=3.447738439843943\n",
      "SubSGD iter. 50/499: loss=1735.1196313385014, w0=35.699999999999996, w1=4.566048660455424\n",
      "SubSGD iter. 51/499: loss=1069.313616529336, w0=36.4, w1=5.075234504401677\n",
      "SubSGD iter. 52/499: loss=972.078010729038, w0=37.1, w1=5.603290319912325\n",
      "SubSGD iter. 53/499: loss=656.3168411279099, w0=37.800000000000004, w1=5.347858438810754\n",
      "SubSGD iter. 54/499: loss=1314.1908462103586, w0=38.50000000000001, w1=5.960850935994314\n",
      "SubSGD iter. 55/499: loss=194.03229371562793, w0=39.20000000000001, w1=5.120823364534503\n",
      "SubSGD iter. 56/499: loss=231.23882637327, w0=39.90000000000001, w1=4.84089437209804\n",
      "SubSGD iter. 57/499: loss=573.6401417841317, w0=40.600000000000016, w1=4.444918128876208\n",
      "SubSGD iter. 58/499: loss=134.28690015025037, w0=41.30000000000002, w1=3.0909612348429802\n",
      "SubSGD iter. 59/499: loss=922.1626682912996, w0=42.00000000000002, w1=3.3564449370672107\n",
      "SubSGD iter. 60/499: loss=362.3657247812496, w0=42.700000000000024, w1=2.669750301561074\n",
      "SubSGD iter. 61/499: loss=390.2114656296359, w0=43.40000000000003, w1=2.5595761846803335\n",
      "SubSGD iter. 62/499: loss=887.4838524374816, w0=44.10000000000003, w1=3.3055618382677308\n",
      "SubSGD iter. 63/499: loss=1234.4736554178191, w0=44.80000000000003, w1=4.668313233659477\n",
      "SubSGD iter. 64/499: loss=209.3169725032069, w0=45.500000000000036, w1=3.989966237684934\n",
      "SubSGD iter. 65/499: loss=98.73743315064134, w0=46.20000000000004, w1=3.2057335096456097\n",
      "SubSGD iter. 66/499: loss=612.3220411054152, w0=46.90000000000004, w1=4.15264577138804\n",
      "SubSGD iter. 67/499: loss=585.2460798824205, w0=47.600000000000044, w1=4.305539528502622\n",
      "SubSGD iter. 68/499: loss=671.4506070499489, w0=48.30000000000005, w1=4.50188412201985\n",
      "SubSGD iter. 69/499: loss=305.0482906804949, w0=49.00000000000005, w1=4.483304663120973\n",
      "SubSGD iter. 70/499: loss=204.57851201972719, w0=49.70000000000005, w1=4.27847114324575\n",
      "SubSGD iter. 71/499: loss=180.10674749809831, w0=50.400000000000055, w1=4.539183364627614\n",
      "SubSGD iter. 72/499: loss=725.7536798476658, w0=51.10000000000006, w1=5.2711479581352485\n",
      "SubSGD iter. 73/499: loss=315.95195489077264, w0=51.80000000000006, w1=5.359675471624374\n",
      "SubSGD iter. 74/499: loss=609.743725273255, w0=52.500000000000064, w1=6.1548451967417215\n",
      "SubSGD iter. 75/499: loss=77.0827971406429, w0=53.20000000000007, w1=6.004388520290594\n",
      "SubSGD iter. 76/499: loss=65.30275402348826, w0=53.90000000000007, w1=5.56470011520541\n",
      "SubSGD iter. 77/499: loss=430.66236405037324, w0=54.60000000000007, w1=6.289720305286214\n",
      "SubSGD iter. 78/499: loss=251.39788784598443, w0=55.300000000000075, w1=7.236632567028645\n",
      "SubSGD iter. 79/499: loss=832.4553295504506, w0=56.00000000000008, w1=8.417437267728541\n",
      "SubSGD iter. 80/499: loss=83.12373634163339, w0=56.70000000000008, w1=7.739090271753998\n",
      "SubSGD iter. 81/499: loss=138.9758830349084, w0=57.400000000000084, w1=7.704399412412527\n",
      "SubSGD iter. 82/499: loss=537.5888595188748, w0=58.10000000000009, w1=8.717077753765293\n",
      "SubSGD iter. 83/499: loss=285.1350036389718, w0=58.80000000000009, w1=9.923094043535741\n",
      "SubSGD iter. 84/499: loss=15.486382439309976, w0=59.50000000000009, w1=9.433410510139153\n",
      "SubSGD iter. 85/499: loss=9.590455026060518, w0=60.200000000000095, w1=8.82061233913774\n",
      "SubSGD iter. 86/499: loss=173.92037058439212, w0=60.9000000000001, w1=9.329798183083993\n",
      "SubSGD iter. 87/499: loss=163.97262383664247, w0=61.6000000000001, w1=10.091155133703062\n",
      "SubSGD iter. 88/499: loss=6.344275520027849, w0=62.300000000000104, w1=9.847128308993911\n",
      "SubSGD iter. 89/499: loss=57.011043059154524, w0=63.00000000000011, w1=9.26281577144802\n",
      "SubSGD iter. 90/499: loss=95.23447477382882, w0=63.70000000000011, w1=9.278123438160156\n",
      "SubSGD iter. 91/499: loss=183.37743581230757, w0=64.4000000000001, w1=9.474468031677384\n",
      "SubSGD iter. 92/499: loss=26.11203007075674, w0=65.10000000000011, w1=9.12135228011007\n",
      "SubSGD iter. 93/499: loss=24.128511484378983, w0=65.80000000000011, w1=8.567245206113489\n",
      "SubSGD iter. 94/499: loss=0.049349358164379915, w0=65.10000000000011, w1=9.367075940176198\n",
      "SubSGD iter. 95/499: loss=5.354639934142725, w0=65.80000000000011, w1=8.59625287410801\n",
      "SubSGD iter. 96/499: loss=60.672000570638154, w0=66.50000000000011, w1=8.611560540820147\n",
      "SubSGD iter. 97/499: loss=77.6930195443017, w0=67.20000000000012, w1=9.345836284311947\n",
      "SubSGD iter. 98/499: loss=63.48883339907353, w0=67.90000000000012, w1=9.855022128258199\n",
      "SubSGD iter. 99/499: loss=8.26291957399585, w0=67.20000000000012, w1=10.589136451259188\n",
      "SubSGD iter. 100/499: loss=100.84602861352442, w0=67.90000000000012, w1=10.854620153483419\n",
      "SubSGD iter. 101/499: loss=15.228302675331628, w0=68.60000000000012, w1=10.300513079486837\n",
      "SubSGD iter. 102/499: loss=19.292810186858926, w0=69.30000000000013, w1=9.96415012181076\n",
      "SubSGD iter. 103/499: loss=2.126700054182059, w0=68.60000000000012, w1=10.3899362691015\n",
      "SubSGD iter. 104/499: loss=2.1805796602580156, w0=67.90000000000012, w1=11.573938469495392\n",
      "SubSGD iter. 105/499: loss=0.273674173085153, w0=67.20000000000012, w1=11.694443020974434\n",
      "SubSGD iter. 106/499: loss=2.3201405088109066, w0=66.50000000000011, w1=12.583274845140927\n",
      "SubSGD iter. 107/499: loss=66.05439416048601, w0=67.20000000000012, w1=11.843942045696984\n",
      "SubSGD iter. 108/499: loss=15.052877690734826, w0=67.90000000000012, w1=12.094223162159041\n",
      "SubSGD iter. 109/499: loss=0.39628930316950123, w0=67.20000000000012, w1=11.895462296340517\n",
      "SubSGD iter. 110/499: loss=72.99672222222529, w0=67.90000000000012, w1=11.65448238167387\n",
      "SubSGD iter. 111/499: loss=5002.96132262262, w0=68.60000000000012, w1=8.88418268478488\n",
      "SubSGD iter. 112/499: loss=13.605338942544323, w0=67.90000000000012, w1=9.324377939491852\n",
      "SubSGD iter. 113/499: loss=55.10044174925095, w0=68.60000000000012, w1=9.083398024825204\n",
      "SubSGD iter. 114/499: loss=87.3417206039045, w0=69.30000000000013, w1=9.91673560738478\n",
      "SubSGD iter. 115/499: loss=17.936951442224835, w0=70.00000000000013, w1=10.11917249742578\n",
      "SubSGD iter. 116/499: loss=80.18668619451064, w0=70.70000000000013, w1=10.851137090933415\n",
      "SubSGD iter. 117/499: loss=1.2130147904659838, w0=70.00000000000013, w1=11.221405181462211\n",
      "SubSGD iter. 118/499: loss=7.987973244004279, w0=69.30000000000013, w1=11.644644606234726\n",
      "SubSGD iter. 119/499: loss=7.334402063544476, w0=70.00000000000013, w1=12.345578603669479\n",
      "SubSGD iter. 120/499: loss=0.17765930180294992, w0=69.30000000000013, w1=12.54632814071873\n",
      "SubSGD iter. 121/499: loss=3.0875255163320894, w0=68.60000000000012, w1=12.728191984293899\n",
      "SubSGD iter. 122/499: loss=2.5977319400822956, w0=69.30000000000013, w1=13.063420039597153\n",
      "SubSGD iter. 123/499: loss=0.02111350072685585, w0=68.60000000000012, w1=14.006926523134274\n",
      "SubSGD iter. 124/499: loss=174.9561944685177, w0=69.30000000000013, w1=15.13205648326458\n",
      "SubSGD iter. 125/499: loss=3.7412627864859, w0=70.00000000000013, w1=14.347823755225257\n",
      "SubSGD iter. 126/499: loss=5.624606253091263, w0=69.30000000000013, w1=14.591850579934407\n",
      "SubSGD iter. 127/499: loss=3.493325835425343, w0=68.60000000000012, w1=14.937027667659406\n",
      "SubSGD iter. 128/499: loss=80.90505583944183, w0=69.30000000000013, w1=14.537569895272826\n",
      "SubSGD iter. 129/499: loss=1.9509010574025418, w0=70.00000000000013, w1=14.543997776151173\n",
      "SubSGD iter. 130/499: loss=6.168791817888145, w0=69.30000000000013, w1=15.102121479900212\n",
      "SubSGD iter. 131/499: loss=0.04614871432333141, w0=70.00000000000013, w1=16.1585227112791\n",
      "SubSGD iter. 132/499: loss=19.47337735556447, w0=70.70000000000013, w1=16.77318369240949\n",
      "SubSGD iter. 133/499: loss=36.36637027686244, w0=71.40000000000013, w1=17.436328009452538\n",
      "SubSGD iter. 134/499: loss=0.30555481177297256, w0=70.70000000000013, w1=17.620488787770856\n",
      "SubSGD iter. 135/499: loss=1.5058473534685364, w0=70.00000000000013, w1=16.52786427344506\n",
      "SubSGD iter. 136/499: loss=0.12564792967832034, w0=70.70000000000013, w1=15.957904148909076\n",
      "SubSGD iter. 137/499: loss=12.153259475831975, w0=71.40000000000013, w1=15.187081082840889\n",
      "SubSGD iter. 138/499: loss=1.515408500238565, w0=70.70000000000013, w1=14.691454822372057\n",
      "SubSGD iter. 139/499: loss=55.12996705211315, w0=71.40000000000013, w1=15.354599139415104\n",
      "SubSGD iter. 140/499: loss=21.049706518612076, w0=70.70000000000013, w1=15.832647341841227\n",
      "SubSGD iter. 141/499: loss=11.289670024739964, w0=70.00000000000013, w1=15.633886476022703\n",
      "SubSGD iter. 142/499: loss=2.344312711618044, w0=69.30000000000013, w1=16.169406158848908\n",
      "SubSGD iter. 143/499: loss=12.748423832016242, w0=68.60000000000012, w1=15.908693937467044\n",
      "SubSGD iter. 144/499: loss=0.4153285345762286, w0=67.90000000000012, w1=16.090557781042214\n",
      "SubSGD iter. 145/499: loss=88.01925284318551, w0=68.60000000000012, w1=17.27136248174211\n",
      "SubSGD iter. 146/499: loss=0.9216513333485168, w0=67.90000000000012, w1=16.065346191971663\n",
      "SubSGD iter. 147/499: loss=4.644629994892916, w0=68.60000000000012, w1=15.625657786886478\n",
      "SubSGD iter. 148/499: loss=59.31423282975518, w0=69.30000000000013, w1=15.891141489110709\n",
      "SubSGD iter. 149/499: loss=0.33300535837328926, w0=70.00000000000013, w1=15.051113917650898\n",
      "SubSGD iter. 150/499: loss=4.288132607757602, w0=69.30000000000013, w1=14.872761796271083\n",
      "SubSGD iter. 151/499: loss=23.12769516150622, w0=70.00000000000013, w1=16.41282413465827\n",
      "SubSGD iter. 152/499: loss=56.43820305934241, w0=70.70000000000013, w1=17.593628835358164\n",
      "SubSGD iter. 153/499: loss=8.603338485015081, w0=71.40000000000013, w1=18.325593428865798\n",
      "SubSGD iter. 154/499: loss=1.793859878120609, w0=70.70000000000013, w1=17.706460032921132\n",
      "SubSGD iter. 155/499: loss=63.30983026788252, w0=71.40000000000013, w1=17.356522101440817\n",
      "SubSGD iter. 156/499: loss=28.369522374151146, w0=72.10000000000014, w1=16.783512018426194\n",
      "SubSGD iter. 157/499: loss=4.597496835562552, w0=72.80000000000014, w1=16.92684260902071\n",
      "SubSGD iter. 158/499: loss=10.868434247393031, w0=73.50000000000014, w1=16.671410727919138\n",
      "SubSGD iter. 159/499: loss=23.402099952842004, w0=72.80000000000014, w1=16.493058606539325\n",
      "SubSGD iter. 160/499: loss=0.4206302221871962, w0=73.50000000000014, w1=15.549552123002204\n",
      "SubSGD iter. 161/499: loss=1.9664127415965118, w0=74.20000000000014, w1=14.572754644060286\n",
      "SubSGD iter. 162/499: loss=0.02295199359794018, w0=73.50000000000014, w1=15.26933939530105\n",
      "SubSGD iter. 163/499: loss=23.119006246747322, w0=74.20000000000014, w1=15.810296447272192\n",
      "SubSGD iter. 164/499: loss=4.979509735831026, w0=74.90000000000015, w1=15.975749239204818\n",
      "SubSGD iter. 165/499: loss=41.28935187501663, w0=74.20000000000014, w1=15.552207986150803\n",
      "SubSGD iter. 166/499: loss=2.199619846711444, w0=73.50000000000014, w1=14.71101781792616\n",
      "SubSGD iter. 167/499: loss=0.4010750793854229, w0=74.20000000000014, w1=15.089557601282891\n",
      "SubSGD iter. 168/499: loss=78.88028255995975, w0=74.90000000000015, w1=16.32887390226089\n",
      "SubSGD iter. 169/499: loss=6.58147006852587, w0=74.20000000000014, w1=15.709740506316225\n",
      "SubSGD iter. 170/499: loss=47.806985659843065, w0=73.50000000000014, w1=15.44902828493436\n",
      "SubSGD iter. 171/499: loss=51.08549130942773, w0=72.80000000000014, w1=15.72050846691694\n",
      "SubSGD iter. 172/499: loss=2.9553669475443565, w0=73.50000000000014, w1=15.99064691720661\n",
      "SubSGD iter. 173/499: loss=10.649896381481755, w0=74.20000000000014, w1=16.01701956783143\n",
      "SubSGD iter. 174/499: loss=21.37285636855883, w0=73.50000000000014, w1=16.57514327158047\n",
      "SubSGD iter. 175/499: loss=23.464816036870015, w0=74.20000000000014, w1=17.755947972280364\n",
      "SubSGD iter. 176/499: loss=3.6339789029562515, w0=74.90000000000015, w1=17.92140076421299\n",
      "SubSGD iter. 177/499: loss=104.12651953799917, w0=74.20000000000014, w1=16.963265673311184\n",
      "SubSGD iter. 178/499: loss=1.7737498914824268, w0=73.50000000000014, w1=15.844955452699702\n",
      "SubSGD iter. 179/499: loss=0.22170846836110375, w0=72.80000000000014, w1=16.62918818073903\n",
      "SubSGD iter. 180/499: loss=48.68713115540498, w0=72.10000000000014, w1=16.080793676981216\n",
      "SubSGD iter. 181/499: loss=1.542321379560266, w0=71.40000000000013, w1=16.920821248441026\n",
      "SubSGD iter. 182/499: loss=0.6895185478593424, w0=70.70000000000013, w1=16.17483559485363\n",
      "SubSGD iter. 183/499: loss=3.134206380763706, w0=71.40000000000013, w1=15.390602866814305\n",
      "SubSGD iter. 184/499: loss=7.708547259075235, w0=72.10000000000014, w1=16.185772591931652\n",
      "SubSGD iter. 185/499: loss=42.83037147397776, w0=71.40000000000013, w1=15.227637501029845\n",
      "SubSGD iter. 186/499: loss=0.1879536907604268, w0=72.10000000000014, w1=14.326005236617087\n",
      "SubSGD iter. 187/499: loss=0.6266679407728639, w0=71.40000000000013, w1=14.696273327145883\n",
      "SubSGD iter. 188/499: loss=0.860831979240137, w0=70.70000000000013, w1=15.135961732231069\n",
      "SubSGD iter. 189/499: loss=2.625148624135112, w0=71.40000000000013, w1=14.33613099816836\n",
      "SubSGD iter. 190/499: loss=24.566672301089575, w0=70.70000000000013, w1=14.616059990604823\n",
      "SubSGD iter. 191/499: loss=5.639657713403299, w0=70.00000000000013, w1=14.963143976224814\n",
      "SubSGD iter. 192/499: loss=0.8185936403014782, w0=69.30000000000013, w1=15.575942147226225\n",
      "SubSGD iter. 193/499: loss=29.321830389294707, w0=70.00000000000013, w1=16.30790674073386\n",
      "SubSGD iter. 194/499: loss=0.18130181033628756, w0=70.70000000000013, w1=15.57379241773287\n",
      "SubSGD iter. 195/499: loss=10.627348266362924, w0=71.40000000000013, w1=15.823390880420048\n",
      "SubSGD iter. 196/499: loss=2.994218063566902, w0=70.70000000000013, w1=16.32840861224437\n",
      "SubSGD iter. 197/499: loss=9.359603916691373, w0=71.40000000000013, w1=16.980465509686827\n",
      "SubSGD iter. 198/499: loss=72.15026852397065, w0=72.10000000000014, w1=18.219781810664827\n",
      "SubSGD iter. 199/499: loss=15.207510086912121, w0=72.80000000000014, w1=17.523197059424064\n",
      "SubSGD iter. 200/499: loss=3.4831603649728557, w0=73.50000000000014, w1=18.241360383455252\n",
      "SubSGD iter. 201/499: loss=0.19093840016813682, w0=72.80000000000014, w1=18.038923493414252\n",
      "SubSGD iter. 202/499: loss=0.9772922671127839, w0=73.50000000000014, w1=18.6553104422515\n",
      "SubSGD iter. 203/499: loss=0.007713483509277083, w0=72.80000000000014, w1=18.67388990115038\n",
      "SubSGD iter. 204/499: loss=4.559130445677839, w0=72.10000000000014, w1=19.232013604899418\n",
      "SubSGD iter. 205/499: loss=11.171374969129246, w0=71.40000000000013, w1=18.63969564363124\n",
      "SubSGD iter. 206/499: loss=27.854158184816097, w0=70.70000000000013, w1=17.27694424823949\n",
      "SubSGD iter. 207/499: loss=0.24392066354400951, w0=70.00000000000013, w1=17.781961980063812\n",
      "SubSGD iter. 208/499: loss=2.307078140384641, w0=70.70000000000013, w1=17.047847657062825\n",
      "SubSGD iter. 209/499: loss=4.441466482229475, w0=70.00000000000013, w1=16.346913659628072\n",
      "SubSGD iter. 210/499: loss=0.6584533384941591, w0=70.70000000000013, w1=17.188103827852714\n",
      "SubSGD iter. 211/499: loss=21.813243771971802, w0=70.00000000000013, w1=16.019773027335244\n",
      "SubSGD iter. 212/499: loss=0.023084390154164636, w0=70.70000000000013, w1=15.624336235392239\n",
      "SubSGD iter. 213/499: loss=0.06423883664484605, w0=70.00000000000013, w1=15.019804129538079\n",
      "SubSGD iter. 214/499: loss=15.089040798027865, w0=70.70000000000013, w1=15.163134720132595\n",
      "SubSGD iter. 215/499: loss=8.32479973979989, w0=71.40000000000013, w1=15.139168880329503\n",
      "SubSGD iter. 216/499: loss=6.76109045633815, w0=70.70000000000013, w1=15.691140312267182\n",
      "SubSGD iter. 217/499: loss=0.5048137964598676, w0=71.40000000000013, w1=15.034259344514219\n",
      "SubSGD iter. 218/499: loss=0.7503279621122013, w0=70.70000000000013, w1=15.23500888156347\n",
      "SubSGD iter. 219/499: loss=22.954037979657056, w0=71.40000000000013, w1=14.650696344017579\n",
      "SubSGD iter. 220/499: loss=7.136324339442827e-05, w0=70.70000000000013, w1=13.566820947326278\n",
      "SubSGD iter. 221/499: loss=4.54479194544456, w0=71.40000000000013, w1=14.185954343270945\n",
      "SubSGD iter. 222/499: loss=5.377781567377591, w0=72.10000000000014, w1=14.564494126627675\n",
      "SubSGD iter. 223/499: loss=121.52213049469908, w0=72.80000000000014, w1=15.803810427605676\n",
      "SubSGD iter. 224/499: loss=0.7853454043188431, w0=72.10000000000014, w1=15.671486901251837\n",
      "SubSGD iter. 225/499: loss=0.1395284935200398, w0=71.40000000000013, w1=15.539163374898\n",
      "SubSGD iter. 226/499: loss=10.364361989869575, w0=70.70000000000013, w1=14.37083257438053\n",
      "SubSGD iter. 227/499: loss=0.10717969155564694, w0=70.00000000000013, w1=13.875206313911699\n",
      "SubSGD iter. 228/499: loss=11.980388687353727, w0=69.30000000000013, w1=13.614494092529835\n",
      "SubSGD iter. 229/499: loss=17.302101410984566, w0=70.00000000000013, w1=13.703021606018961\n",
      "SubSGD iter. 230/499: loss=18.172850932176804, w0=70.70000000000013, w1=12.849175335593692\n",
      "SubSGD iter. 231/499: loss=126.97360535868162, w0=71.40000000000013, w1=14.029980036293589\n",
      "SubSGD iter. 232/499: loss=36.62237027868312, w0=70.70000000000013, w1=14.301460218276167\n",
      "SubSGD iter. 233/499: loss=12.07166432144825, w0=70.00000000000013, w1=15.137499075548984\n",
      "SubSGD iter. 234/499: loss=14.88500714454403, w0=70.70000000000013, w1=15.407637525838654\n",
      "SubSGD iter. 235/499: loss=0.6458665877426982, w0=71.40000000000013, w1=16.141913269330452\n",
      "SubSGD iter. 236/499: loss=17.705586359481238, w0=70.70000000000013, w1=16.619961471756575\n",
      "SubSGD iter. 237/499: loss=11.269546784820541, w0=71.40000000000013, w1=17.236348420593824\n",
      "SubSGD iter. 238/499: loss=0.10286263048989032, w0=70.70000000000013, w1=16.947851156036407\n",
      "SubSGD iter. 239/499: loss=4.229083985026793, w0=71.40000000000013, w1=15.821918742535235\n",
      "SubSGD iter. 240/499: loss=7.18987713222642, w0=72.10000000000014, w1=16.092057192824907\n",
      "SubSGD iter. 241/499: loss=49.53258080528018, w0=71.40000000000013, w1=15.486654041380921\n",
      "SubSGD iter. 242/499: loss=0.8532597729309418, w0=70.70000000000013, w1=14.402778644689619\n",
      "SubSGD iter. 243/499: loss=0.055054473861403615, w0=70.00000000000013, w1=13.701844647254866\n",
      "SubSGD iter. 244/499: loss=35.262453434762186, w0=70.70000000000013, w1=14.316505628385253\n",
      "SubSGD iter. 245/499: loss=5.317973917148296, w0=70.00000000000013, w1=14.806189161781841\n",
      "SubSGD iter. 246/499: loss=6.411306228692336, w0=70.70000000000013, w1=15.149901782965232\n",
      "SubSGD iter. 247/499: loss=0.9335389638311812, w0=70.00000000000013, w1=14.794874305435407\n",
      "SubSGD iter. 248/499: loss=16.708309792984174, w0=70.70000000000013, w1=15.913184526046889\n",
      "SubSGD iter. 249/499: loss=50.337816080086036, w0=71.40000000000013, w1=15.38855160133515\n",
      "SubSGD iter. 250/499: loss=5.890205861217888, w0=72.10000000000014, w1=16.506861821946632\n",
      "SubSGD iter. 251/499: loss=4.895386225720638, w0=71.40000000000013, w1=16.99654535534322\n",
      "SubSGD iter. 252/499: loss=1.9932991541549385, w0=72.10000000000014, w1=16.375493781772626\n",
      "SubSGD iter. 253/499: loss=0.18588127070519997, w0=72.80000000000014, w1=15.285698758699406\n",
      "SubSGD iter. 254/499: loss=8.774452606601335, w0=73.50000000000014, w1=15.909186880985583\n",
      "SubSGD iter. 255/499: loss=16.237642556569444, w0=72.80000000000014, w1=16.349382135692554\n",
      "SubSGD iter. 256/499: loss=32.128672089798684, w0=73.50000000000014, w1=16.108402221025905\n",
      "SubSGD iter. 257/499: loss=0.37900849623877847, w0=72.80000000000014, w1=16.313235740901128\n",
      "SubSGD iter. 258/499: loss=20.678218539958113, w0=73.50000000000014, w1=16.578719443125358\n",
      "SubSGD iter. 259/499: loss=1.9678330081637379, w0=74.20000000000014, w1=17.195106391962607\n",
      "SubSGD iter. 260/499: loss=7.928129363864174, w0=73.50000000000014, w1=16.93032128277184\n",
      "SubSGD iter. 261/499: loss=3.808917538270463, w0=72.80000000000014, w1=16.311187886827174\n",
      "SubSGD iter. 262/499: loss=20.544881588162813, w0=73.50000000000014, w1=15.457341616401905\n",
      "SubSGD iter. 263/499: loss=0.3623472079490021, w0=72.80000000000014, w1=15.567515733282645\n",
      "SubSGD iter. 264/499: loss=1.5248688857108332, w0=73.50000000000014, w1=16.685825953894128\n",
      "SubSGD iter. 265/499: loss=2.2393078476297243, w0=72.80000000000014, w1=17.574657778060622\n",
      "SubSGD iter. 266/499: loss=8.899119941509053, w0=72.10000000000014, w1=16.94349243806577\n",
      "SubSGD iter. 267/499: loss=8.580391601142653, w0=71.40000000000013, w1=17.12535628164094\n",
      "SubSGD iter. 268/499: loss=2.2280126259787925, w0=72.10000000000014, w1=16.504304708070347\n",
      "SubSGD iter. 269/499: loss=46.23773982394378, w0=72.80000000000014, w1=16.944229150954527\n",
      "SubSGD iter. 270/499: loss=19.25415303332597, w0=73.50000000000014, w1=17.485186202925668\n",
      "SubSGD iter. 271/499: loss=1.4239754960922317, w0=72.80000000000014, w1=17.910972350216408\n",
      "SubSGD iter. 272/499: loss=2.604587790722479, w0=72.10000000000014, w1=17.660691233754353\n",
      "SubSGD iter. 273/499: loss=0.0031407228281262966, w0=72.80000000000014, w1=18.004403854937742\n",
      "SubSGD iter. 274/499: loss=4.363926509663449, w0=73.50000000000014, w1=17.06089737140062\n",
      "SubSGD iter. 275/499: loss=42.53257997832722, w0=74.20000000000014, w1=16.321564571956678\n",
      "SubSGD iter. 276/499: loss=2.384260586485672, w0=74.90000000000015, w1=16.964494270720472\n",
      "SubSGD iter. 277/499: loss=2.382709385068819, w0=75.60000000000015, w1=17.1299470626531\n",
      "SubSGD iter. 278/499: loss=12.813642122356738, w0=74.90000000000015, w1=17.314107840971417\n",
      "SubSGD iter. 279/499: loss=40.40622423251632, w0=74.20000000000014, w1=16.119369003866993\n",
      "SubSGD iter. 280/499: loss=12.933850109478225, w0=73.50000000000014, w1=15.035493607175692\n",
      "SubSGD iter. 281/499: loss=0.2163226951024146, w0=74.20000000000014, w1=15.914665919406499\n",
      "SubSGD iter. 282/499: loss=0.07649928260461826, w0=73.50000000000014, w1=14.374603581019313\n",
      "SubSGD iter. 283/499: loss=4.851699508249239, w0=72.80000000000014, w1=14.917620042384412\n",
      "SubSGD iter. 284/499: loss=7.4196040982905584, w0=72.10000000000014, w1=15.757647613844222\n",
      "SubSGD iter. 285/499: loss=23.89584747996381, w0=71.40000000000013, w1=16.235695816270344\n",
      "SubSGD iter. 286/499: loss=6.181321556241548, w0=72.10000000000014, w1=16.881843049657565\n",
      "SubSGD iter. 287/499: loss=55.40311816100088, w0=72.80000000000014, w1=16.49971706510723\n",
      "SubSGD iter. 288/499: loss=2.5800786080542584, w0=73.50000000000014, w1=17.13199149358892\n",
      "SubSGD iter. 289/499: loss=1.1200046545409263, w0=74.20000000000014, w1=17.74837844242617\n",
      "SubSGD iter. 290/499: loss=36.29601279366548, w0=74.90000000000015, w1=16.39442154839294\n",
      "SubSGD iter. 291/499: loss=1.5964298111564752, w0=74.20000000000014, w1=15.561083965833365\n",
      "SubSGD iter. 292/499: loss=0.034864597637964255, w0=73.50000000000014, w1=15.182544182476633\n",
      "SubSGD iter. 293/499: loss=21.46402776801225, w0=72.80000000000014, w1=15.527721270201631\n",
      "SubSGD iter. 294/499: loss=66.81484364259323, w0=72.10000000000014, w1=15.471411655156327\n",
      "SubSGD iter. 295/499: loss=10.420884760110496, w0=72.80000000000014, w1=15.486719321868463\n",
      "SubSGD iter. 296/499: loss=3.1787848215229837, w0=73.50000000000014, w1=15.756857772158133\n",
      "SubSGD iter. 297/499: loss=5.532519719895663, w0=72.80000000000014, w1=14.851682668971069\n",
      "SubSGD iter. 298/499: loss=34.020371988588025, w0=73.50000000000014, w1=15.514826986014116\n",
      "SubSGD iter. 299/499: loss=2.9550880410807543, w0=72.80000000000014, w1=15.865163919382507\n",
      "SubSGD iter. 300/499: loss=47.03143658442801, w0=73.50000000000014, w1=16.109476643091643\n",
      "SubSGD iter. 301/499: loss=29.154528815221923, w0=74.20000000000014, w1=17.290281343791538\n",
      "SubSGD iter. 302/499: loss=5.365424056733115, w0=73.50000000000014, w1=17.00178407923412\n",
      "SubSGD iter. 303/499: loss=0.4484247947069312, w0=72.80000000000014, w1=16.94962373370441\n",
      "SubSGD iter. 304/499: loss=0.9025257841340995, w0=72.10000000000014, w1=16.224603543623605\n",
      "SubSGD iter. 305/499: loss=0.3076420884053381, w0=72.80000000000014, w1=16.568316164806994\n",
      "SubSGD iter. 306/499: loss=10.581166781767527, w0=72.10000000000014, w1=15.867382167372241\n",
      "SubSGD iter. 307/499: loss=59.9721887706227, w0=72.80000000000014, w1=16.215349213762657\n",
      "SubSGD iter. 308/499: loss=6.775801195085312, w0=73.50000000000014, w1=16.85827891252645\n",
      "SubSGD iter. 309/499: loss=8.359068716616566, w0=72.80000000000014, w1=17.281518337298966\n",
      "SubSGD iter. 310/499: loss=21.02903997190778, w0=72.10000000000014, w1=16.225117105920077\n",
      "SubSGD iter. 311/499: loss=9.082094043424375, w0=71.40000000000013, w1=16.507664925006303\n",
      "SubSGD iter. 312/499: loss=40.084713315742576, w0=72.10000000000014, w1=15.980476286259076\n",
      "SubSGD iter. 313/499: loss=2.4543932356931024, w0=71.40000000000013, w1=15.075301183072012\n",
      "SubSGD iter. 314/499: loss=22.72691305589337, w0=70.70000000000013, w1=14.814588961690148\n",
      "SubSGD iter. 315/499: loss=20.637818524089617, w0=71.40000000000013, w1=14.230276424144257\n",
      "SubSGD iter. 316/499: loss=6.40013011335734, w0=70.70000000000013, w1=14.653515848916772\n",
      "SubSGD iter. 317/499: loss=19.638860675150678, w0=70.00000000000013, w1=15.131564051342895\n",
      "SubSGD iter. 318/499: loss=81.73632300982233, w0=70.70000000000013, w1=16.31236875204279\n",
      "SubSGD iter. 319/499: loss=0.6411836386237739, w0=70.00000000000013, w1=15.432638622210312\n",
      "SubSGD iter. 320/499: loss=2.813601131470115, w0=70.70000000000013, w1=14.648405894170988\n",
      "SubSGD iter. 321/499: loss=0.20620317794827442, w0=70.00000000000013, w1=15.088094299256174\n",
      "SubSGD iter. 322/499: loss=42.76269810543907, w0=70.70000000000013, w1=14.657762710795437\n",
      "SubSGD iter. 323/499: loss=2.5965302318226886, w0=70.00000000000013, w1=15.49918122052138\n",
      "SubSGD iter. 324/499: loss=10.377467588480298, w0=69.30000000000013, w1=15.524478985910223\n",
      "SubSGD iter. 325/499: loss=0.24560594339669686, w0=70.00000000000013, w1=16.88723038130197\n",
      "SubSGD iter. 326/499: loss=11.975321294827115, w0=70.70000000000013, w1=16.52130965057157\n",
      "SubSGD iter. 327/499: loss=0.00793407202833712, w0=71.40000000000013, w1=15.787195327570581\n",
      "SubSGD iter. 328/499: loss=66.12307127079558, w0=70.70000000000013, w1=15.907087141058913\n",
      "SubSGD iter. 329/499: loss=4.4799388956771296, w0=71.40000000000013, w1=16.74042472361849\n",
      "SubSGD iter. 330/499: loss=29.577541829874548, w0=72.10000000000014, w1=17.353417220802047\n",
      "SubSGD iter. 331/499: loss=9.572151829602515, w0=72.80000000000014, w1=17.877597772253765\n",
      "SubSGD iter. 332/499: loss=0.004703517673038867, w0=73.50000000000014, w1=16.98876594808727\n",
      "SubSGD iter. 333/499: loss=0.5909186035456218, w0=72.80000000000014, w1=17.53178240945237\n",
      "SubSGD iter. 334/499: loss=8.42855592172439, w0=72.10000000000014, w1=16.92725030359821\n",
      "SubSGD iter. 335/499: loss=11.268703627706428, w0=72.80000000000014, w1=17.939928644950978\n",
      "SubSGD iter. 336/499: loss=9.391928237444368, w0=72.10000000000014, w1=16.847304130625183\n",
      "SubSGD iter. 337/499: loss=76.31910368400476, w0=71.40000000000013, w1=16.861059326600675\n",
      "SubSGD iter. 338/499: loss=5.462355176110402, w0=70.70000000000013, w1=17.1436071456869\n",
      "SubSGD iter. 339/499: loss=0.25149791866371246, w0=71.40000000000013, w1=16.302188635960956\n",
      "SubSGD iter. 340/499: loss=0.00018552141456610203, w0=72.10000000000014, w1=16.344515192564447\n",
      "SubSGD iter. 341/499: loss=1.4422071199305508, w0=72.80000000000014, w1=16.309824333222977\n",
      "SubSGD iter. 342/499: loss=0.006238636312113559, w0=72.10000000000014, w1=16.5146578530982\n",
      "SubSGD iter. 343/499: loss=6.735451781918626, w0=71.40000000000013, w1=17.350696710371015\n",
      "SubSGD iter. 344/499: loss=21.75688895847134, w0=72.10000000000014, w1=16.373899231429096\n",
      "SubSGD iter. 345/499: loss=0.23466293059883914, w0=71.40000000000013, w1=16.331572674825605\n",
      "SubSGD iter. 346/499: loss=0.009965301089631904, w0=70.70000000000013, w1=16.32514479394726\n",
      "SubSGD iter. 347/499: loss=12.99830675377368, w0=70.00000000000013, w1=15.901603540893245\n",
      "SubSGD iter. 348/499: loss=27.482579954252685, w0=70.70000000000013, w1=15.347496466896663\n",
      "SubSGD iter. 349/499: loss=1.456416826256971, w0=71.40000000000013, w1=15.14266294702144\n",
      "SubSGD iter. 350/499: loss=68.30677933009687, w0=70.70000000000013, w1=15.15641814299693\n",
      "SubSGD iter. 351/499: loss=8.19148103214546, w0=71.40000000000013, w1=14.385595076928743\n",
      "SubSGD iter. 352/499: loss=6.386754793051558, w0=72.10000000000014, w1=15.26476738915955\n",
      "SubSGD iter. 353/499: loss=3.7019371254730644, w0=72.80000000000014, w1=15.137752943931524\n",
      "SubSGD iter. 354/499: loss=39.37718005067908, w0=73.50000000000014, w1=14.755626959381189\n",
      "SubSGD iter. 355/499: loss=12.106752573645283, w0=74.20000000000014, w1=15.279807510832907\n",
      "SubSGD iter. 356/499: loss=43.78823687358666, w0=74.90000000000015, w1=14.660666355418524\n",
      "SubSGD iter. 357/499: loss=18.258695074327917, w0=74.20000000000014, w1=15.468780096460456\n",
      "SubSGD iter. 358/499: loss=45.36637617922081, w0=74.90000000000015, w1=14.849638941046074\n",
      "SubSGD iter. 359/499: loss=5.307795385374629, w0=75.60000000000015, w1=14.876011591670894\n",
      "SubSGD iter. 360/499: loss=30.185508803852866, w0=74.90000000000015, w1=15.365695125067482\n",
      "SubSGD iter. 361/499: loss=5801.475694893664, w0=75.60000000000015, w1=12.595395428178493\n",
      "SubSGD iter. 362/499: loss=5.7859043529699345, w0=74.90000000000015, w1=12.705569545059234\n",
      "SubSGD iter. 363/499: loss=2.176238351273292, w0=74.20000000000014, w1=12.417072280501818\n",
      "SubSGD iter. 364/499: loss=50.8545559473894, w0=74.90000000000015, w1=12.856996723386\n",
      "SubSGD iter. 365/499: loss=30.699300873487076, w0=75.60000000000015, w1=13.869675064738768\n",
      "SubSGD iter. 366/499: loss=0.5326485284783784, w0=74.90000000000015, w1=13.996689509966794\n",
      "SubSGD iter. 367/499: loss=45.449965062499125, w0=74.20000000000014, w1=14.505190627706627\n",
      "SubSGD iter. 368/499: loss=11.256101358848122, w0=73.50000000000014, w1=14.861916146062464\n",
      "SubSGD iter. 369/499: loss=10.667655116185806, w0=72.80000000000014, w1=15.701943717522274\n",
      "SubSGD iter. 370/499: loss=0.44264385883462026, w0=73.50000000000014, w1=15.86774997708977\n",
      "SubSGD iter. 371/499: loss=1.7769963445114296, w0=74.20000000000014, w1=15.684284198039352\n",
      "SubSGD iter. 372/499: loss=0.5462000687839322, w0=74.90000000000015, w1=15.954422648329022\n",
      "SubSGD iter. 373/499: loss=25.647107620149306, w0=75.60000000000015, w1=16.394347091213202\n",
      "SubSGD iter. 374/499: loss=13.68454397796297, w0=76.30000000000015, w1=16.153367176546553\n",
      "SubSGD iter. 375/499: loss=0.00033060152722363045, w0=77.00000000000016, w1=15.176569697604634\n",
      "SubSGD iter. 376/499: loss=45.26935941087943, w0=76.30000000000015, w1=15.71208938043084\n",
      "SubSGD iter. 377/499: loss=0.0014063649744292862, w0=75.60000000000015, w1=15.09570243159359\n",
      "SubSGD iter. 378/499: loss=26.31377305912301, w0=74.90000000000015, w1=15.518941856366105\n",
      "SubSGD iter. 379/499: loss=39.2737033666938, w0=74.20000000000014, w1=15.32018099054758\n",
      "SubSGD iter. 380/499: loss=5.3517999507232075, w0=74.90000000000015, w1=15.485633782480207\n",
      "SubSGD iter. 381/499: loss=31.82515235040378, w0=74.20000000000014, w1=15.636090458931335\n",
      "SubSGD iter. 382/499: loss=0.4063264603378411, w0=74.90000000000015, w1=16.08627020513763\n",
      "SubSGD iter. 383/499: loss=7.755892155829987, w0=74.20000000000014, w1=16.51205635242837\n",
      "SubSGD iter. 384/499: loss=22.60602548979467, w0=73.50000000000014, w1=15.45565512104948\n",
      "SubSGD iter. 385/499: loss=19.63022040999861, w0=72.80000000000014, w1=14.798472417854494\n",
      "SubSGD iter. 386/499: loss=7.14733943773275, w0=72.10000000000014, w1=15.532586740855484\n",
      "SubSGD iter. 387/499: loss=2.377531612336207, w0=71.40000000000013, w1=16.421418565021977\n",
      "SubSGD iter. 388/499: loss=0.8371516716761103, w0=72.10000000000014, w1=17.961480903409164\n",
      "SubSGD iter. 389/499: loss=5.285000321369501, w0=72.80000000000014, w1=18.974159244761932\n",
      "SubSGD iter. 390/499: loss=60.15550460767639, w0=73.50000000000014, w1=18.592033260211597\n",
      "SubSGD iter. 391/499: loss=24.963594313444407, w0=72.80000000000014, w1=17.53411795338404\n",
      "SubSGD iter. 392/499: loss=50.0250052641949, w0=73.50000000000014, w1=17.13466018099746\n",
      "SubSGD iter. 393/499: loss=2.4979976158041697, w0=72.80000000000014, w1=16.409639990916656\n",
      "SubSGD iter. 394/499: loss=25.689332237115764, w0=72.10000000000014, w1=16.88768819334278\n",
      "SubSGD iter. 395/499: loss=40.098945726471456, w0=72.80000000000014, w1=15.53373129930955\n",
      "SubSGD iter. 396/499: loss=4.830321409769376, w0=72.10000000000014, w1=16.103691423845536\n",
      "SubSGD iter. 397/499: loss=1.9941952113082928, w0=72.80000000000014, w1=15.303860689782827\n",
      "SubSGD iter. 398/499: loss=2.8075919220878176, w0=72.10000000000014, w1=13.94110929439108\n",
      "SubSGD iter. 399/499: loss=39.86437673866658, w0=71.40000000000013, w1=14.226283788940373\n",
      "SubSGD iter. 400/499: loss=2.1774416591389074, w0=70.70000000000013, w1=14.58300930729621\n",
      "SubSGD iter. 401/499: loss=0.2131278719975753, w0=70.00000000000013, w1=15.00879545458695\n",
      "SubSGD iter. 402/499: loss=1.193181941678593, w0=69.30000000000013, w1=15.089609760075662\n",
      "SubSGD iter. 403/499: loss=10.798579903290243, w0=68.60000000000012, w1=14.828897538693798\n",
      "SubSGD iter. 404/499: loss=0.6293413243997183, w0=67.90000000000012, w1=15.380868970631477\n",
      "SubSGD iter. 405/499: loss=19.086953311691204, w0=68.60000000000012, w1=15.890054814577729\n",
      "SubSGD iter. 406/499: loss=22.785663352078576, w0=69.30000000000013, w1=16.746276867082265\n",
      "SubSGD iter. 407/499: loss=85.18930253904848, w0=70.00000000000013, w1=16.346819094695686\n",
      "SubSGD iter. 408/499: loss=3.988590796006795, w0=70.70000000000013, w1=16.690531715879075\n",
      "SubSGD iter. 409/499: loss=110.18381572571852, w0=71.40000000000013, w1=17.03499996274954\n",
      "SubSGD iter. 410/499: loss=8.59091154175362, w0=70.70000000000013, w1=17.18545663920067\n",
      "SubSGD iter. 411/499: loss=0.035752722032275065, w0=71.40000000000013, w1=17.473953903758087\n",
      "SubSGD iter. 412/499: loss=0.6507953092918768, w0=70.70000000000013, w1=16.85482050781342\n",
      "SubSGD iter. 413/499: loss=34.37081651132338, w0=71.40000000000013, w1=17.46781300499698\n",
      "SubSGD iter. 414/499: loss=13.632663953764952, w0=72.10000000000014, w1=17.483120671709116\n",
      "SubSGD iter. 415/499: loss=18.99195246846863, w0=72.80000000000014, w1=17.14675771403304\n",
      "SubSGD iter. 416/499: loss=1.1991760252172812, w0=72.10000000000014, w1=17.140329833154695\n",
      "SubSGD iter. 417/499: loss=7.128786299875429, w0=71.40000000000013, w1=16.785302355624868\n",
      "SubSGD iter. 418/499: loss=5.563892517921881, w0=70.70000000000013, w1=17.067850174711094\n",
      "SubSGD iter. 419/499: loss=17.584630057857172, w0=71.40000000000013, w1=17.08315784142323\n",
      "SubSGD iter. 420/499: loss=0.6493961016245542, w0=70.70000000000013, w1=16.81837273223246\n",
      "SubSGD iter. 421/499: loss=12.658817324814889, w0=70.00000000000013, w1=16.619611866413937\n",
      "SubSGD iter. 422/499: loss=27.01251528244674, w0=70.70000000000013, w1=16.364179985312365\n",
      "SubSGD iter. 423/499: loss=0.0007534386698669634, w0=70.00000000000013, w1=17.098294308313353\n",
      "SubSGD iter. 424/499: loss=117.72109681665988, w0=70.70000000000013, w1=17.44276255518382\n",
      "SubSGD iter. 425/499: loss=0.936839461752106, w0=71.40000000000013, w1=16.634648814141887\n",
      "SubSGD iter. 426/499: loss=18.546146098321074, w0=70.70000000000013, w1=16.91457780657835\n",
      "SubSGD iter. 427/499: loss=0.7165206804272376, w0=71.40000000000013, w1=16.09591943677272\n",
      "SubSGD iter. 428/499: loss=9.601589895514339, w0=72.10000000000014, w1=15.325096370704534\n",
      "SubSGD iter. 429/499: loss=21.012856247956336, w0=72.80000000000014, w1=15.362383682322404\n",
      "SubSGD iter. 430/499: loss=1.4142255572871163, w0=73.50000000000014, w1=14.758617612355891\n",
      "SubSGD iter. 431/499: loss=26.258112425206136, w0=72.80000000000014, w1=14.559856746537367\n",
      "SubSGD iter. 432/499: loss=34.00388759768653, w0=72.10000000000014, w1=14.011462242779556\n",
      "SubSGD iter. 433/499: loss=18.223231876069026, w0=71.40000000000013, w1=14.569585946528596\n",
      "SubSGD iter. 434/499: loss=0.7186285174395631, w0=70.70000000000013, w1=13.868651949093843\n",
      "SubSGD iter. 435/499: loss=141.5941231029055, w0=71.40000000000013, w1=14.99378190922415\n",
      "SubSGD iter. 436/499: loss=8.237587616038027, w0=70.70000000000013, w1=15.276329728310376\n",
      "SubSGD iter. 437/499: loss=9.29470814180696, w0=71.40000000000013, w1=16.394639948921856\n",
      "SubSGD iter. 438/499: loss=19.330993792639276, w0=72.10000000000014, w1=17.407318290274624\n",
      "SubSGD iter. 439/499: loss=5.990868469271412, w0=71.40000000000013, w1=17.07209023497137\n",
      "SubSGD iter. 440/499: loss=17.49604946220408, w0=70.70000000000013, w1=17.352019227407833\n",
      "SubSGD iter. 441/499: loss=15.088641913757142, w0=71.40000000000013, w1=17.548539310186335\n",
      "SubSGD iter. 442/499: loss=55.24132779820157, w0=72.10000000000014, w1=17.79285203389547\n",
      "SubSGD iter. 443/499: loss=83.2351318019714, w0=72.80000000000014, w1=18.137320280765937\n",
      "SubSGD iter. 444/499: loss=1.9385990038352674, w0=72.10000000000014, w1=17.688661241981226\n",
      "SubSGD iter. 445/499: loss=1.3446587837919723, w0=71.40000000000013, w1=18.313477341792172\n",
      "SubSGD iter. 446/499: loss=5.082511354857741, w0=72.10000000000014, w1=18.402004855281298\n",
      "SubSGD iter. 447/499: loss=0.009513495466836953, w0=72.80000000000014, w1=18.51230488874209\n",
      "SubSGD iter. 448/499: loss=67.99216551764812, w0=73.50000000000014, w1=17.47479069492855\n",
      "SubSGD iter. 449/499: loss=0.03707006902361361, w0=74.20000000000014, w1=18.120937928315772\n",
      "SubSGD iter. 450/499: loss=21.900317983645934, w0=73.50000000000014, w1=17.62531166784694\n",
      "SubSGD iter. 451/499: loss=2.3344033393764123, w0=72.80000000000014, w1=17.58298511124345\n",
      "SubSGD iter. 452/499: loss=1.4484992020854508, w0=72.10000000000014, w1=17.978421903186455\n",
      "SubSGD iter. 453/499: loss=1.8471173561068772, w0=71.40000000000013, w1=18.81446076045927\n",
      "SubSGD iter. 454/499: loss=6.143922326566467, w0=72.10000000000014, w1=18.79049492065618\n",
      "SubSGD iter. 455/499: loss=5.688002818966659, w0=71.40000000000013, w1=19.135672008381178\n",
      "SubSGD iter. 456/499: loss=36.580231873588836, w0=72.10000000000014, w1=19.24949233519549\n",
      "SubSGD iter. 457/499: loss=4.577113086685355, w0=71.40000000000013, w1=18.524472145114686\n",
      "SubSGD iter. 458/499: loss=0.5455683565685427, w0=72.10000000000014, w1=18.098685997823946\n",
      "SubSGD iter. 459/499: loss=0.38000432342851864, w0=71.40000000000013, w1=17.56898502376183\n",
      "SubSGD iter. 460/499: loss=94.01632788075266, w0=72.10000000000014, w1=17.913453270632296\n",
      "SubSGD iter. 461/499: loss=7272.171617696723, w0=72.80000000000014, w1=15.143153573743307\n",
      "SubSGD iter. 462/499: loss=30.97721967786441, w0=73.50000000000014, w1=14.79321564226299\n",
      "SubSGD iter. 463/499: loss=8.389385387078107, w0=74.20000000000014, w1=14.958668434195616\n",
      "SubSGD iter. 464/499: loss=4.176483004364039, w0=74.90000000000015, w1=15.42525056016219\n",
      "SubSGD iter. 465/499: loss=1.9693390924900744, w0=75.60000000000015, w1=16.03991154129258\n",
      "SubSGD iter. 466/499: loss=1.1837033533531047, w0=76.30000000000015, w1=15.784479660191009\n",
      "SubSGD iter. 467/499: loss=0.09036274456179046, w0=75.60000000000015, w1=15.81690866795837\n",
      "SubSGD iter. 468/499: loss=62.923490038230824, w0=74.90000000000015, w1=15.556196446576505\n",
      "SubSGD iter. 469/499: loss=13.467352966064022, w0=74.20000000000014, w1=16.126156571112492\n",
      "SubSGD iter. 470/499: loss=0.38251120341433115, w0=74.90000000000015, w1=16.396295021402164\n",
      "SubSGD iter. 471/499: loss=0.0024784956751379397, w0=75.60000000000015, w1=16.666433471691835\n",
      "SubSGD iter. 472/499: loss=68.86475271716108, w0=74.90000000000015, w1=16.937913653674414\n",
      "SubSGD iter. 473/499: loss=3.6103008370417693, w0=75.60000000000015, w1=15.753911453280523\n",
      "SubSGD iter. 474/499: loss=31.614108360913, w0=74.90000000000015, w1=15.935775296855692\n",
      "SubSGD iter. 475/499: loss=15.134651942762133, w0=74.20000000000014, w1=14.85189990016439\n",
      "SubSGD iter. 476/499: loss=24.94711212457302, w0=73.50000000000014, w1=15.292095154871362\n",
      "SubSGD iter. 477/499: loss=4.4213816455395705, w0=74.20000000000014, w1=14.080672266945182\n",
      "SubSGD iter. 478/499: loss=39.795881966663494, w0=74.90000000000015, w1=14.324984990654317\n",
      "SubSGD iter. 479/499: loss=18.158529697988058, w0=74.20000000000014, w1=13.156654190136848\n",
      "SubSGD iter. 480/499: loss=8.011378505934944, w0=73.50000000000014, w1=13.587664874885462\n",
      "SubSGD iter. 481/499: loss=0.2846586569824997, w0=72.80000000000014, w1=14.56446235382738\n",
      "SubSGD iter. 482/499: loss=11.403593375730173, w0=72.10000000000014, w1=14.987701778599895\n",
      "SubSGD iter. 483/499: loss=13.284777676658836, w0=72.80000000000014, w1=15.611189900886073\n",
      "SubSGD iter. 484/499: loss=0.19388672914515773, w0=72.10000000000014, w1=14.849832950267004\n",
      "SubSGD iter. 485/499: loss=9.194295660645468, w0=71.40000000000013, w1=15.339516483663592\n",
      "SubSGD iter. 486/499: loss=0.5043770072669796, w0=72.10000000000014, w1=15.13468296378837\n",
      "SubSGD iter. 487/499: loss=0.6293035791215708, w0=71.40000000000013, w1=14.229507860601306\n",
      "SubSGD iter. 488/499: loss=10.607919663147289, w0=72.10000000000014, w1=14.6796876068076\n",
      "SubSGD iter. 489/499: loss=66.61246011659337, w0=72.80000000000014, w1=15.027654653198013\n",
      "SubSGD iter. 490/499: loss=84.88943193386883, w0=72.10000000000014, w1=15.147546466686345\n",
      "SubSGD iter. 491/499: loss=0.31552018982376473, w0=72.80000000000014, w1=15.675602282196994\n",
      "SubSGD iter. 492/499: loss=8.556387932504698, w0=72.10000000000014, w1=16.30041838200794\n",
      "SubSGD iter. 493/499: loss=7.633623940518852, w0=72.80000000000014, w1=15.529595315939753\n",
      "SubSGD iter. 494/499: loss=35.05239717065858, w0=72.10000000000014, w1=15.961387894223174\n",
      "SubSGD iter. 495/499: loss=1.8395789541375203, w0=71.40000000000013, w1=16.35682468616618\n",
      "SubSGD iter. 496/499: loss=4.001265794880103, w0=72.10000000000014, w1=17.15199441128353\n",
      "SubSGD iter. 497/499: loss=6913.281085111043, w0=72.80000000000014, w1=14.38169471439454\n",
      "SubSGD iter. 498/499: loss=1.9083506783977329, w0=73.50000000000014, w1=14.357728874591448\n",
      "SubSGD iter. 499/499: loss=6.972310937952388, w0=74.20000000000014, w1=15.897791212978634\n",
      "SubSGD: execution time=0.020 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
