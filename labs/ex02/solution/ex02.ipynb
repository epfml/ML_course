{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    loss = np.zeros((len(w0), len(w1)))\n",
    "    # compute loss for each combinationof w0 and w1.\n",
    "    for ind_row, row in enumerate(w0):\n",
    "        for ind_col, col in enumerate(w1):\n",
    "            w = np.array([row, col])\n",
    "            loss[ind_row, ind_col] = compute_loss(y, tx, w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.014 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAF5CAYAAAAbAcfLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # update w by gradient descent\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2792.2367127591674, w0=51.30574540147369, w1=9.435798704492312\n",
      "Gradient Descent(1/49): loss=265.30246210895854, w0=66.69746902191572, w1=12.266538315840002\n",
      "Gradient Descent(2/49): loss=37.87837955044118, w0=71.31498610804834, w1=13.115760199244331\n",
      "Gradient Descent(3/49): loss=17.41021212017447, w0=72.70024123388814, w1=13.37052676426563\n",
      "Gradient Descent(4/49): loss=15.568077051450457, w0=73.11581777164007, w1=13.446956733772023\n",
      "Gradient Descent(5/49): loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "Gradient Descent(6/49): loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "Gradient Descent(7/49): loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "Gradient Descent(8/49): loss=15.385899822261674, w0=73.29247935783843, w1=13.47944711380919\n",
      "Gradient Descent(9/49): loss=15.385888944638305, w0=73.29348920882516, w1=13.47963283863509\n",
      "Gradient Descent(10/49): loss=15.3858879656522, w0=73.29379216412119, w1=13.479688556082861\n",
      "Gradient Descent(11/49): loss=15.385887877543453, w0=73.29388305071, w1=13.479705271317192\n",
      "Gradient Descent(12/49): loss=15.385887869613667, w0=73.29391031668663, w1=13.479710285887492\n",
      "Gradient Descent(13/49): loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "Gradient Descent(14/49): loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "Gradient Descent(15/49): loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "Gradient Descent(16/49): loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "Gradient Descent(17/49): loss=15.385887868829403, w0=73.29392197370963, w1=13.479712429766732\n",
      "Gradient Descent(18/49): loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "Gradient Descent(19/49): loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "Gradient Descent(20/49): loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "Gradient Descent(21/49): loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "Gradient Descent(22/49): loss=15.3858878688294, w0=73.2939220020362, w1=13.479712434976358\n",
      "Gradient Descent(23/49): loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "Gradient Descent(24/49): loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "Gradient Descent(25/49): loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "Gradient Descent(26/49): loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "Gradient Descent(27/49): loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "Gradient Descent(28/49): loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "Gradient Descent(29/49): loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "Gradient Descent(30/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(31/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(32/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(33/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(34/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(35/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(36/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(37/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(38/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(39/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(40/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(41/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(42/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(43/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(44/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(45/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(46/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(47/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(48/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(49/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent: execution time=0.046 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1509ede4b7f472f82fa96f815ac834a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=2368.5568763553542, w0=6.171073326055082, w1=-0.6929782970183794\n",
      "SGD(1/49): loss=2030.4128077722064, w0=11.824358068629802, w1=-2.3805066566609376\n",
      "SGD(2/49): loss=1882.4592088860725, w0=15.486840018797306, w1=-6.331595311083898\n",
      "SGD(3/49): loss=1422.4554223962184, w0=23.099084903383545, w1=-3.6847100046847476\n",
      "SGD(4/49): loss=1256.940132138785, w0=26.834600251886528, w1=-4.538054103890629\n",
      "SGD(5/49): loss=900.3074086777259, w0=33.373894227266824, w1=0.20438102391205248\n",
      "SGD(6/49): loss=603.9485982262493, w0=39.26561406154602, w1=9.097968556426796\n",
      "SGD(7/49): loss=480.3306019062167, w0=42.81404844972528, w1=12.548725697325075\n",
      "SGD(8/49): loss=399.4285921619436, w0=45.58662205929873, w1=14.104963157168335\n",
      "SGD(9/49): loss=322.819590011557, w0=48.707390631092835, w1=16.69994215159009\n",
      "SGD(10/49): loss=293.6570417415089, w0=50.20832698895222, w1=18.337449627761586\n",
      "SGD(11/49): loss=250.41985215224364, w0=52.556059168074086, w1=19.804977144369964\n",
      "SGD(12/49): loss=210.63124749063272, w0=55.0013882387136, w1=20.954598906286463\n",
      "SGD(13/49): loss=204.886151825637, w0=55.76945116397721, w1=21.958712951395263\n",
      "SGD(14/49): loss=180.0170272448478, w0=57.05854605120085, w1=21.58371431583986\n",
      "SGD(15/49): loss=177.81684836570898, w0=58.08427041914861, w1=23.150721674919758\n",
      "SGD(16/49): loss=122.89021073745684, w0=61.104640844816856, w1=21.630176876100997\n",
      "SGD(17/49): loss=120.56050744096872, w0=61.494151775699336, w1=21.912663386364514\n",
      "SGD(18/49): loss=83.22398371711604, w0=63.57276741942811, w1=19.89651422998549\n",
      "SGD(19/49): loss=68.79134134876188, w0=64.58026025099463, w1=19.0369604811984\n",
      "SGD(20/49): loss=55.282059329401065, w0=65.77351998565986, w1=18.30007512177811\n",
      "SGD(21/49): loss=55.312042080433876, w0=65.76188048794502, w1=18.288107078960845\n",
      "SGD(22/49): loss=34.678056918953125, w0=67.40167634090294, w1=15.445870914941942\n",
      "SGD(23/49): loss=31.210303501960034, w0=67.96326585747134, w1=15.277749234929098\n",
      "SGD(24/49): loss=34.933137157700344, w0=67.23491139982646, w1=15.023373375485956\n",
      "SGD(25/49): loss=28.933783683865926, w0=68.10714090214123, w1=13.919136434253595\n",
      "SGD(26/49): loss=28.701160505986337, w0=68.14913179056967, w1=13.881805650055108\n",
      "SGD(27/49): loss=26.682110141047783, w0=68.54324747077646, w1=13.633127042576964\n",
      "SGD(28/49): loss=26.535299723284353, w0=68.57691960032736, w1=13.700420501293996\n",
      "SGD(29/49): loss=25.630455858767686, w0=68.7694497194768, w1=13.614940182156522\n",
      "SGD(30/49): loss=22.866844652188178, w0=69.48421188933594, w1=14.14905720667346\n",
      "SGD(31/49): loss=23.876457863221514, w0=69.25468344297343, w1=14.295611847421373\n",
      "SGD(32/49): loss=23.284927654906554, w0=69.81202398164683, w1=15.396602037256862\n",
      "SGD(33/49): loss=22.96988206928668, w0=70.11721105835758, w1=15.732820506251661\n",
      "SGD(34/49): loss=23.06170351322867, w0=70.01719756349549, w1=15.627899634051657\n",
      "SGD(35/49): loss=23.032552772776835, w0=70.30078255852648, w1=15.99654498388621\n",
      "SGD(36/49): loss=23.497762205599457, w0=70.08977089299187, w1=15.920442723961584\n",
      "SGD(37/49): loss=24.834929316337583, w0=69.80325989402226, w1=16.07072786184594\n",
      "SGD(38/49): loss=23.726990127622376, w0=70.2404520591621, w1=16.192372720599657\n",
      "SGD(39/49): loss=23.835482826325027, w0=70.09107605805504, w1=16.05671995800343\n",
      "SGD(40/49): loss=22.410370394042133, w0=70.55843384198361, w1=16.04214674170993\n",
      "SGD(41/49): loss=22.433739708604662, w0=70.64617447040392, w1=16.1415044418747\n",
      "SGD(42/49): loss=20.43659981170082, w0=71.35762754634953, w1=16.000067105735358\n",
      "SGD(43/49): loss=22.400506384989182, w0=72.19421984582809, w1=17.060200139171126\n",
      "SGD(44/49): loss=21.735986192846223, w0=72.79137373711193, w1=17.00783401585696\n",
      "SGD(45/49): loss=20.582426352556485, w0=73.70032620611386, w1=16.67782321956457\n",
      "SGD(46/49): loss=19.903610065993565, w0=74.96383282278278, w1=15.979080804260022\n",
      "SGD(47/49): loss=20.094883596174128, w0=74.81886203022147, w1=16.14289650022716\n",
      "SGD(48/49): loss=20.7315258293856, w0=73.47902838678598, w1=16.74422029184325\n",
      "SGD(49/49): loss=18.52046665544633, w0=74.0491570252753, w1=15.866923701930267\n",
      "SGD: execution time=0.089 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05184b4a9062451ca990c630be9d8660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_subgradient(y, tx, w):\n",
    "    \"\"\"Compute the subgradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -np.dot(tx.T,np.sign(err)) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"SubGradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_subgradient(y, tx, w)\n",
    "        loss = calculate_mae(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Sub Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub Gradient Descent(0/499): loss=73.29392200210518, w0=0.7, w1=-2.546585164964199e-16\n",
      "Sub Gradient Descent(1/499): loss=72.59392200210517, w0=1.4, w1=-5.093170329928398e-16\n",
      "Sub Gradient Descent(2/499): loss=71.89392200210517, w0=2.0999999999999996, w1=-7.639755494892597e-16\n",
      "Sub Gradient Descent(3/499): loss=71.19392200210518, w0=2.8, w1=-1.0186340659856795e-15\n",
      "Sub Gradient Descent(4/499): loss=70.49392200210517, w0=3.5, w1=-1.2732925824820993e-15\n",
      "Sub Gradient Descent(5/499): loss=69.79392200210518, w0=4.2, w1=-1.5279510989785191e-15\n",
      "Sub Gradient Descent(6/499): loss=69.09392200210517, w0=4.9, w1=-1.782609615474939e-15\n",
      "Sub Gradient Descent(7/499): loss=68.39392200210517, w0=5.6000000000000005, w1=-2.0372681319713587e-15\n",
      "Sub Gradient Descent(8/499): loss=67.69392200210518, w0=6.300000000000001, w1=-2.2919266484677785e-15\n",
      "Sub Gradient Descent(9/499): loss=66.99392200210517, w0=7.000000000000001, w1=-2.5465851649641983e-15\n",
      "Sub Gradient Descent(10/499): loss=66.29392200210518, w0=7.700000000000001, w1=-2.801243681460618e-15\n",
      "Sub Gradient Descent(11/499): loss=65.59392200210517, w0=8.4, w1=-3.055902197957038e-15\n",
      "Sub Gradient Descent(12/499): loss=64.89392200210517, w0=9.1, w1=-3.3105607144534576e-15\n",
      "Sub Gradient Descent(13/499): loss=64.19392200210518, w0=9.799999999999999, w1=-3.565219230949878e-15\n",
      "Sub Gradient Descent(14/499): loss=63.49392200210517, w0=10.499999999999998, w1=-3.8198777474462976e-15\n",
      "Sub Gradient Descent(15/499): loss=62.79392200210517, w0=11.199999999999998, w1=-4.074536263942717e-15\n",
      "Sub Gradient Descent(16/499): loss=62.093922002105174, w0=11.899999999999997, w1=-4.329194780439137e-15\n",
      "Sub Gradient Descent(17/499): loss=61.393922002105185, w0=12.599999999999996, w1=-4.583853296935557e-15\n",
      "Sub Gradient Descent(18/499): loss=60.69392200210518, w0=13.299999999999995, w1=-4.838511813431977e-15\n",
      "Sub Gradient Descent(19/499): loss=59.99392200210517, w0=13.999999999999995, w1=-5.0931703299283965e-15\n",
      "Sub Gradient Descent(20/499): loss=59.293922002105184, w0=14.699999999999994, w1=-5.347828846424816e-15\n",
      "Sub Gradient Descent(21/499): loss=58.59392200210518, w0=15.399999999999993, w1=-5.602487362921236e-15\n",
      "Sub Gradient Descent(22/499): loss=57.893922002105185, w0=16.099999999999994, w1=-5.857145879417656e-15\n",
      "Sub Gradient Descent(23/499): loss=57.19392200210518, w0=16.799999999999994, w1=-6.111804395914076e-15\n",
      "Sub Gradient Descent(24/499): loss=56.49392200210518, w0=17.499999999999993, w1=-6.3664629124104954e-15\n",
      "Sub Gradient Descent(25/499): loss=55.793922002105184, w0=18.199999999999992, w1=-6.621121428906915e-15\n",
      "Sub Gradient Descent(26/499): loss=55.09392200210518, w0=18.89999999999999, w1=-6.875779945403335e-15\n",
      "Sub Gradient Descent(27/499): loss=54.393922002105185, w0=19.59999999999999, w1=-7.130438461899756e-15\n",
      "Sub Gradient Descent(28/499): loss=53.69392200210518, w0=20.29999999999999, w1=-7.385096978396176e-15\n",
      "Sub Gradient Descent(29/499): loss=52.99392200210518, w0=20.99999999999999, w1=-7.639755494892597e-15\n",
      "Sub Gradient Descent(30/499): loss=52.29392200210519, w0=21.69999999999999, w1=-7.894414011389017e-15\n",
      "Sub Gradient Descent(31/499): loss=51.59392200210518, w0=22.399999999999988, w1=-8.149072527885438e-15\n",
      "Sub Gradient Descent(32/499): loss=50.893922002105185, w0=23.099999999999987, w1=-8.403731044381858e-15\n",
      "Sub Gradient Descent(33/499): loss=50.19392200210518, w0=23.799999999999986, w1=-8.658389560878279e-15\n",
      "Sub Gradient Descent(34/499): loss=49.49392200210519, w0=24.499999999999986, w1=-8.9130480773747e-15\n",
      "Sub Gradient Descent(35/499): loss=48.79392200210519, w0=25.199999999999985, w1=-9.16770659387112e-15\n",
      "Sub Gradient Descent(36/499): loss=48.09392200210519, w0=25.899999999999984, w1=-9.42236511036754e-15\n",
      "Sub Gradient Descent(37/499): loss=47.393922002105185, w0=26.599999999999984, w1=-9.677023626863961e-15\n",
      "Sub Gradient Descent(38/499): loss=46.6939220021052, w0=27.299999999999983, w1=-9.931682143360382e-15\n",
      "Sub Gradient Descent(39/499): loss=45.993922002105194, w0=27.999999999999982, w1=-1.0186340659856802e-14\n",
      "Sub Gradient Descent(40/499): loss=45.29392200210519, w0=28.69999999999998, w1=-1.0440999176353223e-14\n",
      "Sub Gradient Descent(41/499): loss=44.59392200210519, w0=29.39999999999998, w1=-1.0695657692849644e-14\n",
      "Sub Gradient Descent(42/499): loss=43.89392723059967, w0=30.099859999999982, w1=0.00044046577029800113\n",
      "Sub Gradient Descent(43/499): loss=43.194206925442394, w0=30.799719999999983, w1=0.0008809315406066979\n",
      "Sub Gradient Descent(44/499): loss=42.494486620285116, w0=31.499579999999984, w1=0.0013213973109153948\n",
      "Sub Gradient Descent(45/499): loss=41.794801882440076, w0=32.19929999999999, w1=0.002151200058863469\n",
      "Sub Gradient Descent(46/499): loss=41.09536078676493, w0=32.899019999999986, w1=0.0029810028068115433\n",
      "Sub Gradient Descent(47/499): loss=40.39601512176232, w0=33.59859999999998, w1=0.004238399708434908\n",
      "Sub Gradient Descent(48/499): loss=39.696964631836686, w0=34.298039999999986, w1=0.0058238224088555945\n",
      "Sub Gradient Descent(49/499): loss=38.99808059302934, w0=34.99747999999999, w1=0.007409245109276281\n",
      "Sub Gradient Descent(50/499): loss=38.299196554222, w0=35.69691999999999, w1=0.008994667809696967\n",
      "Sub Gradient Descent(51/499): loss=37.600470352601796, w0=36.39607999999999, w1=0.011248049853556336\n",
      "Sub Gradient Descent(52/499): loss=36.90236166793369, w0=37.09495999999999, w1=0.014269124444766603\n",
      "Sub Gradient Descent(53/499): loss=36.20468598163629, w0=37.793699999999994, w1=0.017663499462410342\n",
      "Sub Gradient Descent(54/499): loss=35.5072889013021, w0=38.49215999999999, w1=0.02158797299122859\n",
      "Sub Gradient Descent(55/499): loss=34.810472725597016, w0=39.19019999999999, w1=0.026549981650587084\n",
      "Sub Gradient Descent(56/499): loss=34.11463938511051, w0=39.88739999999999, w1=0.0334590206309787\n",
      "Sub Gradient Descent(57/499): loss=33.42034933030703, w0=40.584039999999995, w1=0.0415535299136616\n",
      "Sub Gradient Descent(58/499): loss=32.727082605905416, w0=41.28025999999999, w1=0.05080738787878193\n",
      "Sub Gradient Descent(59/499): loss=32.035274191106154, w0=41.97507999999999, w1=0.06316953459272466\n",
      "Sub Gradient Descent(60/499): loss=31.346253501120465, w0=42.667939999999994, w1=0.0797066345319525\n",
      "Sub Gradient Descent(61/499): loss=30.660811185738613, w0=43.35925999999999, w1=0.0992900379696581\n",
      "Sub Gradient Descent(62/499): loss=29.97824373392408, w0=44.04889999999999, w1=0.12249682042565051\n",
      "Sub Gradient Descent(63/499): loss=29.29919217002557, w0=44.735739999999986, w1=0.15117673515153837\n",
      "Sub Gradient Descent(64/499): loss=28.625370150150538, w0=45.41991999999998, w1=0.18484712944088777\n",
      "Sub Gradient Descent(65/499): loss=27.956408127884874, w0=46.10073999999998, w1=0.22490170464890552\n",
      "Sub Gradient Descent(66/499): loss=27.293211335702274, w0=46.77847999999998, w1=0.2703080759890902\n",
      "Sub Gradient Descent(67/499): loss=26.63594677549217, w0=47.45243999999998, w1=0.3220564131365356\n",
      "Sub Gradient Descent(68/499): loss=25.986354455661566, w0=48.119959999999985, w1=0.38436028435173947\n",
      "Sub Gradient Descent(69/499): loss=25.346359900549608, w0=48.78257999999998, w1=0.4546014318707423\n",
      "Sub Gradient Descent(70/499): loss=24.713407766391814, w0=49.44141999999998, w1=0.5311991534379574\n",
      "Sub Gradient Descent(71/499): loss=24.086931144489537, w0=50.094659999999976, w1=0.6167864550872693\n",
      "Sub Gradient Descent(72/499): loss=23.469061835298334, w0=50.74159999999998, w1=0.7118294204257967\n",
      "Sub Gradient Descent(73/499): loss=22.860312219449877, w0=51.382239999999975, w1=0.8163783816189527\n",
      "Sub Gradient Descent(74/499): loss=22.260394194603595, w0=52.015879999999974, w1=0.930981947918109\n",
      "Sub Gradient Descent(75/499): loss=21.670214357989344, w0=52.64223999999997, w1=1.0560410394456612\n",
      "Sub Gradient Descent(76/499): loss=21.089713092061285, w0=53.261179999999975, w1=1.1909782249708385\n",
      "Sub Gradient Descent(77/499): loss=20.51839520029385, w0=53.87353999999998, w1=1.3354326069269742\n",
      "Sub Gradient Descent(78/499): loss=19.95484201902195, w0=54.479039999999976, w1=1.4888591554232045\n",
      "Sub Gradient Descent(79/499): loss=19.399817277965212, w0=55.07641999999998, w1=1.6526057998942387\n",
      "Sub Gradient Descent(80/499): loss=18.85429212730872, w0=55.664699999999975, w1=1.8271999117386422\n",
      "Sub Gradient Descent(81/499): loss=18.318783947893138, w0=56.24345999999998, w1=2.0132129159763203\n",
      "Sub Gradient Descent(82/499): loss=17.792491479525136, w0=56.81479999999998, w1=2.207598649786113\n",
      "Sub Gradient Descent(83/499): loss=17.27474434540424, w0=57.37633999999998, w1=2.411376270974571\n",
      "Sub Gradient Descent(84/499): loss=16.766906752678803, w0=57.92975999999998, w1=2.623048101370584\n",
      "Sub Gradient Descent(85/499): loss=16.267994479749266, w0=58.47407999999998, w1=2.843326779065217\n",
      "Sub Gradient Descent(86/499): loss=15.777334186667346, w0=59.00957999999998, w1=3.0723309655204627\n",
      "Sub Gradient Descent(87/499): loss=15.294906834780756, w0=59.53569999999998, w1=3.31029249454101\n",
      "Sub Gradient Descent(88/499): loss=14.820404573025048, w0=60.053559999999976, w1=3.555351748353934\n",
      "Sub Gradient Descent(89/499): loss=14.354283714893906, w0=60.559939999999976, w1=3.8091706918861723\n",
      "Sub Gradient Descent(90/499): loss=13.898001392894878, w0=61.057499999999976, w1=4.069720751434093\n",
      "Sub Gradient Descent(91/499): loss=13.448823828103738, w0=61.547919999999976, w1=4.335508393727016\n",
      "Sub Gradient Descent(92/499): loss=13.006044196344059, w0=62.02965999999998, w1=4.6069897470011485\n",
      "Sub Gradient Descent(93/499): loss=12.571101749360636, w0=62.502719999999975, w1=4.885020142516931\n",
      "Sub Gradient Descent(94/499): loss=12.143380485959344, w0=62.96583999999997, w1=5.168754626653991\n",
      "Sub Gradient Descent(95/499): loss=11.723894837245536, w0=63.41971999999997, w1=5.456484559032156\n",
      "Sub Gradient Descent(96/499): loss=11.313435023601507, w0=63.86463999999997, w1=5.747997496770221\n",
      "Sub Gradient Descent(97/499): loss=10.911156920015054, w0=64.30129999999997, w1=6.043221223968491\n",
      "Sub Gradient Descent(98/499): loss=10.516948122608037, w0=64.72773999999997, w1=6.34035408083703\n",
      "Sub Gradient Descent(99/499): loss=10.133595131038625, w0=65.14381999999996, w1=6.639945245906446\n",
      "Sub Gradient Descent(100/499): loss=9.760677429923442, w0=65.54995999999996, w1=6.939754306257099\n",
      "Sub Gradient Descent(101/499): loss=9.39867462318989, w0=65.94755999999995, w1=7.240382293155465\n",
      "Sub Gradient Descent(102/499): loss=9.04655247713361, w0=66.33451999999996, w1=7.542759507922977\n",
      "Sub Gradient Descent(103/499): loss=8.704760514732369, w0=66.71139999999995, w1=7.846017516367682\n",
      "Sub Gradient Descent(104/499): loss=8.372871671628223, w0=67.07791999999995, w1=8.14916807677345\n",
      "Sub Gradient Descent(105/499): loss=8.05271387226255, w0=67.43603999999995, w1=8.450471378640499\n",
      "Sub Gradient Descent(106/499): loss=7.741845358498772, w0=67.78645999999995, w1=8.751045452661195\n",
      "Sub Gradient Descent(107/499): loss=7.440636497847905, w0=68.12511999999995, w1=9.04921572225298\n",
      "Sub Gradient Descent(108/499): loss=7.152144892432337, w0=68.45537999999995, w1=9.343976087699735\n",
      "Sub Gradient Descent(109/499): loss=6.874953545674662, w0=68.77695999999995, w1=9.6350923726731\n",
      "Sub Gradient Descent(110/499): loss=6.609759691657214, w0=69.08803999999995, w1=9.91949328207346\n",
      "Sub Gradient Descent(111/499): loss=6.359664465347366, w0=69.38749999999995, w1=10.197623582279098\n",
      "Sub Gradient Descent(112/499): loss=6.125559830337047, w0=69.67281999999994, w1=10.468063593613484\n",
      "Sub Gradient Descent(113/499): loss=5.908756851126069, w0=69.94651999999995, w1=10.728826346302098\n",
      "Sub Gradient Descent(114/499): loss=5.708346277966635, w0=70.20887999999995, w1=10.979299692022371\n",
      "Sub Gradient Descent(115/499): loss=5.524903595719315, w0=70.45653999999995, w1=11.220392679253465\n",
      "Sub Gradient Descent(116/499): loss=5.3585721104767465, w0=70.69215999999994, w1=11.449066352016812\n",
      "Sub Gradient Descent(117/499): loss=5.2083665787332984, w0=70.91545999999994, w1=11.664512641643498\n",
      "Sub Gradient Descent(118/499): loss=5.0754520509180105, w0=71.12447999999993, w1=11.864387168309028\n",
      "Sub Gradient Descent(119/499): loss=4.96083991488238, w0=71.31865999999994, w1=12.045105403559567\n",
      "Sub Gradient Descent(120/499): loss=4.864240181709663, w0=71.49939999999994, w1=12.209836073166061\n",
      "Sub Gradient Descent(121/499): loss=4.782724768945244, w0=71.66543999999993, w1=12.357476521898802\n",
      "Sub Gradient Descent(122/499): loss=4.715208074668119, w0=71.81593999999993, w1=12.48980888194037\n",
      "Sub Gradient Descent(123/499): loss=4.660146063774239, w0=71.95369999999993, w1=12.612418157319379\n",
      "Sub Gradient Descent(124/499): loss=4.613848453064209, w0=72.08067999999993, w1=12.721366944472228\n",
      "Sub Gradient Descent(125/499): loss=4.576038370591753, w0=72.19435999999993, w1=12.815458765817693\n",
      "Sub Gradient Descent(126/499): loss=4.546649851049032, w0=72.29697999999993, w1=12.90059276939542\n",
      "Sub Gradient Descent(127/499): loss=4.522563544988966, w0=72.38965999999994, w1=12.97576965428\n",
      "Sub Gradient Descent(128/499): loss=4.503077398110052, w0=72.47435999999993, w1=13.04396334930235\n",
      "Sub Gradient Descent(129/499): loss=4.487012427842279, w0=72.55093999999994, w1=13.103277584601768\n",
      "Sub Gradient Descent(130/499): loss=4.474309393425113, w0=72.62051999999994, w1=13.156032035188224\n",
      "Sub Gradient Descent(131/499): loss=4.4641666691566, w0=72.68239999999994, w1=13.20187056696924\n",
      "Sub Gradient Descent(132/499): loss=4.4562021249361425, w0=72.73769999999995, w1=13.241365939188775\n",
      "Sub Gradient Descent(133/499): loss=4.449913837301204, w0=72.78823999999994, w1=13.276636749385828\n",
      "Sub Gradient Descent(134/499): loss=4.444739448232765, w0=72.83401999999994, w1=13.308112586749766\n",
      "Sub Gradient Descent(135/499): loss=4.4405770631501325, w0=72.87447999999993, w1=13.335151520405086\n",
      "Sub Gradient Descent(136/499): loss=4.437396761058307, w0=72.91115999999994, w1=13.358430829065538\n",
      "Sub Gradient Descent(137/499): loss=4.434846528707498, w0=72.94447999999994, w1=13.379256780851343\n",
      "Sub Gradient Descent(138/499): loss=4.43281155143695, w0=72.97401999999994, w1=13.396406376905624\n",
      "Sub Gradient Descent(139/499): loss=4.431233661858319, w0=73.00103999999993, w1=13.41218912045945\n",
      "Sub Gradient Descent(140/499): loss=4.429913776135237, w0=73.02483999999993, w1=13.425274895313164\n",
      "Sub Gradient Descent(141/499): loss=4.4289160846764135, w0=73.04625999999993, w1=13.436222021322383\n",
      "Sub Gradient Descent(142/499): loss=4.428112214589719, w0=73.06613999999993, w1=13.447440046601333\n",
      "Sub Gradient Descent(143/499): loss=4.427409042227932, w0=73.08405999999994, w1=13.457856669597973\n",
      "Sub Gradient Descent(144/499): loss=4.426825784996948, w0=73.10029999999993, w1=13.467335162734953\n",
      "Sub Gradient Descent(145/499): loss=4.426369603365168, w0=73.11401999999994, w1=13.475393628220303\n",
      "Sub Gradient Descent(146/499): loss=4.426030178383451, w0=73.12619999999994, w1=13.482212864437972\n",
      "Sub Gradient Descent(147/499): loss=4.425766799370374, w0=73.13697999999994, w1=13.48799725691154\n",
      "Sub Gradient Descent(148/499): loss=4.425555929007912, w0=73.14747999999993, w1=13.493479439140332\n",
      "Sub Gradient Descent(149/499): loss=4.425362490640804, w0=73.15699999999993, w1=13.498468271254444\n",
      "Sub Gradient Descent(150/499): loss=4.425203617363584, w0=73.16595999999993, w1=13.50294680429721\n",
      "Sub Gradient Descent(151/499): loss=4.425068527947261, w0=73.17365999999993, w1=13.506576029236331\n",
      "Sub Gradient Descent(152/499): loss=4.424972782434855, w0=73.18009999999992, w1=13.509768665455187\n",
      "Sub Gradient Descent(153/499): loss=4.424903121175711, w0=73.18597999999993, w1=13.512571468366664\n",
      "Sub Gradient Descent(154/499): loss=4.424846327975846, w0=73.19143999999993, w1=13.515117410468935\n",
      "Sub Gradient Descent(155/499): loss=4.42479923253376, w0=73.19605999999993, w1=13.517185961787765\n",
      "Sub Gradient Descent(156/499): loss=4.4247664094040005, w0=73.19969999999994, w1=13.518069930874363\n",
      "Sub Gradient Descent(157/499): loss=4.424747663519319, w0=73.20291999999993, w1=13.518854137767093\n",
      "Sub Gradient Descent(158/499): loss=4.424732182668888, w0=73.20599999999993, w1=13.51952983926895\n",
      "Sub Gradient Descent(159/499): loss=4.42471797842243, w0=73.20907999999993, w1=13.520205540770807\n",
      "Sub Gradient Descent(160/499): loss=4.424704053977274, w0=73.21201999999992, w1=13.521000216270595\n",
      "Sub Gradient Descent(161/499): loss=4.424690803821345, w0=73.21495999999992, w1=13.521794891770384\n",
      "Sub Gradient Descent(162/499): loss=4.424678207991666, w0=73.21747999999992, w1=13.522407209547023\n",
      "Sub Gradient Descent(163/499): loss=4.424668911605651, w0=73.21985999999993, w1=13.522732908460966\n",
      "Sub Gradient Descent(164/499): loss=4.424660947850395, w0=73.22181999999992, w1=13.522991164028817\n",
      "Sub Gradient Descent(165/499): loss=4.424655537307837, w0=73.22363999999992, w1=13.523086524955888\n",
      "Sub Gradient Descent(166/499): loss=4.424651183353665, w0=73.22503999999992, w1=13.522757010157475\n",
      "Sub Gradient Descent(167/499): loss=4.424648228239376, w0=73.22643999999993, w1=13.522427495359063\n",
      "Sub Gradient Descent(168/499): loss=4.424645273125086, w0=73.22783999999993, w1=13.52209798056065\n",
      "Sub Gradient Descent(169/499): loss=4.424642498283796, w0=73.22895999999993, w1=13.522225963141322\n",
      "Sub Gradient Descent(170/499): loss=4.424640682884452, w0=73.23007999999993, w1=13.522353945721994\n",
      "Sub Gradient Descent(171/499): loss=4.424638867485107, w0=73.23119999999993, w1=13.522481928302666\n",
      "Sub Gradient Descent(172/499): loss=4.4246373008521855, w0=73.23189999999992, w1=13.5230134957471\n",
      "Sub Gradient Descent(173/499): loss=4.424636258655273, w0=73.23273999999992, w1=13.52326627956587\n",
      "Sub Gradient Descent(174/499): loss=4.424635339412461, w0=73.23329999999991, w1=13.523741235074846\n",
      "Sub Gradient Descent(175/499): loss=4.424634640448435, w0=73.23399999999991, w1=13.523937406958156\n",
      "Sub Gradient Descent(176/499): loss=4.424633962890472, w0=73.2345599999999, w1=13.523888604707771\n",
      "Sub Gradient Descent(177/499): loss=4.424633633499305, w0=73.23497999999991, w1=13.524118586083052\n",
      "Sub Gradient Descent(178/499): loss=4.424633305940114, w0=73.23539999999991, w1=13.524348567458333\n",
      "Sub Gradient Descent(179/499): loss=4.424632978380925, w0=73.23581999999992, w1=13.524578548833613\n",
      "Sub Gradient Descent(180/499): loss=4.424632650821734, w0=73.23623999999992, w1=13.524808530208894\n",
      "Sub Gradient Descent(181/499): loss=4.424632389924093, w0=73.23651999999993, w1=13.524964267899344\n",
      "Sub Gradient Descent(182/499): loss=4.4246322432751946, w0=73.23679999999993, w1=13.525120005589795\n",
      "Sub Gradient Descent(183/499): loss=4.424632096626299, w0=73.23707999999993, w1=13.525275743280245\n",
      "Sub Gradient Descent(184/499): loss=4.424631993328702, w0=73.23721999999994, w1=13.525174424196276\n",
      "Sub Gradient Descent(185/499): loss=4.42463195066362, w0=73.23735999999994, w1=13.525073105112307\n",
      "Sub Gradient Descent(186/499): loss=4.424631907998538, w0=73.23749999999994, w1=13.524971786028338\n",
      "Sub Gradient Descent(187/499): loss=4.424631865333457, w0=73.23763999999994, w1=13.52487046694437\n",
      "Sub Gradient Descent(188/499): loss=4.424631822668376, w0=73.23777999999994, w1=13.5247691478604\n",
      "Sub Gradient Descent(189/499): loss=4.424631782685974, w0=73.23805999999995, w1=13.52492488555085\n",
      "Sub Gradient Descent(190/499): loss=4.424631746545009, w0=73.23819999999995, w1=13.524823566466882\n",
      "Sub Gradient Descent(191/499): loss=4.424631703879928, w0=73.23833999999995, w1=13.524722247382913\n",
      "Sub Gradient Descent(192/499): loss=4.424631661214847, w0=73.23847999999995, w1=13.524620928298944\n",
      "Sub Gradient Descent(193/499): loss=4.424631622214842, w0=73.23847999999995, w1=13.524586781150388\n",
      "Sub Gradient Descent(194/499): loss=4.424631620549089, w0=73.23847999999995, w1=13.524552634001832\n",
      "Sub Gradient Descent(195/499): loss=4.424631618883335, w0=73.23847999999995, w1=13.524518486853276\n",
      "Sub Gradient Descent(196/499): loss=4.424631617217582, w0=73.23847999999995, w1=13.52448433970472\n",
      "Sub Gradient Descent(197/499): loss=4.424631615551827, w0=73.23847999999995, w1=13.524450192556165\n",
      "Sub Gradient Descent(198/499): loss=4.424631613886074, w0=73.23847999999995, w1=13.524416045407609\n",
      "Sub Gradient Descent(199/499): loss=4.4246316122203195, w0=73.23847999999995, w1=13.524381898259053\n",
      "Sub Gradient Descent(200/499): loss=4.424631615444577, w0=73.23861999999995, w1=13.524604807884916\n",
      "Sub Gradient Descent(201/499): loss=4.424631621428463, w0=73.23861999999995, w1=13.52457066073636\n",
      "Sub Gradient Descent(202/499): loss=4.424631619762709, w0=73.23861999999995, w1=13.524536513587805\n",
      "Sub Gradient Descent(203/499): loss=4.424631618096956, w0=73.23861999999995, w1=13.524502366439249\n",
      "Sub Gradient Descent(204/499): loss=4.4246316164312, w0=73.23861999999995, w1=13.524468219290693\n",
      "Sub Gradient Descent(205/499): loss=4.424631614765446, w0=73.23861999999995, w1=13.524434072142137\n",
      "Sub Gradient Descent(206/499): loss=4.4246316130996926, w0=73.23861999999995, w1=13.524399924993581\n",
      "Sub Gradient Descent(207/499): loss=4.42463161143394, w0=73.23861999999995, w1=13.524365777845025\n",
      "Sub Gradient Descent(208/499): loss=4.424631609768185, w0=73.23861999999995, w1=13.52433163069647\n",
      "Sub Gradient Descent(209/499): loss=4.424631608102431, w0=73.23861999999995, w1=13.524297483547914\n",
      "Sub Gradient Descent(210/499): loss=4.4246316143257935, w0=73.23875999999996, w1=13.524520393173777\n",
      "Sub Gradient Descent(211/499): loss=4.424631617310575, w0=73.23875999999996, w1=13.524486246025221\n",
      "Sub Gradient Descent(212/499): loss=4.424631615644821, w0=73.23875999999996, w1=13.524452098876665\n",
      "Sub Gradient Descent(213/499): loss=4.4246316139790665, w0=73.23875999999996, w1=13.52441795172811\n",
      "Sub Gradient Descent(214/499): loss=4.424631612313314, w0=73.23875999999996, w1=13.524383804579553\n",
      "Sub Gradient Descent(215/499): loss=4.424631610647559, w0=73.23875999999996, w1=13.524349657430998\n",
      "Sub Gradient Descent(216/499): loss=4.424631608981804, w0=73.23875999999996, w1=13.524315510282442\n",
      "Sub Gradient Descent(217/499): loss=4.4246316073160505, w0=73.23875999999996, w1=13.524281363133886\n",
      "Sub Gradient Descent(218/499): loss=4.424631605650297, w0=73.23875999999996, w1=13.52424721598533\n",
      "Sub Gradient Descent(219/499): loss=4.424631603984544, w0=73.23875999999996, w1=13.524213068836774\n",
      "Sub Gradient Descent(220/499): loss=4.42463161320701, w0=73.23889999999996, w1=13.524435978462638\n",
      "Sub Gradient Descent(221/499): loss=4.424631613192687, w0=73.23889999999996, w1=13.524401831314082\n",
      "Sub Gradient Descent(222/499): loss=4.424631611526932, w0=73.23889999999996, w1=13.524367684165526\n",
      "Sub Gradient Descent(223/499): loss=4.424631609861179, w0=73.23889999999996, w1=13.52433353701697\n",
      "Sub Gradient Descent(224/499): loss=4.424631608195425, w0=73.23889999999996, w1=13.524299389868414\n",
      "Sub Gradient Descent(225/499): loss=4.424631606529671, w0=73.23889999999996, w1=13.524265242719858\n",
      "Sub Gradient Descent(226/499): loss=4.424631604863916, w0=73.23889999999996, w1=13.524231095571302\n",
      "Sub Gradient Descent(227/499): loss=4.424631603198163, w0=73.23889999999996, w1=13.524196948422746\n",
      "Sub Gradient Descent(228/499): loss=4.4246316015324085, w0=73.23889999999996, w1=13.52416280127419\n",
      "Sub Gradient Descent(229/499): loss=4.4246316012143305, w0=73.23903999999996, w1=13.524385710900054\n",
      "Sub Gradient Descent(230/499): loss=4.424631610740553, w0=73.23903999999996, w1=13.524351563751498\n",
      "Sub Gradient Descent(231/499): loss=4.424631609074798, w0=73.23903999999996, w1=13.524317416602942\n",
      "Sub Gradient Descent(232/499): loss=4.424631607409045, w0=73.23903999999996, w1=13.524283269454386\n",
      "Sub Gradient Descent(233/499): loss=4.424631605743291, w0=73.23903999999996, w1=13.52424912230583\n",
      "Sub Gradient Descent(234/499): loss=4.424631604077537, w0=73.23903999999996, w1=13.524214975157275\n",
      "Sub Gradient Descent(235/499): loss=4.424631602411783, w0=73.23903999999996, w1=13.524180828008719\n",
      "Sub Gradient Descent(236/499): loss=4.42463160074603, w0=73.23903999999996, w1=13.524146680860163\n",
      "Sub Gradient Descent(237/499): loss=4.424631599080274, w0=73.23903999999996, w1=13.524112533711607\n",
      "Sub Gradient Descent(238/499): loss=4.424631597414521, w0=73.23903999999996, w1=13.524078386563051\n",
      "Sub Gradient Descent(239/499): loss=4.424631600095546, w0=73.23917999999996, w1=13.524301296188915\n",
      "Sub Gradient Descent(240/499): loss=4.424631606622665, w0=73.23917999999996, w1=13.524267149040359\n",
      "Sub Gradient Descent(241/499): loss=4.42463160495691, w0=73.23917999999996, w1=13.524233001891803\n",
      "Sub Gradient Descent(242/499): loss=4.424631603291156, w0=73.23917999999996, w1=13.524198854743247\n",
      "Sub Gradient Descent(243/499): loss=4.424631601625403, w0=73.23917999999996, w1=13.524164707594691\n",
      "Sub Gradient Descent(244/499): loss=4.424631599959648, w0=73.23917999999996, w1=13.524130560446135\n",
      "Sub Gradient Descent(245/499): loss=4.424631598293895, w0=73.23917999999996, w1=13.52409641329758\n",
      "Sub Gradient Descent(246/499): loss=4.42463159662814, w0=73.23917999999996, w1=13.524062266149024\n",
      "Sub Gradient Descent(247/499): loss=4.424631594962386, w0=73.23917999999996, w1=13.524028119000468\n",
      "Sub Gradient Descent(248/499): loss=4.424631593296633, w0=73.23917999999996, w1=13.523993971851912\n",
      "Sub Gradient Descent(249/499): loss=4.424631598976763, w0=73.23931999999996, w1=13.524216881477775\n",
      "Sub Gradient Descent(250/499): loss=4.424631602504776, w0=73.23931999999996, w1=13.52418273432922\n",
      "Sub Gradient Descent(251/499): loss=4.424631600839023, w0=73.23931999999996, w1=13.524148587180663\n",
      "Sub Gradient Descent(252/499): loss=4.424631599173268, w0=73.23931999999996, w1=13.524114440032108\n",
      "Sub Gradient Descent(253/499): loss=4.424631597507514, w0=73.23931999999996, w1=13.524080292883552\n",
      "Sub Gradient Descent(254/499): loss=4.424631595841761, w0=73.23931999999996, w1=13.524046145734996\n",
      "Sub Gradient Descent(255/499): loss=4.424631594176007, w0=73.23931999999996, w1=13.52401199858644\n",
      "Sub Gradient Descent(256/499): loss=4.424631592510252, w0=73.23931999999996, w1=13.523977851437884\n",
      "Sub Gradient Descent(257/499): loss=4.424631590844498, w0=73.23931999999996, w1=13.523943704289328\n",
      "Sub Gradient Descent(258/499): loss=4.424631589178745, w0=73.23931999999996, w1=13.523909557140772\n",
      "Sub Gradient Descent(259/499): loss=4.424631597857981, w0=73.23945999999997, w1=13.524132466766636\n",
      "Sub Gradient Descent(260/499): loss=4.424631598386888, w0=73.23945999999997, w1=13.52409831961808\n",
      "Sub Gradient Descent(261/499): loss=4.4246315967211345, w0=73.23945999999997, w1=13.524064172469524\n",
      "Sub Gradient Descent(262/499): loss=4.42463159505538, w0=73.23945999999997, w1=13.524030025320968\n",
      "Sub Gradient Descent(263/499): loss=4.424631593389626, w0=73.23945999999997, w1=13.523995878172412\n",
      "Sub Gradient Descent(264/499): loss=4.424631591723872, w0=73.23945999999997, w1=13.523961731023856\n",
      "Sub Gradient Descent(265/499): loss=4.424631590058119, w0=73.23945999999997, w1=13.5239275838753\n",
      "Sub Gradient Descent(266/499): loss=4.424631588392364, w0=73.23945999999997, w1=13.523893436726745\n",
      "Sub Gradient Descent(267/499): loss=4.424631586726611, w0=73.23945999999997, w1=13.523859289578189\n",
      "Sub Gradient Descent(268/499): loss=4.424631585865299, w0=73.23959999999997, w1=13.524082199204052\n",
      "Sub Gradient Descent(269/499): loss=4.424631595934754, w0=73.23959999999997, w1=13.524048052055496\n",
      "Sub Gradient Descent(270/499): loss=4.424631594268999, w0=73.23959999999997, w1=13.52401390490694\n",
      "Sub Gradient Descent(271/499): loss=4.424631592603246, w0=73.23959999999997, w1=13.523979757758385\n",
      "Sub Gradient Descent(272/499): loss=4.424631590937492, w0=73.23959999999997, w1=13.523945610609829\n",
      "Sub Gradient Descent(273/499): loss=4.424631589271739, w0=73.23959999999997, w1=13.523911463461273\n",
      "Sub Gradient Descent(274/499): loss=4.424631587605985, w0=73.23959999999997, w1=13.523877316312717\n",
      "Sub Gradient Descent(275/499): loss=4.424631585940229, w0=73.23959999999997, w1=13.523843169164161\n",
      "Sub Gradient Descent(276/499): loss=4.424631584274477, w0=73.23959999999997, w1=13.523809022015605\n",
      "Sub Gradient Descent(277/499): loss=4.424631582608722, w0=73.23959999999997, w1=13.52377487486705\n",
      "Sub Gradient Descent(278/499): loss=4.424631584746516, w0=73.23973999999997, w1=13.523997784492913\n",
      "Sub Gradient Descent(279/499): loss=4.424631591816865, w0=73.23973999999997, w1=13.523963637344357\n",
      "Sub Gradient Descent(280/499): loss=4.424631590151113, w0=73.23973999999997, w1=13.523929490195801\n",
      "Sub Gradient Descent(281/499): loss=4.424631588485357, w0=73.23973999999997, w1=13.523895343047245\n",
      "Sub Gradient Descent(282/499): loss=4.424631586819603, w0=73.23973999999997, w1=13.52386119589869\n",
      "Sub Gradient Descent(283/499): loss=4.4246315851538505, w0=73.23973999999997, w1=13.523827048750134\n",
      "Sub Gradient Descent(284/499): loss=4.424631583488096, w0=73.23973999999997, w1=13.523792901601578\n",
      "Sub Gradient Descent(285/499): loss=4.424631581822342, w0=73.23973999999997, w1=13.523758754453022\n",
      "Sub Gradient Descent(286/499): loss=4.424631580156587, w0=73.23973999999997, w1=13.523724607304466\n",
      "Sub Gradient Descent(287/499): loss=4.4246315784908345, w0=73.23973999999997, w1=13.52369046015591\n",
      "Sub Gradient Descent(288/499): loss=4.4246315836277335, w0=73.23987999999997, w1=13.523913369781773\n",
      "Sub Gradient Descent(289/499): loss=4.424631587698977, w0=73.23987999999997, w1=13.523879222633218\n",
      "Sub Gradient Descent(290/499): loss=4.4246315860332235, w0=73.23987999999997, w1=13.523845075484662\n",
      "Sub Gradient Descent(291/499): loss=4.424631584367471, w0=73.23987999999997, w1=13.523810928336106\n",
      "Sub Gradient Descent(292/499): loss=4.424631582701716, w0=73.23987999999997, w1=13.52377678118755\n",
      "Sub Gradient Descent(293/499): loss=4.424631581035961, w0=73.23987999999997, w1=13.523742634038994\n",
      "Sub Gradient Descent(294/499): loss=4.4246315793702085, w0=73.23987999999997, w1=13.523708486890438\n",
      "Sub Gradient Descent(295/499): loss=4.424631577704453, w0=73.23987999999997, w1=13.523674339741882\n",
      "Sub Gradient Descent(296/499): loss=4.4246315760387, w0=73.23987999999997, w1=13.523640192593327\n",
      "Sub Gradient Descent(297/499): loss=4.424631574372946, w0=73.23987999999997, w1=13.52360604544477\n",
      "Sub Gradient Descent(298/499): loss=4.42463158250895, w0=73.24001999999997, w1=13.523828955070634\n",
      "Sub Gradient Descent(299/499): loss=4.42463158358109, w0=73.24001999999997, w1=13.523794807922078\n",
      "Sub Gradient Descent(300/499): loss=4.424631581915335, w0=73.24001999999997, w1=13.523760660773522\n",
      "Sub Gradient Descent(301/499): loss=4.4246315802495815, w0=73.24001999999997, w1=13.523726513624966\n",
      "Sub Gradient Descent(302/499): loss=4.424631578583828, w0=73.24001999999997, w1=13.52369236647641\n",
      "Sub Gradient Descent(303/499): loss=4.424631576918074, w0=73.24001999999997, w1=13.523658219327855\n",
      "Sub Gradient Descent(304/499): loss=4.424631575252319, w0=73.24001999999997, w1=13.523624072179299\n",
      "Sub Gradient Descent(305/499): loss=4.4246315735865664, w0=73.24001999999997, w1=13.523589925030743\n",
      "Sub Gradient Descent(306/499): loss=4.424631571920812, w0=73.24001999999997, w1=13.523555777882187\n",
      "Sub Gradient Descent(307/499): loss=4.424631570516269, w0=73.24015999999997, w1=13.52377868750805\n",
      "Sub Gradient Descent(308/499): loss=4.4246315811289545, w0=73.24015999999997, w1=13.523744540359495\n",
      "Sub Gradient Descent(309/499): loss=4.424631579463202, w0=73.24015999999997, w1=13.523710393210939\n",
      "Sub Gradient Descent(310/499): loss=4.424631577797448, w0=73.24015999999997, w1=13.523676246062383\n",
      "Sub Gradient Descent(311/499): loss=4.424631576131694, w0=73.24015999999997, w1=13.523642098913827\n",
      "Sub Gradient Descent(312/499): loss=4.4246315744659395, w0=73.24015999999997, w1=13.523607951765271\n",
      "Sub Gradient Descent(313/499): loss=4.424631572800186, w0=73.24015999999997, w1=13.523573804616715\n",
      "Sub Gradient Descent(314/499): loss=4.424631571134432, w0=73.24015999999997, w1=13.52353965746816\n",
      "Sub Gradient Descent(315/499): loss=4.424631569468677, w0=73.24015999999997, w1=13.523505510319604\n",
      "Sub Gradient Descent(316/499): loss=4.424631567802924, w0=73.24015999999997, w1=13.523471363171048\n",
      "Sub Gradient Descent(317/499): loss=4.424631569397486, w0=73.24029999999998, w1=13.523694272796911\n",
      "Sub Gradient Descent(318/499): loss=4.424631577011067, w0=73.24029999999998, w1=13.523660125648355\n",
      "Sub Gradient Descent(319/499): loss=4.4246315753453125, w0=73.24029999999998, w1=13.5236259784998\n",
      "Sub Gradient Descent(320/499): loss=4.4246315736795605, w0=73.24029999999998, w1=13.523591831351244\n",
      "Sub Gradient Descent(321/499): loss=4.424631572013805, w0=73.24029999999998, w1=13.523557684202688\n",
      "Sub Gradient Descent(322/499): loss=4.424631570348052, w0=73.24029999999998, w1=13.523523537054132\n",
      "Sub Gradient Descent(323/499): loss=4.4246315686822975, w0=73.24029999999998, w1=13.523489389905576\n",
      "Sub Gradient Descent(324/499): loss=4.424631567016544, w0=73.24029999999998, w1=13.52345524275702\n",
      "Sub Gradient Descent(325/499): loss=4.424631565350789, w0=73.24029999999998, w1=13.523421095608464\n",
      "Sub Gradient Descent(326/499): loss=4.424631563685035, w0=73.24029999999998, w1=13.523386948459908\n",
      "Sub Gradient Descent(327/499): loss=4.424631568278702, w0=73.24043999999998, w1=13.523609858085772\n",
      "Sub Gradient Descent(328/499): loss=4.42463157289318, w0=73.24043999999998, w1=13.523575710937216\n",
      "Sub Gradient Descent(329/499): loss=4.424631571227426, w0=73.24043999999998, w1=13.52354156378866\n",
      "Sub Gradient Descent(330/499): loss=4.4246315695616705, w0=73.24043999999998, w1=13.523507416640104\n",
      "Sub Gradient Descent(331/499): loss=4.424631567895917, w0=73.24043999999998, w1=13.523473269491548\n",
      "Sub Gradient Descent(332/499): loss=4.424631566230163, w0=73.24043999999998, w1=13.523439122342992\n",
      "Sub Gradient Descent(333/499): loss=4.424631564564409, w0=73.24043999999998, w1=13.523404975194437\n",
      "Sub Gradient Descent(334/499): loss=4.4246315628986554, w0=73.24043999999998, w1=13.52337082804588\n",
      "Sub Gradient Descent(335/499): loss=4.424631561232902, w0=73.24043999999998, w1=13.523336680897325\n",
      "Sub Gradient Descent(336/499): loss=4.424631559567148, w0=73.24043999999998, w1=13.523302533748769\n",
      "Sub Gradient Descent(337/499): loss=4.424631567159919, w0=73.24057999999998, w1=13.523525443374632\n",
      "Sub Gradient Descent(338/499): loss=4.4246315687752915, w0=73.24057999999998, w1=13.523491296226076\n",
      "Sub Gradient Descent(339/499): loss=4.424631567109538, w0=73.24057999999998, w1=13.52345714907752\n",
      "Sub Gradient Descent(340/499): loss=4.424631565443783, w0=73.24057999999998, w1=13.523423001928965\n",
      "Sub Gradient Descent(341/499): loss=4.424631563778029, w0=73.24057999999998, w1=13.523388854780409\n",
      "Sub Gradient Descent(342/499): loss=4.424631562112276, w0=73.24057999999998, w1=13.523354707631853\n",
      "Sub Gradient Descent(343/499): loss=4.424631560446521, w0=73.24057999999998, w1=13.523320560483297\n",
      "Sub Gradient Descent(344/499): loss=4.424631558780766, w0=73.24057999999998, w1=13.523286413334741\n",
      "Sub Gradient Descent(345/499): loss=4.424631557115013, w0=73.24057999999998, w1=13.523252266186185\n",
      "Sub Gradient Descent(346/499): loss=4.424631555449259, w0=73.24057999999998, w1=13.52321811903763\n",
      "Sub Gradient Descent(347/499): loss=4.424631566041136, w0=73.24071999999998, w1=13.523441028663493\n",
      "Sub Gradient Descent(348/499): loss=4.424631564657402, w0=73.24071999999998, w1=13.523406881514937\n",
      "Sub Gradient Descent(349/499): loss=4.4246315629916495, w0=73.24071999999998, w1=13.523372734366381\n",
      "Sub Gradient Descent(350/499): loss=4.424631561325894, w0=73.24071999999998, w1=13.523338587217825\n",
      "Sub Gradient Descent(351/499): loss=4.424631559660142, w0=73.24071999999998, w1=13.52330444006927\n",
      "Sub Gradient Descent(352/499): loss=4.4246315579943865, w0=73.24071999999998, w1=13.523270292920714\n",
      "Sub Gradient Descent(353/499): loss=4.424631556328634, w0=73.24071999999998, w1=13.523236145772158\n",
      "Sub Gradient Descent(354/499): loss=4.424631554662879, w0=73.24071999999998, w1=13.523201998623602\n",
      "Sub Gradient Descent(355/499): loss=4.424631552997125, w0=73.24071999999998, w1=13.523167851475046\n",
      "Sub Gradient Descent(356/499): loss=4.424631554048455, w0=73.24085999999998, w1=13.52339076110091\n",
      "Sub Gradient Descent(357/499): loss=4.424631562205269, w0=73.24085999999998, w1=13.523356613952354\n",
      "Sub Gradient Descent(358/499): loss=4.424631560539515, w0=73.24085999999998, w1=13.523322466803798\n",
      "Sub Gradient Descent(359/499): loss=4.424631558873761, w0=73.24085999999998, w1=13.523288319655242\n",
      "Sub Gradient Descent(360/499): loss=4.4246315572080075, w0=73.24085999999998, w1=13.523254172506686\n",
      "Sub Gradient Descent(361/499): loss=4.424631555542253, w0=73.24085999999998, w1=13.52322002535813\n",
      "Sub Gradient Descent(362/499): loss=4.424631553876499, w0=73.24085999999998, w1=13.523185878209574\n",
      "Sub Gradient Descent(363/499): loss=4.4246315522107444, w0=73.24085999999998, w1=13.523151731061018\n",
      "Sub Gradient Descent(364/499): loss=4.424631550544991, w0=73.24085999999998, w1=13.523117583912462\n",
      "Sub Gradient Descent(365/499): loss=4.424631548879238, w0=73.24085999999998, w1=13.523083436763907\n",
      "Sub Gradient Descent(366/499): loss=4.4246315529296725, w0=73.24099999999999, w1=13.52330634638977\n",
      "Sub Gradient Descent(367/499): loss=4.4246315580873805, w0=73.24099999999999, w1=13.523272199241214\n",
      "Sub Gradient Descent(368/499): loss=4.424631556421627, w0=73.24099999999999, w1=13.523238052092658\n",
      "Sub Gradient Descent(369/499): loss=4.424631554755873, w0=73.24099999999999, w1=13.523203904944102\n",
      "Sub Gradient Descent(370/499): loss=4.424631553090119, w0=73.24099999999999, w1=13.523169757795547\n",
      "Sub Gradient Descent(371/499): loss=4.424631551424365, w0=73.24099999999999, w1=13.52313561064699\n",
      "Sub Gradient Descent(372/499): loss=4.424631549758611, w0=73.24099999999999, w1=13.523101463498435\n",
      "Sub Gradient Descent(373/499): loss=4.424631548092857, w0=73.24099999999999, w1=13.523067316349879\n",
      "Sub Gradient Descent(374/499): loss=4.424631546427102, w0=73.24099999999999, w1=13.523033169201323\n",
      "Sub Gradient Descent(375/499): loss=4.424631544761349, w0=73.24099999999999, w1=13.522999022052767\n",
      "Sub Gradient Descent(376/499): loss=4.424631551810888, w0=73.24113999999999, w1=13.52322193167863\n",
      "Sub Gradient Descent(377/499): loss=4.424631553969492, w0=73.24113999999999, w1=13.523187784530075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub Gradient Descent(378/499): loss=4.4246315523037385, w0=73.24113999999999, w1=13.523153637381519\n",
      "Sub Gradient Descent(379/499): loss=4.424631550637985, w0=73.24113999999999, w1=13.523119490232963\n",
      "Sub Gradient Descent(380/499): loss=4.424631548972231, w0=73.24113999999999, w1=13.523085343084407\n",
      "Sub Gradient Descent(381/499): loss=4.4246315473064755, w0=73.24113999999999, w1=13.523051195935851\n",
      "Sub Gradient Descent(382/499): loss=4.4246315456407235, w0=73.24113999999999, w1=13.523017048787295\n",
      "Sub Gradient Descent(383/499): loss=4.424631543974968, w0=73.24113999999999, w1=13.52298290163874\n",
      "Sub Gradient Descent(384/499): loss=4.424631542309215, w0=73.24113999999999, w1=13.522948754490184\n",
      "Sub Gradient Descent(385/499): loss=4.424631540643461, w0=73.24113999999999, w1=13.522914607341628\n",
      "Sub Gradient Descent(386/499): loss=4.424631550692105, w0=73.24127999999999, w1=13.523137516967491\n",
      "Sub Gradient Descent(387/499): loss=4.424631549851604, w0=73.24127999999999, w1=13.523103369818935\n",
      "Sub Gradient Descent(388/499): loss=4.424631548185849, w0=73.24127999999999, w1=13.52306922267038\n",
      "Sub Gradient Descent(389/499): loss=4.4246315465200965, w0=73.24127999999999, w1=13.523035075521824\n",
      "Sub Gradient Descent(390/499): loss=4.424631544854343, w0=73.24127999999999, w1=13.523000928373268\n",
      "Sub Gradient Descent(391/499): loss=4.424631543188589, w0=73.24127999999999, w1=13.522966781224712\n",
      "Sub Gradient Descent(392/499): loss=4.424631541522834, w0=73.24127999999999, w1=13.522932634076156\n",
      "Sub Gradient Descent(393/499): loss=4.424631539857081, w0=73.24127999999999, w1=13.5228984869276\n",
      "Sub Gradient Descent(394/499): loss=4.424631538191327, w0=73.24127999999999, w1=13.522864339779044\n",
      "Sub Gradient Descent(395/499): loss=4.4246315386994235, w0=73.24141999999999, w1=13.523087249404908\n",
      "Sub Gradient Descent(396/499): loss=4.42463154739947, w0=73.24141999999999, w1=13.523053102256352\n",
      "Sub Gradient Descent(397/499): loss=4.424631545733717, w0=73.24141999999999, w1=13.523018955107796\n",
      "Sub Gradient Descent(398/499): loss=4.424631544067962, w0=73.24141999999999, w1=13.52298480795924\n",
      "Sub Gradient Descent(399/499): loss=4.424631542402208, w0=73.24141999999999, w1=13.522950660810684\n",
      "Sub Gradient Descent(400/499): loss=4.424631540736454, w0=73.24141999999999, w1=13.522916513662128\n",
      "Sub Gradient Descent(401/499): loss=4.424631539070701, w0=73.24141999999999, w1=13.522882366513572\n",
      "Sub Gradient Descent(402/499): loss=4.424631537404946, w0=73.24141999999999, w1=13.522848219365017\n",
      "Sub Gradient Descent(403/499): loss=4.424631535739193, w0=73.24141999999999, w1=13.52281407221646\n",
      "Sub Gradient Descent(404/499): loss=4.424631534073439, w0=73.24141999999999, w1=13.522779925067905\n",
      "Sub Gradient Descent(405/499): loss=4.424631537580641, w0=73.24155999999999, w1=13.523002834693768\n",
      "Sub Gradient Descent(406/499): loss=4.424631543281582, w0=73.24155999999999, w1=13.522968687545212\n",
      "Sub Gradient Descent(407/499): loss=4.4246315416158275, w0=73.24155999999999, w1=13.522934540396657\n",
      "Sub Gradient Descent(408/499): loss=4.424631539950075, w0=73.24155999999999, w1=13.5229003932481\n",
      "Sub Gradient Descent(409/499): loss=4.42463153828432, w0=73.24155999999999, w1=13.522866246099545\n",
      "Sub Gradient Descent(410/499): loss=4.424631536618567, w0=73.24155999999999, w1=13.522832098950989\n",
      "Sub Gradient Descent(411/499): loss=4.4246315349528125, w0=73.24155999999999, w1=13.522797951802433\n",
      "Sub Gradient Descent(412/499): loss=4.424631533287058, w0=73.24155999999999, w1=13.522763804653877\n",
      "Sub Gradient Descent(413/499): loss=4.424631531621305, w0=73.24155999999999, w1=13.522729657505321\n",
      "Sub Gradient Descent(414/499): loss=4.42463152995555, w0=73.24155999999999, w1=13.522695510356765\n",
      "Sub Gradient Descent(415/499): loss=4.424631536461858, w0=73.2417, w1=13.522918419982629\n",
      "Sub Gradient Descent(416/499): loss=4.424631540335803, w0=73.24155999999999, w1=13.522905630681162\n",
      "Sub Gradient Descent(417/499): loss=4.424631538539811, w0=73.24155999999999, w1=13.522871483532606\n",
      "Sub Gradient Descent(418/499): loss=4.424631536874057, w0=73.24155999999999, w1=13.52283733638405\n",
      "Sub Gradient Descent(419/499): loss=4.424631535208303, w0=73.24155999999999, w1=13.522803189235495\n",
      "Sub Gradient Descent(420/499): loss=4.424631533542549, w0=73.24155999999999, w1=13.522769042086939\n",
      "Sub Gradient Descent(421/499): loss=4.424631531876795, w0=73.24155999999999, w1=13.522734894938383\n",
      "Sub Gradient Descent(422/499): loss=4.424631530211041, w0=73.24155999999999, w1=13.522700747789827\n",
      "Sub Gradient Descent(423/499): loss=4.424631534794038, w0=73.2417, w1=13.52292365741569\n",
      "Sub Gradient Descent(424/499): loss=4.424631540431493, w0=73.24155999999999, w1=13.522910868114224\n",
      "Sub Gradient Descent(425/499): loss=4.4246315387953015, w0=73.24155999999999, w1=13.522876720965668\n",
      "Sub Gradient Descent(426/499): loss=4.424631537129547, w0=73.24155999999999, w1=13.522842573817112\n",
      "Sub Gradient Descent(427/499): loss=4.424631535463794, w0=73.24155999999999, w1=13.522808426668556\n",
      "Sub Gradient Descent(428/499): loss=4.424631533798039, w0=73.24155999999999, w1=13.52277427952\n",
      "Sub Gradient Descent(429/499): loss=4.424631532132285, w0=73.24155999999999, w1=13.522740132371444\n",
      "Sub Gradient Descent(430/499): loss=4.424631530466532, w0=73.24155999999999, w1=13.522705985222888\n",
      "Sub Gradient Descent(431/499): loss=4.424631533126218, w0=73.2417, w1=13.522928894848752\n",
      "Sub Gradient Descent(432/499): loss=4.424631540527184, w0=73.24155999999999, w1=13.522916105547285\n",
      "Sub Gradient Descent(433/499): loss=4.424631539050791, w0=73.24155999999999, w1=13.52288195839873\n",
      "Sub Gradient Descent(434/499): loss=4.424631537385038, w0=73.24155999999999, w1=13.522847811250173\n",
      "Sub Gradient Descent(435/499): loss=4.424631535719284, w0=73.24155999999999, w1=13.522813664101617\n",
      "Sub Gradient Descent(436/499): loss=4.424631534053529, w0=73.24155999999999, w1=13.522779516953062\n",
      "Sub Gradient Descent(437/499): loss=4.424631532387776, w0=73.24155999999999, w1=13.522745369804506\n",
      "Sub Gradient Descent(438/499): loss=4.424631530722022, w0=73.24155999999999, w1=13.52271122265595\n",
      "Sub Gradient Descent(439/499): loss=4.424631531458397, w0=73.2417, w1=13.522934132281813\n",
      "Sub Gradient Descent(440/499): loss=4.424631540622873, w0=73.24155999999999, w1=13.522921342980347\n",
      "Sub Gradient Descent(441/499): loss=4.424631539306283, w0=73.24155999999999, w1=13.52288719583179\n",
      "Sub Gradient Descent(442/499): loss=4.424631537640528, w0=73.24155999999999, w1=13.522853048683235\n",
      "Sub Gradient Descent(443/499): loss=4.4246315359747745, w0=73.24155999999999, w1=13.522818901534679\n",
      "Sub Gradient Descent(444/499): loss=4.42463153430902, w0=73.24155999999999, w1=13.522784754386123\n",
      "Sub Gradient Descent(445/499): loss=4.424631532643267, w0=73.24155999999999, w1=13.522750607237567\n",
      "Sub Gradient Descent(446/499): loss=4.424631530977512, w0=73.24155999999999, w1=13.522716460089011\n",
      "Sub Gradient Descent(447/499): loss=4.4246315297905765, w0=73.2417, w1=13.522939369714875\n",
      "Sub Gradient Descent(448/499): loss=4.424631540718565, w0=73.24155999999999, w1=13.522926580413408\n",
      "Sub Gradient Descent(449/499): loss=4.424631539561774, w0=73.24155999999999, w1=13.522892433264852\n",
      "Sub Gradient Descent(450/499): loss=4.424631537896019, w0=73.24155999999999, w1=13.522858286116296\n",
      "Sub Gradient Descent(451/499): loss=4.424631536230266, w0=73.24155999999999, w1=13.52282413896774\n",
      "Sub Gradient Descent(452/499): loss=4.4246315345645115, w0=73.24155999999999, w1=13.522789991819185\n",
      "Sub Gradient Descent(453/499): loss=4.424631532898757, w0=73.24155999999999, w1=13.522755844670629\n",
      "Sub Gradient Descent(454/499): loss=4.424631531233003, w0=73.24155999999999, w1=13.522721697522073\n",
      "Sub Gradient Descent(455/499): loss=4.424631529567249, w0=73.24155999999999, w1=13.522687550373517\n",
      "Sub Gradient Descent(456/499): loss=4.424631538996653, w0=73.2417, w1=13.52291045999938\n",
      "Sub Gradient Descent(457/499): loss=4.424631540190371, w0=73.24155999999999, w1=13.522897670697914\n",
      "Sub Gradient Descent(458/499): loss=4.42463153815151, w0=73.24155999999999, w1=13.522863523549358\n",
      "Sub Gradient Descent(459/499): loss=4.424631536485756, w0=73.24155999999999, w1=13.522829376400802\n",
      "Sub Gradient Descent(460/499): loss=4.424631534820002, w0=73.24155999999999, w1=13.522795229252246\n",
      "Sub Gradient Descent(461/499): loss=4.424631533154248, w0=73.24155999999999, w1=13.52276108210369\n",
      "Sub Gradient Descent(462/499): loss=4.424631531488494, w0=73.24155999999999, w1=13.522726934955134\n",
      "Sub Gradient Descent(463/499): loss=4.42463152982274, w0=73.24155999999999, w1=13.522692787806578\n",
      "Sub Gradient Descent(464/499): loss=4.424631537328833, w0=73.2417, w1=13.522915697432442\n",
      "Sub Gradient Descent(465/499): loss=4.424631540286062, w0=73.24155999999999, w1=13.522902908130975\n",
      "Sub Gradient Descent(466/499): loss=4.4246315384070005, w0=73.24155999999999, w1=13.52286876098242\n",
      "Sub Gradient Descent(467/499): loss=4.424631536741246, w0=73.24155999999999, w1=13.522834613833863\n",
      "Sub Gradient Descent(468/499): loss=4.424631535075492, w0=73.24155999999999, w1=13.522800466685307\n",
      "Sub Gradient Descent(469/499): loss=4.424631533409738, w0=73.24155999999999, w1=13.522766319536752\n",
      "Sub Gradient Descent(470/499): loss=4.424631531743984, w0=73.24155999999999, w1=13.522732172388196\n",
      "Sub Gradient Descent(471/499): loss=4.424631530078232, w0=73.24155999999999, w1=13.52269802523964\n",
      "Sub Gradient Descent(472/499): loss=4.424631535661012, w0=73.2417, w1=13.522920934865503\n",
      "Sub Gradient Descent(473/499): loss=4.4246315403817515, w0=73.24155999999999, w1=13.522908145564037\n",
      "Sub Gradient Descent(474/499): loss=4.42463153866249, w0=73.24155999999999, w1=13.52287399841548\n",
      "Sub Gradient Descent(475/499): loss=4.4246315369967375, w0=73.24155999999999, w1=13.522839851266925\n",
      "Sub Gradient Descent(476/499): loss=4.424631535330983, w0=73.24155999999999, w1=13.522805704118369\n",
      "Sub Gradient Descent(477/499): loss=4.424631533665229, w0=73.24155999999999, w1=13.522771556969813\n",
      "Sub Gradient Descent(478/499): loss=4.4246315319994745, w0=73.24155999999999, w1=13.522737409821257\n",
      "Sub Gradient Descent(479/499): loss=4.424631530333722, w0=73.24155999999999, w1=13.522703262672701\n",
      "Sub Gradient Descent(480/499): loss=4.424631533993192, w0=73.2417, w1=13.522926172298565\n",
      "Sub Gradient Descent(481/499): loss=4.424631540477442, w0=73.24155999999999, w1=13.522913382997098\n",
      "Sub Gradient Descent(482/499): loss=4.42463153891798, w0=73.24155999999999, w1=13.522879235848542\n",
      "Sub Gradient Descent(483/499): loss=4.424631537252228, w0=73.24155999999999, w1=13.522845088699986\n",
      "Sub Gradient Descent(484/499): loss=4.424631535586474, w0=73.24155999999999, w1=13.52281094155143\n",
      "Sub Gradient Descent(485/499): loss=4.42463153392072, w0=73.24155999999999, w1=13.522776794402875\n",
      "Sub Gradient Descent(486/499): loss=4.424631532254966, w0=73.24155999999999, w1=13.522742647254319\n",
      "Sub Gradient Descent(487/499): loss=4.424631530589211, w0=73.24155999999999, w1=13.522708500105763\n",
      "Sub Gradient Descent(488/499): loss=4.424631532325371, w0=73.2417, w1=13.522931409731626\n",
      "Sub Gradient Descent(489/499): loss=4.424631540573133, w0=73.24155999999999, w1=13.52291862043016\n",
      "Sub Gradient Descent(490/499): loss=4.424631539173472, w0=73.24155999999999, w1=13.522884473281604\n",
      "Sub Gradient Descent(491/499): loss=4.424631537507718, w0=73.24155999999999, w1=13.522850326133048\n",
      "Sub Gradient Descent(492/499): loss=4.424631535841964, w0=73.24155999999999, w1=13.522816178984492\n",
      "Sub Gradient Descent(493/499): loss=4.424631534176211, w0=73.24155999999999, w1=13.522782031835936\n",
      "Sub Gradient Descent(494/499): loss=4.424631532510456, w0=73.24155999999999, w1=13.52274788468738\n",
      "Sub Gradient Descent(495/499): loss=4.424631530844703, w0=73.24155999999999, w1=13.522713737538824\n",
      "Sub Gradient Descent(496/499): loss=4.424631530657551, w0=73.2417, w1=13.522936647164688\n",
      "Sub Gradient Descent(497/499): loss=4.424631540668822, w0=73.24155999999999, w1=13.522923857863221\n",
      "Sub Gradient Descent(498/499): loss=4.424631539428963, w0=73.24155999999999, w1=13.522889710714665\n",
      "Sub Gradient Descent(499/499): loss=4.424631537763209, w0=73.24155999999999, w1=13.52285556356611\n",
      "SubGD: execution time=0.282 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3bdcaf86dd44a48abbe3783da6872b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, err = compute_subgradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = calculate_mae(err)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD (subgradient) ({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (subgradient) (0/499): loss=70.55162104133416, w0=0.7, w1=-0.2049816456543527\n",
      "SGD (subgradient) (1/499): loss=81.04776136718982, w0=1.4, w1=0.35816363871414003\n",
      "SGD (subgradient) (2/499): loss=51.46566410250105, w0=2.0999999999999996, w1=-0.6999400859481573\n",
      "SGD (subgradient) (3/499): loss=40.40690746935321, w0=2.8, w1=-2.2494568418534246\n",
      "SGD (subgradient) (4/499): loss=84.68792570443962, w0=3.5, w1=-1.678756875357442\n",
      "SGD (subgradient) (5/499): loss=58.32371804707144, w0=4.2, w1=-1.6182577354425427\n",
      "SGD (subgradient) (6/499): loss=83.04784303965275, w0=4.9, w1=-1.1174879158028217\n",
      "SGD (subgradient) (7/499): loss=81.89418117359665, w0=5.6000000000000005, w1=-0.38314090951857194\n",
      "SGD (subgradient) (8/499): loss=73.59085081828047, w0=6.300000000000001, w1=-0.5710222125239076\n",
      "SGD (subgradient) (9/499): loss=80.23538134447762, w0=7.000000000000001, w1=0.030598302042027847\n",
      "SGD (subgradient) (10/499): loss=75.8396814011829, w0=7.700000000000001, w1=0.04928140663283371\n",
      "SGD (subgradient) (11/499): loss=53.55579876425565, w0=8.4, w1=-0.10974312595890906\n",
      "SGD (subgradient) (12/499): loss=84.0300113740831, w0=9.1, w1=1.0432446984498847\n",
      "SGD (subgradient) (13/499): loss=67.16879768478594, w0=9.799999999999999, w1=0.9608987869822542\n",
      "SGD (subgradient) (14/499): loss=58.60234113609022, w0=10.499999999999998, w1=1.0321318606422587\n",
      "SGD (subgradient) (15/499): loss=56.16559481365486, w0=11.199999999999998, w1=0.622415138490419\n",
      "SGD (subgradient) (16/499): loss=55.73354641376392, w0=11.899999999999997, w1=-0.004401999656457822\n",
      "SGD (subgradient) (17/499): loss=53.36820297424235, w0=12.599999999999996, w1=0.06957507296823479\n",
      "SGD (subgradient) (18/499): loss=44.19185269546719, w0=13.299999999999995, w1=-0.914271757233293\n",
      "SGD (subgradient) (19/499): loss=76.98478898322617, w0=13.999999999999995, w1=-0.2667915223621592\n",
      "SGD (subgradient) (20/499): loss=34.00292615318426, w0=14.699999999999994, w1=-1.9803672740620626\n",
      "SGD (subgradient) (21/499): loss=75.88255033021406, w0=15.399999999999993, w1=-1.239089289547266\n",
      "SGD (subgradient) (22/499): loss=46.705736314554855, w0=16.099999999999994, w1=-1.5278599804747415\n",
      "SGD (subgradient) (23/499): loss=42.74973245412334, w0=16.799999999999994, w1=-1.8388013754541601\n",
      "SGD (subgradient) (24/499): loss=81.51262442177871, w0=17.499999999999993, w1=-0.8219038238575702\n",
      "SGD (subgradient) (25/499): loss=34.77519268670086, w0=18.199999999999992, w1=-1.5817464941662605\n",
      "SGD (subgradient) (26/499): loss=65.86257274618283, w0=18.89999999999999, w1=-1.2340000137494473\n",
      "SGD (subgradient) (27/499): loss=44.85863056577888, w0=19.59999999999999, w1=-1.7193345525219565\n",
      "SGD (subgradient) (28/499): loss=43.64464763916175, w0=20.29999999999999, w1=-2.072206712323735\n",
      "SGD (subgradient) (29/499): loss=60.802105639057736, w0=20.99999999999999, w1=-1.7695222400949793\n",
      "SGD (subgradient) (30/499): loss=56.914235124502895, w0=21.69999999999999, w1=-1.533933929062299\n",
      "SGD (subgradient) (31/499): loss=79.06816346627963, w0=22.399999999999988, w1=-0.4477259906039275\n",
      "SGD (subgradient) (32/499): loss=54.46464844422931, w0=23.099999999999987, w1=-0.8348388749057806\n",
      "SGD (subgradient) (33/499): loss=44.10655147309514, w0=23.799999999999986, w1=-1.4925089444016124\n",
      "SGD (subgradient) (34/499): loss=58.253106670895065, w0=24.499999999999986, w1=-0.8493008080225226\n",
      "SGD (subgradient) (35/499): loss=66.06092534468263, w0=25.199999999999985, w1=-0.0482378458131395\n",
      "SGD (subgradient) (36/499): loss=75.0650184990692, w0=25.899999999999984, w1=1.1814690275721849\n",
      "SGD (subgradient) (37/499): loss=40.90452425030158, w0=26.599999999999984, w1=0.8857088325257092\n",
      "SGD (subgradient) (38/499): loss=53.71511628392459, w0=27.299999999999983, w1=1.3535088270300306\n",
      "SGD (subgradient) (39/499): loss=57.31394336147872, w0=27.999999999999982, w1=1.2692590495490477\n",
      "SGD (subgradient) (40/499): loss=46.27204930370889, w0=28.69999999999998, w1=1.0748254793180523\n",
      "SGD (subgradient) (41/499): loss=71.60812487171964, w0=29.39999999999998, w1=2.1997180256888216\n",
      "SGD (subgradient) (42/499): loss=35.32174026251114, w0=30.09999999999998, w1=2.2572036517149283\n",
      "SGD (subgradient) (43/499): loss=40.54886339626198, w0=30.79999999999998, w1=2.296294224451913\n",
      "SGD (subgradient) (44/499): loss=37.77583160404782, w0=31.49999999999998, w1=2.4573976984006034\n",
      "SGD (subgradient) (45/499): loss=49.4296081061273, w0=32.19999999999998, w1=3.6166381367545553\n",
      "SGD (subgradient) (46/499): loss=25.308357654608876, w0=32.899999999999984, w1=2.767210565209493\n",
      "SGD (subgradient) (47/499): loss=51.83251484633729, w0=33.59999999999999, w1=3.560071779895498\n",
      "SGD (subgradient) (48/499): loss=26.85838844728074, w0=34.29999999999999, w1=2.6540288215574073\n",
      "SGD (subgradient) (49/499): loss=36.40130917109674, w0=34.99999999999999, w1=2.4755655172928357\n",
      "SGD (subgradient) (50/499): loss=53.72102166343579, w0=35.699999999999996, w1=3.9933527097763006\n",
      "SGD (subgradient) (51/499): loss=40.915839589244, w0=36.4, w1=3.9110067983086703\n",
      "SGD (subgradient) (52/499): loss=53.81034647326254, w0=37.1, w1=4.913729266201453\n",
      "SGD (subgradient) (53/499): loss=45.56910947966183, w0=37.800000000000004, w1=5.749772797594883\n",
      "SGD (subgradient) (54/499): loss=20.914530776033, w0=38.50000000000001, w1=5.00195891529532\n",
      "SGD (subgradient) (55/499): loss=38.459099492360664, w0=39.20000000000001, w1=5.454899866540917\n",
      "SGD (subgradient) (56/499): loss=24.401423005680194, w0=39.90000000000001, w1=4.942647227971445\n",
      "SGD (subgradient) (57/499): loss=30.20206071396305, w0=40.600000000000016, w1=5.079506386606556\n",
      "SGD (subgradient) (58/499): loss=27.292763613670047, w0=41.30000000000002, w1=5.364854528926083\n",
      "SGD (subgradient) (59/499): loss=18.41486786774516, w0=42.00000000000002, w1=4.34454267000233\n",
      "SGD (subgradient) (60/499): loss=34.945202574714315, w0=42.700000000000024, w1=4.282949910481586\n",
      "SGD (subgradient) (61/499): loss=51.27123706769696, w0=43.40000000000003, w1=5.745894673194297\n",
      "SGD (subgradient) (62/499): loss=34.58388757790347, w0=44.10000000000003, w1=5.741439386747522\n",
      "SGD (subgradient) (63/499): loss=43.54975372438395, w0=44.80000000000003, w1=6.476835027496538\n",
      "SGD (subgradient) (64/499): loss=35.430086864860016, w0=45.500000000000036, w1=6.841864901063293\n",
      "SGD (subgradient) (65/499): loss=39.23725671533431, w0=46.20000000000004, w1=7.479181691065072\n",
      "SGD (subgradient) (66/499): loss=28.295358559883248, w0=46.90000000000004, w1=7.854320782205771\n",
      "SGD (subgradient) (67/499): loss=20.745558968746195, w0=47.600000000000044, w1=7.058133679938708\n",
      "SGD (subgradient) (68/499): loss=28.67772961500677, w0=48.30000000000005, w1=7.640645051894483\n",
      "SGD (subgradient) (69/499): loss=37.516804308430295, w0=49.00000000000005, w1=8.520180964207475\n",
      "SGD (subgradient) (70/499): loss=29.835372422017215, w0=49.70000000000005, w1=8.898294801873012\n",
      "SGD (subgradient) (71/499): loss=23.209110359154963, w0=50.400000000000055, w1=9.533984021705027\n",
      "SGD (subgradient) (72/499): loss=31.934404335751587, w0=51.10000000000006, w1=9.519371097581274\n",
      "SGD (subgradient) (73/499): loss=13.323492723483568, w0=51.80000000000006, w1=9.338069232275206\n",
      "SGD (subgradient) (74/499): loss=19.814396094715548, w0=52.500000000000064, w1=9.684174223664968\n",
      "SGD (subgradient) (75/499): loss=21.51595195686211, w0=53.20000000000007, w1=9.074097392541873\n",
      "SGD (subgradient) (76/499): loss=31.240692104853522, w0=53.90000000000007, w1=10.984382103799739\n",
      "SGD (subgradient) (77/499): loss=10.565813158621268, w0=54.60000000000007, w1=10.118807420297303\n",
      "SGD (subgradient) (78/499): loss=16.57014636501178, w0=55.300000000000075, w1=10.095029366519562\n",
      "SGD (subgradient) (79/499): loss=14.530016241789227, w0=56.00000000000008, w1=9.185397278544723\n",
      "SGD (subgradient) (80/499): loss=21.440777827983965, w0=56.70000000000008, w1=10.128037956118323\n",
      "SGD (subgradient) (81/499): loss=19.503348239062525, w0=57.400000000000084, w1=10.24375368400349\n",
      "SGD (subgradient) (82/499): loss=19.16781264926349, w0=58.10000000000009, w1=9.373044753448982\n",
      "SGD (subgradient) (83/499): loss=7.064401680510599, w0=58.80000000000009, w1=8.343610909368618\n",
      "SGD (subgradient) (84/499): loss=22.84637360064565, w0=59.50000000000009, w1=7.906030083807299\n",
      "SGD (subgradient) (85/499): loss=25.053590569314764, w0=60.200000000000095, w1=8.364312130025926\n",
      "SGD (subgradient) (86/499): loss=5.127971235419906, w0=60.9000000000001, w1=8.67909894877369\n",
      "SGD (subgradient) (87/499): loss=14.364342537012561, w0=61.6000000000001, w1=8.581165151972\n",
      "SGD (subgradient) (88/499): loss=15.554405078193938, w0=62.300000000000104, w1=8.146519188657594\n",
      "SGD (subgradient) (89/499): loss=15.39077362148759, w0=63.00000000000011, w1=7.650994428048905\n",
      "SGD (subgradient) (90/499): loss=4.40915977487974, w0=63.70000000000011, w1=7.1183872577555265\n",
      "SGD (subgradient) (91/499): loss=2.8343358428565253, w0=64.4000000000001, w1=6.395786333043568\n",
      "SGD (subgradient) (92/499): loss=8.584849272365219, w0=65.10000000000011, w1=6.4204800808044125\n",
      "SGD (subgradient) (93/499): loss=5.053266125248065, w0=65.80000000000011, w1=5.678016942429848\n",
      "SGD (subgradient) (94/499): loss=4.67476704709253, w0=66.50000000000011, w1=5.351746450921904\n",
      "SGD (subgradient) (95/499): loss=14.303135112529958, w0=67.20000000000012, w1=5.370851248970934\n",
      "SGD (subgradient) (96/499): loss=12.69828444198425, w0=66.50000000000011, w1=6.0228084880723545\n",
      "SGD (subgradient) (97/499): loss=11.507488711305271, w0=67.20000000000012, w1=6.18909940090734\n",
      "SGD (subgradient) (98/499): loss=10.468184446799341, w0=67.90000000000012, w1=6.953609423526233\n",
      "SGD (subgradient) (99/499): loss=1.5797047206043544, w0=68.60000000000012, w1=6.65031045509744\n",
      "SGD (subgradient) (100/499): loss=19.17833896503288, w0=69.30000000000013, w1=6.748792147094245\n",
      "SGD (subgradient) (101/499): loss=16.509579257739787, w0=68.60000000000012, w1=8.496020039542518\n",
      "SGD (subgradient) (102/499): loss=18.655847941477077, w0=69.30000000000013, w1=9.699700492859996\n",
      "SGD (subgradient) (103/499): loss=5.988770030305773, w0=70.00000000000013, w1=11.150395508292242\n",
      "SGD (subgradient) (104/499): loss=5.915405403346554, w0=70.70000000000013, w1=10.784442091715844\n",
      "SGD (subgradient) (105/499): loss=7.7176385977663955, w0=71.40000000000013, w1=11.380174536081105\n",
      "SGD (subgradient) (106/499): loss=1.3281469856470949, w0=72.10000000000014, w1=11.850138038264086\n",
      "SGD (subgradient) (107/499): loss=3.0152457165223154, w0=71.40000000000013, w1=12.547589722781549\n",
      "SGD (subgradient) (108/499): loss=4.662940061755833, w0=72.10000000000014, w1=13.489200084952241\n",
      "SGD (subgradient) (109/499): loss=2.1597268888129832, w0=72.80000000000014, w1=12.902184609884422\n",
      "SGD (subgradient) (110/499): loss=8.284233254906873, w0=72.10000000000014, w1=13.393754231806902\n",
      "SGD (subgradient) (111/499): loss=1.5260585492439418, w0=71.40000000000013, w1=12.99592664872788\n",
      "SGD (subgradient) (112/499): loss=3.3353965458419452, w0=72.10000000000014, w1=12.70988679510609\n",
      "SGD (subgradient) (113/499): loss=1.0180168768008997, w0=71.40000000000013, w1=10.480377719146112\n",
      "SGD (subgradient) (114/499): loss=3.204326433311465, w0=72.10000000000014, w1=10.732468622590476\n",
      "SGD (subgradient) (115/499): loss=7.220030739637622, w0=72.80000000000014, w1=11.362157014445923\n",
      "SGD (subgradient) (116/499): loss=4.379370718926353, w0=73.50000000000014, w1=11.305752869710814\n",
      "SGD (subgradient) (117/499): loss=8.191899950994433, w0=72.80000000000014, w1=11.418530702880677\n",
      "SGD (subgradient) (118/499): loss=12.532510428445264, w0=72.10000000000014, w1=10.709182704417072\n",
      "SGD (subgradient) (119/499): loss=9.843774852061216, w0=72.80000000000014, w1=11.343458791771774\n",
      "SGD (subgradient) (120/499): loss=0.7957573820071744, w0=72.10000000000014, w1=12.064593928160017\n",
      "SGD (subgradient) (121/499): loss=0.5208557884511293, w0=72.80000000000014, w1=11.987285580271307\n",
      "SGD (subgradient) (122/499): loss=3.677408389952234, w0=73.50000000000014, w1=12.841794300552754\n",
      "SGD (subgradient) (123/499): loss=7.2940143892310445, w0=72.80000000000014, w1=13.549881961881493\n",
      "SGD (subgradient) (124/499): loss=5.310397585771241, w0=73.50000000000014, w1=13.769960475652427\n",
      "SGD (subgradient) (125/499): loss=3.67643287416125, w0=74.20000000000014, w1=14.74036957939399\n",
      "SGD (subgradient) (126/499): loss=2.0648882805219984, w0=74.90000000000015, w1=13.803367112411873\n",
      "SGD (subgradient) (127/499): loss=1.8379207061329907, w0=75.60000000000015, w1=13.500654423920967\n",
      "SGD (subgradient) (128/499): loss=8.991438245079678, w0=74.90000000000015, w1=13.587688927762837\n",
      "SGD (subgradient) (129/499): loss=2.300708585174192, w0=74.20000000000014, w1=14.81416588863101\n",
      "SGD (subgradient) (130/499): loss=5.85438092542357, w0=74.90000000000015, w1=14.35830157105576\n",
      "SGD (subgradient) (131/499): loss=6.910945640812216, w0=75.60000000000015, w1=14.214240385335676\n",
      "SGD (subgradient) (132/499): loss=18.165554810637815, w0=74.90000000000015, w1=13.504892386872072\n",
      "SGD (subgradient) (133/499): loss=5.332376143597742, w0=74.20000000000014, w1=13.041362179047766\n",
      "SGD (subgradient) (134/499): loss=2.1807611411218133, w0=74.90000000000015, w1=12.33984802987764\n",
      "SGD (subgradient) (135/499): loss=7.545598449035012, w0=75.60000000000015, w1=13.06062570312094\n",
      "SGD (subgradient) (136/499): loss=0.7167913442155509, w0=74.90000000000015, w1=12.543885137694712\n",
      "SGD (subgradient) (137/499): loss=16.15071337416221, w0=74.20000000000014, w1=13.642024653015062\n",
      "SGD (subgradient) (138/499): loss=5.452281183345704, w0=74.90000000000015, w1=13.226047704433308\n",
      "SGD (subgradient) (139/499): loss=3.200375514975761, w0=74.20000000000014, w1=13.276820563644119\n",
      "SGD (subgradient) (140/499): loss=2.6651653202840464, w0=73.50000000000014, w1=13.100568606554345\n",
      "SGD (subgradient) (141/499): loss=1.821610268045859, w0=72.80000000000014, w1=12.350584007965832\n",
      "SGD (subgradient) (142/499): loss=0.4285896784076044, w0=73.50000000000014, w1=12.685747462123164\n",
      "SGD (subgradient) (143/499): loss=4.03083953987899, w0=72.80000000000014, w1=13.668289635861008\n",
      "SGD (subgradient) (144/499): loss=8.976446208248433, w0=73.50000000000014, w1=13.861562882626169\n",
      "SGD (subgradient) (145/499): loss=1.2948689341917188, w0=72.80000000000014, w1=14.553283264885946\n",
      "SGD (subgradient) (146/499): loss=2.164380818866263, w0=72.10000000000014, w1=14.830741609179228\n",
      "SGD (subgradient) (147/499): loss=11.433697515469262, w0=71.40000000000013, w1=15.25002376628739\n",
      "SGD (subgradient) (148/499): loss=5.583359517200904, w0=70.70000000000013, w1=14.354055335916176\n",
      "SGD (subgradient) (149/499): loss=8.03661953305518, w0=71.40000000000013, w1=13.556211428104666\n",
      "SGD (subgradient) (150/499): loss=3.602305555652549, w0=70.70000000000013, w1=13.435473970048966\n",
      "SGD (subgradient) (151/499): loss=7.401564977885357, w0=71.40000000000013, w1=14.1296858853808\n",
      "SGD (subgradient) (152/499): loss=7.26292730384688, w0=72.10000000000014, w1=13.246361380024345\n",
      "SGD (subgradient) (153/499): loss=2.7127052384233252, w0=71.40000000000013, w1=12.38636378309654\n",
      "SGD (subgradient) (154/499): loss=5.053183455501298, w0=72.10000000000014, w1=12.275712513614168\n",
      "SGD (subgradient) (155/499): loss=3.045807027135055, w0=72.80000000000014, w1=12.154256274379245\n",
      "SGD (subgradient) (156/499): loss=1.2873028647478861, w0=72.10000000000014, w1=12.581527064756047\n",
      "SGD (subgradient) (157/499): loss=1.7817573313591524, w0=71.40000000000013, w1=13.276029348877023\n",
      "SGD (subgradient) (158/499): loss=3.4489290556291223, w0=70.70000000000013, w1=14.250399282945535\n",
      "SGD (subgradient) (159/499): loss=6.672622786871514, w0=71.40000000000013, w1=14.75785340771156\n",
      "SGD (subgradient) (160/499): loss=18.707184130260927, w0=72.10000000000014, w1=14.473289660861214\n",
      "SGD (subgradient) (161/499): loss=6.634477867907876, w0=71.40000000000013, w1=12.243780584901238\n",
      "SGD (subgradient) (162/499): loss=1.388548614658987, w0=72.10000000000014, w1=13.284828560133057\n",
      "SGD (subgradient) (163/499): loss=2.7856189194129612, w0=71.40000000000013, w1=13.903263604298761\n",
      "SGD (subgradient) (164/499): loss=2.986274268811137, w0=72.10000000000014, w1=12.990364951972017\n",
      "SGD (subgradient) (165/499): loss=2.1599908582128933, w0=71.40000000000013, w1=12.945446139882971\n",
      "SGD (subgradient) (166/499): loss=0.548597920130014, w0=70.70000000000013, w1=13.722415725297743\n",
      "SGD (subgradient) (167/499): loss=7.907852590136741, w0=70.00000000000013, w1=13.757878012493661\n",
      "SGD (subgradient) (168/499): loss=20.056472059793016, w0=70.70000000000013, w1=13.060066666844577\n",
      "SGD (subgradient) (169/499): loss=3.9277551745093717, w0=71.40000000000013, w1=13.640740034893229\n",
      "SGD (subgradient) (170/499): loss=3.450708215923086, w0=72.10000000000014, w1=13.75416112910778\n",
      "SGD (subgradient) (171/499): loss=4.0850935851185, w0=72.80000000000014, w1=14.604894680259925\n",
      "SGD (subgradient) (172/499): loss=2.976315163330021, w0=72.10000000000014, w1=15.184131340989174\n",
      "SGD (subgradient) (173/499): loss=6.887992717869807, w0=72.80000000000014, w1=14.447869256778224\n",
      "SGD (subgradient) (174/499): loss=7.5144366858747205, w0=73.50000000000014, w1=13.454355470098976\n",
      "SGD (subgradient) (175/499): loss=0.01955967940754988, w0=72.80000000000014, w1=13.81261730703764\n",
      "SGD (subgradient) (176/499): loss=4.051213837169648, w0=72.10000000000014, w1=14.17680914343186\n",
      "SGD (subgradient) (177/499): loss=1.0884730386883916, w0=71.40000000000013, w1=14.844231389088572\n",
      "SGD (subgradient) (178/499): loss=2.498567758157087, w0=72.10000000000014, w1=14.366422829020207\n",
      "SGD (subgradient) (179/499): loss=5.200271035050818, w0=72.80000000000014, w1=15.020141054672113\n",
      "SGD (subgradient) (180/499): loss=8.130119183768024, w0=73.50000000000014, w1=13.353603546974675\n",
      "SGD (subgradient) (181/499): loss=2.997588445342913, w0=74.20000000000014, w1=14.183553399783968\n",
      "SGD (subgradient) (182/499): loss=2.03509060439238, w0=73.50000000000014, w1=13.944747386830498\n",
      "SGD (subgradient) (183/499): loss=9.665242931262874, w0=72.80000000000014, w1=14.37302669896548\n",
      "SGD (subgradient) (184/499): loss=1.1924257411523485, w0=73.50000000000014, w1=13.77532690621565\n",
      "SGD (subgradient) (185/499): loss=11.165491740833609, w0=72.80000000000014, w1=13.775628480280792\n",
      "SGD (subgradient) (186/499): loss=2.8980655728111486, w0=73.50000000000014, w1=15.01972261370126\n",
      "SGD (subgradient) (187/499): loss=0.02498240403861729, w0=74.20000000000014, w1=14.319411969715617\n",
      "SGD (subgradient) (188/499): loss=4.132227240422623, w0=73.50000000000014, w1=14.40497846887982\n",
      "SGD (subgradient) (189/499): loss=1.8387454181408174, w0=74.20000000000014, w1=14.794384664972036\n",
      "SGD (subgradient) (190/499): loss=4.733188810991933, w0=74.90000000000015, w1=15.37042621317704\n",
      "SGD (subgradient) (191/499): loss=9.894143893122546, w0=75.60000000000015, w1=14.952808543973676\n",
      "SGD (subgradient) (192/499): loss=5.865237395634743, w0=76.30000000000015, w1=14.171205013806151\n",
      "SGD (subgradient) (193/499): loss=6.248921543939645, w0=77.00000000000016, w1=14.528468670765397\n",
      "SGD (subgradient) (194/499): loss=2.7957608299669374, w0=77.70000000000016, w1=14.268571666118216\n",
      "SGD (subgradient) (195/499): loss=6.270705934716901, w0=77.00000000000016, w1=13.424498659711293\n",
      "SGD (subgradient) (196/499): loss=3.2049076786679507, w0=76.30000000000015, w1=12.43695000203835\n",
      "SGD (subgradient) (197/499): loss=10.024440975662301, w0=77.00000000000016, w1=12.855348305010047\n",
      "SGD (subgradient) (198/499): loss=2.6711305430763446, w0=76.30000000000015, w1=12.274461017316534\n",
      "SGD (subgradient) (199/499): loss=3.5575991419210027, w0=75.60000000000015, w1=11.974829807783552\n",
      "SGD (subgradient) (200/499): loss=5.020278600496113, w0=76.30000000000015, w1=11.523848642709694\n",
      "SGD (subgradient) (201/499): loss=1.9784995476498466, w0=77.00000000000016, w1=11.734457251439434\n",
      "SGD (subgradient) (202/499): loss=11.297387679282807, w0=77.70000000000016, w1=11.687819411037173\n",
      "SGD (subgradient) (203/499): loss=6.808615836562453, w0=77.00000000000016, w1=11.187065057075708\n",
      "SGD (subgradient) (204/499): loss=4.808704437334228, w0=76.30000000000015, w1=10.736071131943346\n",
      "SGD (subgradient) (205/499): loss=1.8169163608627628, w0=75.60000000000015, w1=11.055125399514838\n",
      "SGD (subgradient) (206/499): loss=5.108298928151157, w0=74.90000000000015, w1=9.979318984542651\n",
      "SGD (subgradient) (207/499): loss=3.213470036539775, w0=74.20000000000014, w1=10.779739326995692\n",
      "SGD (subgradient) (208/499): loss=2.141698992023805, w0=74.90000000000015, w1=11.51924899663052\n",
      "SGD (subgradient) (209/499): loss=0.05470869106591181, w0=75.60000000000015, w1=11.59103506695564\n",
      "SGD (subgradient) (210/499): loss=3.556360023483208, w0=76.30000000000015, w1=11.606405473720837\n",
      "SGD (subgradient) (211/499): loss=7.6892208507021635, w0=75.60000000000015, w1=12.046906141146712\n",
      "SGD (subgradient) (212/499): loss=5.181452245314723, w0=74.90000000000015, w1=13.530468363508527\n",
      "SGD (subgradient) (213/499): loss=0.6820530451364064, w0=74.20000000000014, w1=14.580783604654485\n",
      "SGD (subgradient) (214/499): loss=2.9811153511543864, w0=73.50000000000014, w1=13.9759681009514\n",
      "SGD (subgradient) (215/499): loss=3.1917877592216826, w0=74.20000000000014, w1=13.946521444248642\n",
      "SGD (subgradient) (216/499): loss=5.3451452366526055, w0=74.90000000000015, w1=12.988885176705436\n",
      "SGD (subgradient) (217/499): loss=3.528610583490078, w0=74.20000000000014, w1=13.014315036091334\n",
      "SGD (subgradient) (218/499): loss=1.3519890833602517, w0=74.90000000000015, w1=13.761913400350046\n",
      "SGD (subgradient) (219/499): loss=0.20221831278763602, w0=74.20000000000014, w1=14.339261490841041\n",
      "SGD (subgradient) (220/499): loss=3.911738839606471, w0=74.90000000000015, w1=14.370779592047972\n",
      "SGD (subgradient) (221/499): loss=2.3281430975823127, w0=74.20000000000014, w1=14.965476621253888\n",
      "SGD (subgradient) (222/499): loss=7.982720305216091, w0=73.50000000000014, w1=13.566532695514915\n",
      "SGD (subgradient) (223/499): loss=3.7740001888621038, w0=72.80000000000014, w1=14.076867615147286\n",
      "SGD (subgradient) (224/499): loss=3.238094721538559, w0=73.50000000000014, w1=13.725561318512494\n",
      "SGD (subgradient) (225/499): loss=2.245372152250738, w0=72.80000000000014, w1=14.04499438718446\n",
      "SGD (subgradient) (226/499): loss=0.2035803409204675, w0=72.10000000000014, w1=13.86321761963537\n",
      "SGD (subgradient) (227/499): loss=4.487256283391105, w0=72.80000000000014, w1=13.88424065382734\n",
      "SGD (subgradient) (228/499): loss=4.821733075195951, w0=73.50000000000014, w1=14.67567554429565\n",
      "SGD (subgradient) (229/499): loss=0.254266750054569, w0=72.80000000000014, w1=14.022535930846546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (subgradient) (230/499): loss=3.286108778927087, w0=73.50000000000014, w1=14.612803149042316\n",
      "SGD (subgradient) (231/499): loss=3.422727328689433, w0=72.80000000000014, w1=14.263564748211483\n",
      "SGD (subgradient) (232/499): loss=0.7089449002826811, w0=72.10000000000014, w1=15.366822459213632\n",
      "SGD (subgradient) (233/499): loss=2.645756664295078, w0=72.80000000000014, w1=14.815402697691505\n",
      "SGD (subgradient) (234/499): loss=0.6681833651718563, w0=72.10000000000014, w1=14.596963670589703\n",
      "SGD (subgradient) (235/499): loss=3.346946008207965, w0=71.40000000000013, w1=14.79112092309155\n",
      "SGD (subgradient) (236/499): loss=8.977974763876645, w0=70.70000000000013, w1=14.283609286072762\n",
      "SGD (subgradient) (237/499): loss=7.726552190270027, w0=71.40000000000013, w1=14.242499696685485\n",
      "SGD (subgradient) (238/499): loss=6.627741953081241, w0=72.10000000000014, w1=15.250187156909705\n",
      "SGD (subgradient) (239/499): loss=3.487170789451703, w0=72.80000000000014, w1=14.542218616927517\n",
      "SGD (subgradient) (240/499): loss=3.6435251329200113, w0=73.50000000000014, w1=13.969240388669466\n",
      "SGD (subgradient) (241/499): loss=6.221563717879647, w0=74.20000000000014, w1=13.240322305709189\n",
      "SGD (subgradient) (242/499): loss=7.9656870872204735, w0=74.90000000000015, w1=13.466780831812839\n",
      "SGD (subgradient) (243/499): loss=3.221800115688069, w0=74.20000000000014, w1=13.74938872050802\n",
      "SGD (subgradient) (244/499): loss=2.077936369082707, w0=74.90000000000015, w1=13.318356650307985\n",
      "SGD (subgradient) (245/499): loss=3.170740414852574, w0=74.20000000000014, w1=13.603675185484375\n",
      "SGD (subgradient) (246/499): loss=1.7742079384366747, w0=73.50000000000014, w1=12.882238577273078\n",
      "SGD (subgradient) (247/499): loss=4.9989234557615845, w0=72.80000000000014, w1=13.287414744172345\n",
      "SGD (subgradient) (248/499): loss=1.7159522541360417, w0=73.50000000000014, w1=13.682867027427397\n",
      "SGD (subgradient) (249/499): loss=1.8180347023342023, w0=74.20000000000014, w1=13.1755233834314\n",
      "SGD (subgradient) (250/499): loss=2.555969278071572, w0=73.50000000000014, w1=12.968509511711744\n",
      "SGD (subgradient) (251/499): loss=8.943603601872653, w0=74.20000000000014, w1=13.296794078147178\n",
      "SGD (subgradient) (252/499): loss=4.127360481956174, w0=74.90000000000015, w1=13.198659993357436\n",
      "SGD (subgradient) (253/499): loss=0.9612502056094456, w0=74.20000000000014, w1=13.483232618560125\n",
      "SGD (subgradient) (254/499): loss=6.027043310794376, w0=73.50000000000014, w1=13.982137371901867\n",
      "SGD (subgradient) (255/499): loss=1.318320115532586, w0=72.80000000000014, w1=14.02918030470943\n",
      "SGD (subgradient) (256/499): loss=5.101127098730942, w0=72.10000000000014, w1=14.501211310810053\n",
      "SGD (subgradient) (257/499): loss=2.8265592765419356, w0=71.40000000000013, w1=14.805920059669546\n",
      "SGD (subgradient) (258/499): loss=5.802210630054546, w0=70.70000000000013, w1=15.271893071881712\n",
      "SGD (subgradient) (259/499): loss=6.448014340144795, w0=71.40000000000013, w1=15.306402192349893\n",
      "SGD (subgradient) (260/499): loss=13.991678139089267, w0=72.10000000000014, w1=14.125028472210122\n",
      "SGD (subgradient) (261/499): loss=8.07596024708684, w0=72.80000000000014, w1=13.679086889411755\n",
      "SGD (subgradient) (262/499): loss=1.100118053255656, w0=73.50000000000014, w1=13.273257834794157\n",
      "SGD (subgradient) (263/499): loss=4.325970977775526, w0=72.80000000000014, w1=14.291508163136582\n",
      "SGD (subgradient) (264/499): loss=9.507458078112663, w0=73.50000000000014, w1=12.617309646323891\n",
      "SGD (subgradient) (265/499): loss=2.4333535961424104, w0=74.20000000000014, w1=13.274379087801995\n",
      "SGD (subgradient) (266/499): loss=5.130140649722065, w0=74.90000000000015, w1=13.014482083154814\n",
      "SGD (subgradient) (267/499): loss=3.584715714251786, w0=74.20000000000014, w1=11.853795773951644\n",
      "SGD (subgradient) (268/499): loss=0.08678984195921657, w0=74.90000000000015, w1=12.919837670110258\n",
      "SGD (subgradient) (269/499): loss=6.363963573218967, w0=75.60000000000015, w1=13.243500542516394\n",
      "SGD (subgradient) (270/499): loss=3.8176379430003777, w0=76.30000000000015, w1=13.70782982232393\n",
      "SGD (subgradient) (271/499): loss=3.6078213461729405, w0=75.60000000000015, w1=13.369403837271472\n",
      "SGD (subgradient) (272/499): loss=3.7614488129259342, w0=74.90000000000015, w1=13.777849862868365\n",
      "SGD (subgradient) (273/499): loss=6.510470908515963, w0=74.20000000000014, w1=14.074637819025755\n",
      "SGD (subgradient) (274/499): loss=4.071737519439132, w0=74.90000000000015, w1=13.33217468065119\n",
      "SGD (subgradient) (275/499): loss=1.55295185669263, w0=75.60000000000015, w1=13.600324381101702\n",
      "SGD (subgradient) (276/499): loss=5.072925380910604, w0=76.30000000000015, w1=14.359875760422549\n",
      "SGD (subgradient) (277/499): loss=4.803478502583928, w0=77.00000000000016, w1=14.071498603114334\n",
      "SGD (subgradient) (278/499): loss=0.8993831241910613, w0=76.30000000000015, w1=12.270050498907484\n",
      "SGD (subgradient) (279/499): loss=2.903807001989719, w0=75.60000000000015, w1=11.68588639041002\n",
      "SGD (subgradient) (280/499): loss=2.463126413414628, w0=76.30000000000015, w1=12.801571880645454\n",
      "SGD (subgradient) (281/499): loss=4.685987030093052, w0=75.60000000000015, w1=14.151474161168888\n",
      "SGD (subgradient) (282/499): loss=5.133025074520873, w0=76.30000000000015, w1=14.365447219730095\n",
      "SGD (subgradient) (283/499): loss=19.563185559252076, w0=75.60000000000015, w1=14.386486630061176\n",
      "SGD (subgradient) (284/499): loss=5.284960608833742, w0=74.90000000000015, w1=14.21751365033627\n",
      "SGD (subgradient) (285/499): loss=5.420721334438468, w0=74.20000000000014, w1=14.269643662678098\n",
      "SGD (subgradient) (286/499): loss=1.3187069262797024, w0=74.90000000000015, w1=14.385359390563265\n",
      "SGD (subgradient) (287/499): loss=6.704829717662179, w0=75.60000000000015, w1=14.890863613206552\n",
      "SGD (subgradient) (288/499): loss=1.5589187170066623, w0=74.90000000000015, w1=14.977943574816365\n",
      "SGD (subgradient) (289/499): loss=7.23718101748743, w0=75.60000000000015, w1=15.33520723177561\n",
      "SGD (subgradient) (290/499): loss=1.622716579227891, w0=76.30000000000015, w1=14.185245525417981\n",
      "SGD (subgradient) (291/499): loss=0.12328458935949982, w0=77.00000000000016, w1=14.550450160139416\n",
      "SGD (subgradient) (292/499): loss=8.165840052483858, w0=76.30000000000015, w1=14.648006918630712\n",
      "SGD (subgradient) (293/499): loss=6.335073789602362, w0=75.60000000000015, w1=13.20007451702821\n",
      "SGD (subgradient) (294/499): loss=10.204343340393642, w0=76.30000000000015, w1=12.00909602760034\n",
      "SGD (subgradient) (295/499): loss=4.947094591946353, w0=77.00000000000016, w1=12.260946005288387\n",
      "SGD (subgradient) (296/499): loss=0.2851067417573603, w0=76.30000000000015, w1=11.743275502658472\n",
      "SGD (subgradient) (297/499): loss=2.461209833286503, w0=75.60000000000015, w1=11.497240786400555\n",
      "SGD (subgradient) (298/499): loss=6.544262744189652, w0=74.90000000000015, w1=11.597571479396498\n",
      "SGD (subgradient) (299/499): loss=4.0779255232542795, w0=74.20000000000014, w1=11.822908125022044\n",
      "SGD (subgradient) (300/499): loss=0.5714565201512869, w0=73.50000000000014, w1=11.507905791347511\n",
      "SGD (subgradient) (301/499): loss=3.8770827113843467, w0=74.20000000000014, w1=10.87527891077614\n",
      "SGD (subgradient) (302/499): loss=5.6443659593236575, w0=73.50000000000014, w1=11.710983363280073\n",
      "SGD (subgradient) (303/499): loss=1.5975887240413726, w0=72.80000000000014, w1=10.470053828442126\n",
      "SGD (subgradient) (304/499): loss=1.7359953789819542, w0=73.50000000000014, w1=10.349463849782046\n",
      "SGD (subgradient) (305/499): loss=1.6553394535177333, w0=72.80000000000014, w1=11.217578805433144\n",
      "SGD (subgradient) (306/499): loss=5.367601017320496, w0=73.50000000000014, w1=11.919433597359678\n",
      "SGD (subgradient) (307/499): loss=1.065064149393045, w0=72.80000000000014, w1=12.900616029977957\n",
      "SGD (subgradient) (308/499): loss=5.395172335155053, w0=73.50000000000014, w1=13.031352341262103\n",
      "SGD (subgradient) (309/499): loss=4.408129234856979, w0=72.80000000000014, w1=13.426761875982589\n",
      "SGD (subgradient) (310/499): loss=2.739931052208547, w0=73.50000000000014, w1=13.419476393215165\n",
      "SGD (subgradient) (311/499): loss=6.312063714019999, w0=74.20000000000014, w1=13.609920482750598\n",
      "SGD (subgradient) (312/499): loss=3.093871510620062, w0=73.50000000000014, w1=13.05991908404446\n",
      "SGD (subgradient) (313/499): loss=11.029336304041244, w0=72.80000000000014, w1=13.037810214289284\n",
      "SGD (subgradient) (314/499): loss=1.0231359551365102, w0=73.50000000000014, w1=13.963742340224963\n",
      "SGD (subgradient) (315/499): loss=3.191273462394477, w0=74.20000000000014, w1=13.934295683522205\n",
      "SGD (subgradient) (316/499): loss=2.5496899187457274, w0=73.50000000000014, w1=13.518979513895314\n",
      "SGD (subgradient) (317/499): loss=3.243365387347737, w0=72.80000000000014, w1=13.251090278816983\n",
      "SGD (subgradient) (318/499): loss=8.269598515495403, w0=73.50000000000014, w1=12.660566542489802\n",
      "SGD (subgradient) (319/499): loss=5.232009029800317, w0=72.80000000000014, w1=13.412325454580841\n",
      "SGD (subgradient) (320/499): loss=5.5296643089360344, w0=72.10000000000014, w1=14.271131140860529\n",
      "SGD (subgradient) (321/499): loss=12.745534515064577, w0=72.80000000000014, w1=14.485108671325271\n",
      "SGD (subgradient) (322/499): loss=11.892709911148714, w0=73.50000000000014, w1=14.307179964019298\n",
      "SGD (subgradient) (323/499): loss=12.614707784821455, w0=74.20000000000014, w1=13.007277621045068\n",
      "SGD (subgradient) (324/499): loss=0.8949854638320289, w0=73.50000000000014, w1=12.104290209260068\n",
      "SGD (subgradient) (325/499): loss=7.582620606056395, w0=74.20000000000014, w1=11.998975518565235\n",
      "SGD (subgradient) (326/499): loss=3.5489280455558756, w0=73.50000000000014, w1=12.648092477083337\n",
      "SGD (subgradient) (327/499): loss=2.7987722379924946, w0=74.20000000000014, w1=12.177854882667917\n",
      "SGD (subgradient) (328/499): loss=3.7410914010821372, w0=73.50000000000014, w1=12.962315535423658\n",
      "SGD (subgradient) (329/499): loss=3.3941138093840237, w0=74.20000000000014, w1=13.580977455216821\n",
      "SGD (subgradient) (330/499): loss=4.943841401561414, w0=74.90000000000015, w1=13.284368371876827\n",
      "SGD (subgradient) (331/499): loss=0.09244521581106824, w0=75.60000000000015, w1=13.581495122243567\n",
      "SGD (subgradient) (332/499): loss=11.788651096551163, w0=74.90000000000015, w1=14.467321433508989\n",
      "SGD (subgradient) (333/499): loss=6.018875215391972, w0=75.60000000000015, w1=14.995915020136042\n",
      "SGD (subgradient) (334/499): loss=2.3605367966646824, w0=76.30000000000015, w1=13.604580991160702\n",
      "SGD (subgradient) (335/499): loss=2.080434596455021, w0=77.00000000000016, w1=14.665573313334317\n",
      "SGD (subgradient) (336/499): loss=7.101483770324947, w0=76.30000000000015, w1=13.843712590608746\n",
      "SGD (subgradient) (337/499): loss=8.303407155877885, w0=75.60000000000015, w1=15.106886298228385\n",
      "SGD (subgradient) (338/499): loss=1.4497414675357732, w0=74.90000000000015, w1=14.629370601546537\n",
      "SGD (subgradient) (339/499): loss=0.47591111967901156, w0=75.60000000000015, w1=13.399077738477606\n",
      "SGD (subgradient) (340/499): loss=5.665613715007822, w0=74.90000000000015, w1=13.868982767201528\n",
      "SGD (subgradient) (341/499): loss=8.863542437557498, w0=75.60000000000015, w1=13.36884712571432\n",
      "SGD (subgradient) (342/499): loss=2.36471940186037, w0=74.90000000000015, w1=14.568659355992907\n",
      "SGD (subgradient) (343/499): loss=10.160668581864584, w0=74.20000000000014, w1=14.703980331110621\n",
      "SGD (subgradient) (344/499): loss=9.045896187808992, w0=74.90000000000015, w1=14.413017618155324\n",
      "SGD (subgradient) (345/499): loss=1.9452327599657622, w0=74.20000000000014, w1=13.693029613456527\n",
      "SGD (subgradient) (346/499): loss=7.308683250051878, w0=73.50000000000014, w1=14.31668840060866\n",
      "SGD (subgradient) (347/499): loss=4.384901757599188, w0=74.20000000000014, w1=13.673217591944425\n",
      "SGD (subgradient) (348/499): loss=6.052734496307011, w0=73.50000000000014, w1=12.839565641220982\n",
      "SGD (subgradient) (349/499): loss=3.223822183777699, w0=74.20000000000014, w1=13.068217687596261\n",
      "SGD (subgradient) (350/499): loss=0.6961497283807816, w0=73.50000000000014, w1=13.938274107354909\n",
      "SGD (subgradient) (351/499): loss=5.612420987855174, w0=72.80000000000014, w1=14.91132805657392\n",
      "SGD (subgradient) (352/499): loss=0.9325293687183489, w0=73.50000000000014, w1=15.278871330447\n",
      "SGD (subgradient) (353/499): loss=5.989942578049622, w0=74.20000000000014, w1=15.706980223708955\n",
      "SGD (subgradient) (354/499): loss=6.347445541135876, w0=73.50000000000014, w1=16.339006351692102\n",
      "SGD (subgradient) (355/499): loss=4.7312659593816875, w0=72.80000000000014, w1=16.154127848836794\n",
      "SGD (subgradient) (356/499): loss=7.933123731578831, w0=73.50000000000014, w1=15.10228850370215\n",
      "SGD (subgradient) (357/499): loss=0.5135496930439984, w0=74.20000000000014, w1=15.542142252463588\n",
      "SGD (subgradient) (358/499): loss=5.308204322880314, w0=74.90000000000015, w1=14.423692159918865\n",
      "SGD (subgradient) (359/499): loss=4.169589892074697, w0=74.20000000000014, w1=12.854571203601374\n",
      "SGD (subgradient) (360/499): loss=8.84213225888493, w0=73.50000000000014, w1=13.060062736474595\n",
      "SGD (subgradient) (361/499): loss=5.235408328652838, w0=74.20000000000014, w1=13.522453584838669\n",
      "SGD (subgradient) (362/499): loss=4.92950852353097, w0=73.50000000000014, w1=12.994402795510196\n",
      "SGD (subgradient) (363/499): loss=3.1274709480751426, w0=72.80000000000014, w1=11.886895716347093\n",
      "SGD (subgradient) (364/499): loss=1.7425092328971772, w0=72.10000000000014, w1=11.026898119419288\n",
      "SGD (subgradient) (365/499): loss=1.6488638431446674, w0=72.80000000000014, w1=11.844152231101932\n",
      "SGD (subgradient) (366/499): loss=3.5554658691782492, w0=73.50000000000014, w1=12.087601331149346\n",
      "SGD (subgradient) (367/499): loss=7.431045684856159, w0=74.20000000000014, w1=12.230300842048603\n",
      "SGD (subgradient) (368/499): loss=0.8094276376957907, w0=74.90000000000015, w1=13.27676024917916\n",
      "SGD (subgradient) (369/499): loss=1.9217644531732105, w0=75.60000000000015, w1=14.574176730902868\n",
      "SGD (subgradient) (370/499): loss=0.0636715316891383, w0=74.90000000000015, w1=13.082021958715679\n",
      "SGD (subgradient) (371/499): loss=3.7772142039225827, w0=75.60000000000015, w1=13.208757698078994\n",
      "SGD (subgradient) (372/499): loss=2.0108335013208176, w0=74.90000000000015, w1=12.393644331697962\n",
      "SGD (subgradient) (373/499): loss=1.7961975355138549, w0=75.60000000000015, w1=11.310765207013802\n",
      "SGD (subgradient) (374/499): loss=12.721787342700331, w0=74.90000000000015, w1=11.57147594982966\n",
      "SGD (subgradient) (375/499): loss=2.5502575462503216, w0=74.20000000000014, w1=12.191090122679105\n",
      "SGD (subgradient) (376/499): loss=3.4909881963708713, w0=73.50000000000014, w1=13.388110932750957\n",
      "SGD (subgradient) (377/499): loss=5.450293182109789, w0=72.80000000000014, w1=13.31558859134065\n",
      "SGD (subgradient) (378/499): loss=0.9451586161333552, w0=73.50000000000014, w1=13.740170417058803\n",
      "SGD (subgradient) (379/499): loss=5.125303652202163, w0=72.80000000000014, w1=13.276353316130182\n",
      "SGD (subgradient) (380/499): loss=1.5003211677248416, w0=73.50000000000014, w1=12.337197168630553\n",
      "SGD (subgradient) (381/499): loss=17.318729460300148, w0=72.80000000000014, w1=12.442208858162369\n",
      "SGD (subgradient) (382/499): loss=6.1639716272805885, w0=72.10000000000014, w1=11.893814158406112\n",
      "SGD (subgradient) (383/499): loss=5.154448537800903, w0=72.80000000000014, w1=12.360411528680478\n",
      "SGD (subgradient) (384/499): loss=4.740635192567794, w0=73.50000000000014, w1=12.56590926083077\n",
      "SGD (subgradient) (385/499): loss=3.8490428590168477, w0=74.20000000000014, w1=11.323443774863366\n",
      "SGD (subgradient) (386/499): loss=8.82408185738538, w0=74.90000000000015, w1=11.888934835974757\n",
      "SGD (subgradient) (387/499): loss=6.751207276204092, w0=74.20000000000014, w1=13.090613974567443\n",
      "SGD (subgradient) (388/499): loss=0.260673237273096, w0=73.50000000000014, w1=12.546255779437667\n",
      "SGD (subgradient) (389/499): loss=3.491686757695291, w0=72.80000000000014, w1=12.798144011088565\n",
      "SGD (subgradient) (390/499): loss=2.888168833231319, w0=73.50000000000014, w1=12.141409434977806\n",
      "SGD (subgradient) (391/499): loss=3.4329987091866556, w0=72.80000000000014, w1=12.61522034514153\n",
      "SGD (subgradient) (392/499): loss=4.810287439763478, w0=72.10000000000014, w1=13.160343177443853\n",
      "SGD (subgradient) (393/499): loss=4.0790680093178935, w0=71.40000000000013, w1=12.986376505225289\n",
      "SGD (subgradient) (394/499): loss=0.3436172328177918, w0=70.70000000000013, w1=12.899988169514618\n",
      "SGD (subgradient) (395/499): loss=3.638909405400895, w0=70.00000000000013, w1=12.996192003181042\n",
      "SGD (subgradient) (396/499): loss=3.426729529980946, w0=69.30000000000013, w1=12.437205045335787\n",
      "SGD (subgradient) (397/499): loss=8.896857637010548, w0=70.00000000000013, w1=11.465038191869583\n",
      "SGD (subgradient) (398/499): loss=7.62888233569366, w0=70.70000000000013, w1=11.830068065436338\n",
      "SGD (subgradient) (399/499): loss=8.251119405370389, w0=71.40000000000013, w1=11.449610840020323\n",
      "SGD (subgradient) (400/499): loss=1.5636768617262717, w0=70.70000000000013, w1=11.990659551224066\n",
      "SGD (subgradient) (401/499): loss=5.023234649456853, w0=71.40000000000013, w1=11.54614969462265\n",
      "SGD (subgradient) (402/499): loss=4.530941198005522, w0=72.10000000000014, w1=11.821403349238919\n",
      "SGD (subgradient) (403/499): loss=5.597523313565716, w0=72.80000000000014, w1=11.730760940276065\n",
      "SGD (subgradient) (404/499): loss=8.761798111257505, w0=73.50000000000014, w1=11.379064286524237\n",
      "SGD (subgradient) (405/499): loss=5.681053814737922, w0=74.20000000000014, w1=11.963190387613265\n",
      "SGD (subgradient) (406/499): loss=4.975718196256523, w0=73.50000000000014, w1=12.408924356356591\n",
      "SGD (subgradient) (407/499): loss=2.720126729603763, w0=74.20000000000014, w1=12.569996054692133\n",
      "SGD (subgradient) (408/499): loss=0.47570897211421936, w0=73.50000000000014, w1=12.103834854087498\n",
      "SGD (subgradient) (409/499): loss=5.014082089696274, w0=72.80000000000014, w1=11.753762641932557\n",
      "SGD (subgradient) (410/499): loss=7.745983031448645, w0=73.50000000000014, w1=11.10705978263567\n",
      "SGD (subgradient) (411/499): loss=15.079727145466151, w0=74.20000000000014, w1=11.005599057225648\n",
      "SGD (subgradient) (412/499): loss=5.1210507977693, w0=73.50000000000014, w1=9.668458644489784\n",
      "SGD (subgradient) (413/499): loss=10.668157792412117, w0=74.20000000000014, w1=9.786205184487342\n",
      "SGD (subgradient) (414/499): loss=6.57506927264356, w0=74.90000000000015, w1=9.699804669768845\n",
      "SGD (subgradient) (415/499): loss=2.463693930491317, w0=74.20000000000014, w1=9.572016484659223\n",
      "SGD (subgradient) (416/499): loss=2.2572471021520144, w0=73.50000000000014, w1=10.30998654833734\n",
      "SGD (subgradient) (417/499): loss=13.459229199809357, w0=74.20000000000014, w1=10.377372752583879\n",
      "SGD (subgradient) (418/499): loss=13.900477739652658, w0=74.90000000000015, w1=10.586938211135008\n",
      "SGD (subgradient) (419/499): loss=0.18128241357041475, w0=74.20000000000014, w1=10.182694946560833\n",
      "SGD (subgradient) (420/499): loss=2.974526283347835, w0=73.50000000000014, w1=11.130729135845666\n",
      "SGD (subgradient) (421/499): loss=1.193769726215919, w0=74.20000000000014, w1=11.0252969183956\n",
      "SGD (subgradient) (422/499): loss=3.120677030708009, w0=74.90000000000015, w1=11.207650180633594\n",
      "SGD (subgradient) (423/499): loss=2.730992821020962, w0=74.20000000000014, w1=10.881176026364676\n",
      "SGD (subgradient) (424/499): loss=6.0798198988484415, w0=74.90000000000015, w1=10.129063562062173\n",
      "SGD (subgradient) (425/499): loss=10.284717577458238, w0=75.60000000000015, w1=11.967666833653418\n",
      "SGD (subgradient) (426/499): loss=3.716071625699044, w0=74.90000000000015, w1=11.92032671517303\n",
      "SGD (subgradient) (427/499): loss=2.6814102322521904, w0=74.20000000000014, w1=12.159572421866107\n",
      "SGD (subgradient) (428/499): loss=0.618644129531063, w0=74.90000000000015, w1=11.608364676660983\n",
      "SGD (subgradient) (429/499): loss=3.4749669153632397, w0=74.20000000000014, w1=11.766650365942136\n",
      "SGD (subgradient) (430/499): loss=5.576559115532646, w0=73.50000000000014, w1=11.016737818023216\n",
      "SGD (subgradient) (431/499): loss=3.0083598805020344, w0=74.20000000000014, w1=10.258982384153034\n",
      "SGD (subgradient) (432/499): loss=11.109319188294684, w0=73.50000000000014, w1=10.44159815302425\n",
      "SGD (subgradient) (433/499): loss=6.901241939024331, w0=74.20000000000014, w1=11.283195850793772\n",
      "SGD (subgradient) (434/499): loss=5.123259805941046, w0=73.50000000000014, w1=11.005615290909766\n",
      "SGD (subgradient) (435/499): loss=4.200745534723879, w0=74.20000000000014, w1=11.824590717991853\n",
      "SGD (subgradient) (436/499): loss=12.407763865842128, w0=74.90000000000015, w1=11.636554263066042\n",
      "SGD (subgradient) (437/499): loss=1.9277675426758378, w0=75.60000000000015, w1=12.257457464011914\n",
      "SGD (subgradient) (438/499): loss=6.187285136618826, w0=74.90000000000015, w1=10.999590546739762\n",
      "SGD (subgradient) (439/499): loss=7.673073555125811, w0=75.60000000000015, w1=12.134946713261021\n",
      "SGD (subgradient) (440/499): loss=0.27934877495052035, w0=76.30000000000015, w1=11.491475904596786\n",
      "SGD (subgradient) (441/499): loss=7.807619625332499, w0=75.60000000000015, w1=12.647871663043219\n",
      "SGD (subgradient) (442/499): loss=1.0259440810356892, w0=76.30000000000015, w1=13.025747817689522\n",
      "SGD (subgradient) (443/499): loss=4.170360453760864, w0=77.00000000000016, w1=12.393843375679086\n",
      "SGD (subgradient) (444/499): loss=13.462228020209693, w0=76.30000000000015, w1=12.20384391537728\n",
      "SGD (subgradient) (445/499): loss=10.49415411959123, w0=77.00000000000016, w1=12.1093242023556\n",
      "SGD (subgradient) (446/499): loss=12.451173943036515, w0=76.30000000000015, w1=12.225478495397612\n",
      "SGD (subgradient) (447/499): loss=1.8358087143597572, w0=77.00000000000016, w1=12.821499659816265\n",
      "SGD (subgradient) (448/499): loss=3.7063216008540962, w0=76.30000000000015, w1=13.401930590911556\n",
      "SGD (subgradient) (449/499): loss=11.323612094749485, w0=75.60000000000015, w1=13.634467438288754\n",
      "SGD (subgradient) (450/499): loss=1.2258177488556896, w0=76.30000000000015, w1=13.922931879390815\n",
      "SGD (subgradient) (451/499): loss=0.9406110181954972, w0=75.60000000000015, w1=15.359667234152317\n",
      "SGD (subgradient) (452/499): loss=7.549211035353679, w0=74.90000000000015, w1=15.151559172202987\n",
      "SGD (subgradient) (453/499): loss=1.4982105181134102, w0=75.60000000000015, w1=14.19017920994459\n",
      "SGD (subgradient) (454/499): loss=5.7350416058825715, w0=74.90000000000015, w1=13.566779055081282\n",
      "SGD (subgradient) (455/499): loss=3.2461007490880007, w0=74.20000000000014, w1=14.566920994464839\n",
      "SGD (subgradient) (456/499): loss=10.333404034802797, w0=74.90000000000015, w1=13.317241221111177\n",
      "SGD (subgradient) (457/499): loss=0.04610402816943804, w0=74.20000000000014, w1=12.462732500829729\n",
      "SGD (subgradient) (458/499): loss=8.095717061769747, w0=74.90000000000015, w1=12.166574050563735\n",
      "SGD (subgradient) (459/499): loss=6.780066106065405, w0=74.20000000000014, w1=12.080841552527522\n",
      "SGD (subgradient) (460/499): loss=9.899850054744178, w0=74.90000000000015, w1=13.019935370727168\n",
      "SGD (subgradient) (461/499): loss=10.357342366592462, w0=74.20000000000014, w1=12.267625980950799\n",
      "SGD (subgradient) (462/499): loss=7.171959492939088, w0=73.50000000000014, w1=12.793565220275864\n",
      "SGD (subgradient) (463/499): loss=3.075476708866404, w0=74.20000000000014, w1=13.302261485919587\n",
      "SGD (subgradient) (464/499): loss=3.174697409402519, w0=74.90000000000015, w1=12.443318251102896\n",
      "SGD (subgradient) (465/499): loss=3.14906053571616, w0=74.20000000000014, w1=11.96339378188409\n",
      "SGD (subgradient) (466/499): loss=13.441787618012455, w0=74.90000000000015, w1=12.318893086090934\n",
      "SGD (subgradient) (467/499): loss=0.7688698601737656, w0=74.20000000000014, w1=12.546588726030908\n",
      "SGD (subgradient) (468/499): loss=4.954024985906273, w0=73.50000000000014, w1=11.087094014954802\n",
      "SGD (subgradient) (469/499): loss=7.435170968165764, w0=74.20000000000014, w1=11.716575144166791\n",
      "SGD (subgradient) (470/499): loss=6.222270642775065, w0=73.50000000000014, w1=12.728145593683763\n",
      "SGD (subgradient) (471/499): loss=3.004595152621519, w0=74.20000000000014, w1=13.743435191860524\n",
      "SGD (subgradient) (472/499): loss=9.995898866649341, w0=73.50000000000014, w1=13.549112839391164\n",
      "SGD (subgradient) (473/499): loss=4.625453480445046, w0=72.80000000000014, w1=13.107740974572051\n",
      "SGD (subgradient) (474/499): loss=0.05854730423361332, w0=72.10000000000014, w1=13.973096066544445\n",
      "SGD (subgradient) (475/499): loss=4.452751928845345, w0=71.40000000000013, w1=13.488487419419736\n",
      "SGD (subgradient) (476/499): loss=2.0812449011991063, w0=72.10000000000014, w1=12.503447453820636\n",
      "SGD (subgradient) (477/499): loss=0.050711281484403514, w0=72.80000000000014, w1=11.167740587560857\n",
      "SGD (subgradient) (478/499): loss=7.651800913734675, w0=72.10000000000014, w1=11.366647079174125\n",
      "SGD (subgradient) (479/499): loss=8.353933736216533, w0=72.80000000000014, w1=10.682077913683731\n",
      "SGD (subgradient) (480/499): loss=11.706725480384257, w0=72.10000000000014, w1=11.371915175755573\n",
      "SGD (subgradient) (481/499): loss=1.899721094761425, w0=71.40000000000013, w1=12.284627861319143\n",
      "SGD (subgradient) (482/499): loss=3.02787127932622, w0=72.10000000000014, w1=13.185267709908317\n",
      "SGD (subgradient) (483/499): loss=7.091627264919055, w0=72.80000000000014, w1=13.939548028840676\n",
      "SGD (subgradient) (484/499): loss=3.5176368893174867, w0=72.10000000000014, w1=14.986044674472824\n",
      "SGD (subgradient) (485/499): loss=13.972191996560582, w0=72.80000000000014, w1=14.714810248148414\n",
      "SGD (subgradient) (486/499): loss=1.829972247034135, w0=72.10000000000014, w1=13.756071546863588\n",
      "SGD (subgradient) (487/499): loss=4.939439459928295, w0=72.80000000000014, w1=13.549207644439603\n",
      "SGD (subgradient) (488/499): loss=3.6594870788765803, w0=73.50000000000014, w1=14.50162510482556\n",
      "SGD (subgradient) (489/499): loss=5.205007533855564, w0=74.20000000000014, w1=15.005035427172917\n",
      "SGD (subgradient) (490/499): loss=5.598020825785973, w0=73.50000000000014, w1=15.75284930947248\n",
      "SGD (subgradient) (491/499): loss=3.360886836200905, w0=74.20000000000014, w1=16.15107429414468\n",
      "SGD (subgradient) (492/499): loss=7.288510473355316, w0=74.90000000000015, w1=16.301090166435905\n",
      "SGD (subgradient) (493/499): loss=4.678510767756848, w0=75.60000000000015, w1=15.548523320690164\n",
      "SGD (subgradient) (494/499): loss=0.4083726022255476, w0=76.30000000000015, w1=14.974949136135653\n",
      "SGD (subgradient) (495/499): loss=2.7086877650360606, w0=75.60000000000015, w1=14.209100179385139\n",
      "SGD (subgradient) (496/499): loss=6.6633059347158365, w0=74.90000000000015, w1=13.928355566481532\n",
      "SGD (subgradient) (497/499): loss=1.0853812734433035, w0=75.60000000000015, w1=12.661677977980169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (subgradient) (498/499): loss=0.7299566959797517, w0=76.30000000000015, w1=14.40457621579537\n",
      "SGD (subgradient) (499/499): loss=5.520274361283029, w0=75.60000000000015, w1=14.304639322929935\n",
      "SGD (subgradient) : execution time=0.403 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD (subgradient) : execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4068ce503e64b839f6f3e978e318f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
