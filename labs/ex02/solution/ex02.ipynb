{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "### TEMPLATE\n",
    "### END SOLUTION\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute loss by MSE\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    ### SOLUTION\n",
    "    # compute loss for each combinationof w0 and w1.\n",
    "    for ind_row, row in enumerate(grid_w0):\n",
    "        for ind_col, col in enumerate(grid_w1):\n",
    "            w = np.array([row, col])\n",
    "            losses[ind_row, ind_col] = compute_loss(y, tx, w)\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute loss for each combination of w0 and w1.\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.015 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "    \n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute gradient vector\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # update w by gradient descent\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: compute gradient and loss\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: update w by gradient\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.30574540147368, w1=9.435798704492347\n",
      "GD iter. 1/49: loss=265.3024621089587, w0=66.69746902191575, w1=12.266538315840018\n",
      "GD iter. 2/49: loss=37.87837955044096, w0=71.31498610804836, w1=13.115760199244335\n",
      "GD iter. 3/49: loss=17.41021212017444, w0=72.70024123388815, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450448, w0=73.11581777164008, w1=13.446956733772025\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543452, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613667, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370963, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.2939220020362, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.012 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acda657d3e34d39920366d53dc1d6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: implement stochastic gradient descent.\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2382.1923426902736, w0=5.98196950413687, w1=-0.7580528585584875\n",
      "SGD iter. 1/49: loss=1828.7350117433746, w0=13.739118465918366, w1=4.53971111981686\n",
      "SGD iter. 2/49: loss=1496.9744642972573, w0=19.3167026262254, w1=6.434363522053372\n",
      "SGD iter. 3/49: loss=1178.256706922229, w0=26.728591637717983, w1=26.026092238034884\n",
      "SGD iter. 4/49: loss=841.0833424876338, w0=32.852558758353126, w1=17.466070199227975\n",
      "SGD iter. 5/49: loss=749.2796639205893, w0=35.70133767335279, w1=20.867889178215954\n",
      "SGD iter. 6/49: loss=531.1771118178656, w0=41.313595535962556, w1=10.506302436853984\n",
      "SGD iter. 7/49: loss=431.81303085429835, w0=44.498651112565135, w1=15.399780238204546\n",
      "SGD iter. 8/49: loss=374.2549705624242, w0=47.24183339098254, w1=19.726859334606736\n",
      "SGD iter. 9/49: loss=267.3111877880608, w0=50.96131964562683, w1=15.739241550951029\n",
      "SGD iter. 10/49: loss=230.07062390443002, w0=52.59208757137665, w1=14.376107032197044\n",
      "SGD iter. 11/49: loss=191.75397296072055, w0=54.654633551284114, w1=15.784727886319097\n",
      "SGD iter. 12/49: loss=170.81362594619856, w0=56.01793309071532, w1=17.00046282209012\n",
      "SGD iter. 13/49: loss=127.75683234301606, w0=58.62230483076636, w1=16.55957258214651\n",
      "SGD iter. 14/49: loss=96.83099310450724, w0=60.582018632158565, w1=14.618888972886175\n",
      "SGD iter. 15/49: loss=81.45122251572171, w0=61.825785101580365, w1=14.262339628872247\n",
      "SGD iter. 16/49: loss=65.19267063398746, w0=63.36509481001321, w1=14.495564845475446\n",
      "SGD iter. 17/49: loss=65.89344924994349, w0=63.2968143392549, w1=14.515550810223418\n",
      "SGD iter. 18/49: loss=50.25550746545339, w0=65.2165940253875, w1=11.359332292235957\n",
      "SGD iter. 19/49: loss=36.57404743601873, w0=67.13489336042817, w1=11.371944581220335\n",
      "SGD iter. 20/49: loss=38.354406496265604, w0=66.8335396575567, w1=11.430200976831621\n",
      "SGD iter. 21/49: loss=33.148840214852314, w0=67.36468991961941, w1=12.871344450897881\n",
      "SGD iter. 22/49: loss=25.718195606342046, w0=68.78418086068525, w1=14.05142067562381\n",
      "SGD iter. 23/49: loss=22.2201808965477, w0=69.60574113980304, w1=13.22298693492175\n",
      "SGD iter. 24/49: loss=18.568753910725988, w0=70.9198584978398, w1=12.625572773394818\n",
      "SGD iter. 25/49: loss=18.1835932812984, w0=71.02521488080077, w1=12.81010148624296\n",
      "SGD iter. 26/49: loss=16.651363243828172, w0=71.71509334272253, w1=13.284134281021181\n",
      "SGD iter. 27/49: loss=16.473323167588454, w0=71.81917787125035, w1=13.47912419413401\n",
      "SGD iter. 28/49: loss=15.824258514419977, w0=72.95803072807813, w1=14.353736658459788\n",
      "SGD iter. 29/49: loss=16.30710304967142, w0=72.72622120164178, w1=14.712654511015833\n",
      "SGD iter. 30/49: loss=16.432672530277515, w0=72.86227582445103, w1=14.860744983560146\n",
      "SGD iter. 31/49: loss=15.993982837146998, w0=73.61032665800286, w1=14.536158377932402\n",
      "SGD iter. 32/49: loss=16.198378618275548, w0=74.5678402919752, w1=13.52568731537999\n",
      "SGD iter. 33/49: loss=16.245273135691072, w0=74.60150795462133, w1=13.574525464187627\n",
      "SGD iter. 34/49: loss=16.194305546550073, w0=74.56437520299745, w1=13.53247624629892\n",
      "SGD iter. 35/49: loss=18.73283021825412, w0=75.88024336597056, w1=13.549185463302349\n",
      "SGD iter. 36/49: loss=17.553817181126607, w0=75.19482603756285, w1=12.629758040580029\n",
      "SGD iter. 37/49: loss=16.41677394692107, w0=74.71359949182632, w1=13.264566016560895\n",
      "SGD iter. 38/49: loss=18.160243927267338, w0=75.60414510214684, w1=13.939692157270153\n",
      "SGD iter. 39/49: loss=18.14394649559831, w0=75.5981670769599, w1=13.934214237565948\n",
      "SGD iter. 40/49: loss=16.345120934408587, w0=74.31395603776075, w1=14.416727214628416\n",
      "SGD iter. 41/49: loss=17.11706178724607, w0=74.61283977812657, w1=14.792268619282945\n",
      "SGD iter. 42/49: loss=17.22890016505535, w0=74.68043254684241, w1=14.807723391940987\n",
      "SGD iter. 43/49: loss=15.920822121809461, w0=73.40419448321744, w1=14.508161989352056\n",
      "SGD iter. 44/49: loss=15.919318875289653, w0=73.40679999208689, w1=14.506416176239755\n",
      "SGD iter. 45/49: loss=16.281337807204647, w0=72.56571272769222, w1=14.602481836696017\n",
      "SGD iter. 46/49: loss=16.106637567409958, w0=72.72221818049458, w1=14.535484255751914\n",
      "SGD iter. 47/49: loss=17.10118467948676, w0=73.66284809558795, w1=15.294784653624514\n",
      "SGD iter. 48/49: loss=17.584260411023863, w0=73.87266916082993, w1=15.495102424707408\n",
      "SGD iter. 49/49: loss=19.40335839459817, w0=72.21471082329153, w1=16.10082751889154\n",
      "SGD: execution time=0.030 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48782fc7ecd4ce68a7820714bc5e262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "### SOLUTION\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "### TEMPLATE\n",
    "## ***************************************************\n",
    "## INSERT YOUR CODE HERE\n",
    "## TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "## ***************************************************\n",
    "#raise NotImplementedError\n",
    "### END SOLUTION\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.282124701595, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260339, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260336, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989095\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "### SOLUTION\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "### TEMPLATE\n",
    "# # ***************************************************\n",
    "# # INSERT YOUR CODE HERE\n",
    "# # TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "# #       and the model fit\n",
    "# # ***************************************************\n",
    "# raise NotImplementedError\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401d2c3a56d547d7971f947ef322ecb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -np.dot(tx.T,np.sign(err)) / len(err)\n",
    "    return grad, err\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute subgradient gradient vector for MAE\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_subgradient_mae(y, tx, w)\n",
    "        loss = calculate_mae(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        \n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: compute subgradient and loss\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: update w by subgradient\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=27.173270209668917, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=26.4904515637512, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=23.899295346035586, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=23.28439292565714, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=22.68687644418184, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=21.53781882800843, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=19.91191015895784, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=19.389644090563227, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=18.887989064395878, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=18.41596050185423, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=17.954898543040382, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=17.505757656579817, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=17.074957426931608, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=16.652967297509893, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=16.248540731496718, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=15.849105212654152, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=15.466919791231321, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=15.108294621512211, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=14.754896345922827, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=14.404528961620272, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=14.05578702812727, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=13.714620911605627, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=13.381236307284146, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=13.058821615166227, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=12.740251724339231, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=12.423218888756102, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=12.107561731901159, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=11.800622097398126, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=11.495041794646415, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=11.189461491894704, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=10.883881189142992, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=10.58459340831319, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=10.295816534318933, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=10.01135208122135, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=9.728084326668117, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=9.448125461122496, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=9.171041104096656, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=8.903656131158947, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=8.63627115822124, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=8.376151920302359, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=8.140540838751482, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=7.918544501597259, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=7.7052797283769845, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=7.493695831178626, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=7.2899924057434, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=7.097234035781528, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=6.919905294668907, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=6.7505735273154395, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=6.584744810805652, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=6.4303432763477915, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=6.27807148189034, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=6.133663329263311, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=6.005840798343018, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=5.885021825223206, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=5.771635252269647, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=5.667162061790248, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=5.586726765993136, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=5.523847812160378, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=5.480093708591866, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=5.453088003502018, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=5.4273926308629, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=5.407322445682747, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=5.387252260502595, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=5.370460780338691, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=5.335714659517469, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=5.330043910465359, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=5.322176726526588, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=5.317240048565146, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=5.315557122666141, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=5.313876880922164, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=5.3123778390243865, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=5.312132229725042, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=5.311886620425695, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=5.311505047641535, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=5.3114827328344205, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=5.311393473605971, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=5.311304214377522, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=5.31128189957041, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=5.311125695920622, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=5.311036436692172, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=5.310859611715928, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=5.310837296908815, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=5.310823570190172, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=5.310727416353907, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=5.310684480220336, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=5.3106135738747895, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=5.310574966149885, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=5.310576651555034, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=5.310574635520698, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=5.310626930749844, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=5.310626494042512, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=5.310623394482, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=5.310577014443433, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=5.310624310505836, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=5.3105760279022824, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=5.310579393366167, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=5.310576945786756, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=5.310579062736981, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=5.310625226529675, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=5.310622563676497, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=5.310576054815084, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=5.310577740220233, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=5.310623916407669, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=5.310626142553513, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=5.310584288862545, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=5.310580527767431, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=5.310577111221071, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=5.310577684556792, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=5.310575095186737, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=5.310580438210212, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=5.310575788704324, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=5.310625791064515, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=5.310584109748109, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=5.31057648222191, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=5.310578167627058, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=5.3105761515927234, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=5.310577836997869, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=5.310626707088355, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=5.310624044235176, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=5.310626270381018, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=5.310625833673685, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=5.310578861144642, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=5.310575159705164, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=5.31057684511031, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=5.310578423326829, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=5.310578530515457, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=5.310576514481121, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=5.310579341211301, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=5.310577869257082, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=5.310579554662228, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=5.310578893403855, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=5.310575191964375, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.024 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cf6103baae413a8030feb173d087ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        ### SOLUTION\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic subgradient and loss\n",
    "            grad, err = compute_subgradient_mae(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic subgradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = calculate_mae(err)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: implement stochastic subgradient descent.\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=113.47561257055952, w0=0.7, w1=1.239316300978001\n",
      "SubSGD iter. 1/499: loss=86.90328827780351, w0=1.4, w1=1.4836290246871353\n",
      "SubSGD iter. 2/499: loss=72.91204801413195, w0=2.0999999999999996, w1=1.4596631848840445\n",
      "SubSGD iter. 3/499: loss=47.775558408903585, w0=2.8, w1=0.7790640072279618\n",
      "SubSGD iter. 4/499: loss=66.59107058192123, w0=3.5, w1=0.25443108251622226\n",
      "SubSGD iter. 5/499: loss=57.8916499072674, w0=4.2, w1=-0.02811673657000291\n",
      "SubSGD iter. 6/499: loss=80.14237651298956, w0=4.9, w1=0.4999390789406458\n",
      "SubSGD iter. 7/499: loss=50.66140782945712, w0=5.6000000000000005, w1=-0.008562038799187599\n",
      "SubSGD iter. 8/499: loss=83.87658020169829, w0=6.300000000000001, w1=0.5156185126525306\n",
      "SubSGD iter. 9/499: loss=52.36411899371199, w0=7.000000000000001, w1=-0.011338523116317334\n",
      "SubSGD iter. 10/499: loss=86.76081304462494, w0=7.700000000000001, w1=0.6016539740672424\n",
      "SubSGD iter. 11/499: loss=81.73151045025607, w0=8.4, w1=1.233928402548933\n",
      "SubSGD iter. 12/499: loss=48.41830377189213, w0=9.1, w1=0.6984087197227276\n",
      "SubSGD iter. 13/499: loss=73.95574112821993, w0=9.799999999999999, w1=1.2907266809909057\n",
      "SubSGD iter. 14/499: loss=73.46385098755688, w0=10.499999999999998, w1=1.9218920209857582\n",
      "SubSGD iter. 15/499: loss=66.50492419168556, w0=11.199999999999998, w1=2.0876982805532553\n",
      "SubSGD iter. 16/499: loss=69.91780835824298, w0=11.899999999999997, w1=2.5363573193379656\n",
      "SubSGD iter. 17/499: loss=54.97557655293258, w0=12.599999999999996, w1=1.9520447817920743\n",
      "SubSGD iter. 18/499: loss=67.94461644040676, w0=13.299999999999995, w1=2.2221832320817443\n",
      "SubSGD iter. 19/499: loss=48.00413316998153, w0=13.999999999999995, w1=2.004926227901994\n",
      "SubSGD iter. 20/499: loss=77.9951628093251, w0=14.699999999999994, w1=2.617918725085554\n",
      "SubSGD iter. 21/499: loss=68.66238015098207, w0=15.399999999999993, w1=3.1271045690318062\n",
      "SubSGD iter. 22/499: loss=59.717821284635306, w0=16.099999999999994, w1=2.7449785844814714\n",
      "SubSGD iter. 23/499: loss=42.733225358845736, w0=16.799999999999994, w1=1.9451478504187618\n",
      "SubSGD iter. 24/499: loss=56.284747153240204, w0=17.499999999999993, w1=1.9104569910772906\n",
      "SubSGD iter. 25/499: loss=87.11935647464284, w0=18.199999999999992, w1=3.4505193294644765\n",
      "SubSGD iter. 26/499: loss=74.74131214316577, w0=18.89999999999999, w1=4.4629576673308975\n",
      "SubSGD iter. 27/499: loss=42.268260148564394, w0=19.59999999999999, w1=3.806076699577934\n",
      "SubSGD iter. 28/499: loss=49.25523264966933, w0=20.29999999999999, w1=4.066788920959798\n",
      "SubSGD iter. 29/499: loss=44.439090166993516, w0=20.99999999999999, w1=3.4630228509932843\n",
      "SubSGD iter. 30/499: loss=59.47821918542657, w0=21.69999999999999, w1=3.8415626343500158\n",
      "SubSGD iter. 31/499: loss=58.69610324441404, w0=22.399999999999988, w1=4.433880595618194\n",
      "SubSGD iter. 32/499: loss=37.6499067457161, w0=23.099999999999987, w1=3.64964786757887\n",
      "SubSGD iter. 33/499: loss=38.455058641659605, w0=23.799999999999986, w1=3.0285962940082767\n",
      "SubSGD iter. 34/499: loss=28.13060553809388, w0=24.499999999999986, w1=1.9388012709350564\n",
      "SubSGD iter. 35/499: loss=32.85755133605062, w0=25.199999999999985, w1=1.403281588108851\n",
      "SubSGD iter. 36/499: loss=23.405243350029593, w0=25.899999999999984, w1=0.27734917460768016\n",
      "SubSGD iter. 37/499: loss=44.09541668289721, w0=26.599999999999984, w1=-0.05901378306839655\n",
      "SubSGD iter. 38/499: loss=38.97029788748411, w0=27.299999999999983, w1=-0.7457084185745332\n",
      "SubSGD iter. 39/499: loss=55.45417950398215, w0=27.999999999999982, w1=-0.36716863521780174\n",
      "SubSGD iter. 40/499: loss=54.09967250212482, w0=28.69999999999998, w1=-0.21427487810322018\n",
      "SubSGD iter. 41/499: loss=44.27772968162031, w0=29.39999999999998, w1=-0.24896573744469128\n",
      "SubSGD iter. 42/499: loss=33.10928233929561, w0=30.09999999999998, w1=-0.19265612239938779\n",
      "SubSGD iter. 43/499: loss=25.376638895970594, w0=30.79999999999998, w1=-0.9924868564620973\n",
      "SubSGD iter. 44/499: loss=46.89524170687437, w0=31.49999999999998, w1=-0.8266805968946003\n",
      "SubSGD iter. 45/499: loss=45.107449480347796, w0=32.19999999999998, w1=-0.2782860931367894\n",
      "SubSGD iter. 46/499: loss=24.41051074924303, w0=32.899999999999984, w1=-1.31580028695033\n",
      "SubSGD iter. 47/499: loss=78.81740980012556, w0=33.59999999999999, w1=0.22426205143685585\n",
      "SubSGD iter. 48/499: loss=31.60633053278226, w0=34.29999999999999, w1=0.10375749995781353\n",
      "SubSGD iter. 49/499: loss=39.96480547140267, w0=34.99999999999999, w1=0.07979166015472262\n",
      "SubSGD iter. 50/499: loss=47.307772902606224, w0=35.699999999999996, w1=0.45833144351145405\n",
      "SubSGD iter. 51/499: loss=31.075377334717494, w0=36.4, w1=0.377517138022743\n",
      "SubSGD iter. 52/499: loss=72.81288362327265, w0=37.1, w1=1.5026470981530502\n",
      "SubSGD iter. 53/499: loss=53.83869568747268, w0=37.800000000000004, w1=2.0436041501241924\n",
      "SubSGD iter. 54/499: loss=58.445266518239514, w0=38.50000000000001, w1=3.406355545515939\n",
      "SubSGD iter. 55/499: loss=42.008863227624836, w0=39.20000000000001, w1=3.7848953288726706\n",
      "SubSGD iter. 56/499: loss=30.169115724229464, w0=39.90000000000001, w1=3.983656194691195\n",
      "SubSGD iter. 57/499: loss=97.6717696132567, w0=40.600000000000016, w1=1.2133564978022058\n",
      "SubSGD iter. 58/499: loss=48.27899064511986, w0=41.30000000000002, w1=1.8456309262838966\n",
      "SubSGD iter. 59/499: loss=34.961711854242544, w0=42.00000000000002, w1=2.0959120427459528\n",
      "SubSGD iter. 60/499: loss=33.90921072275537, w0=42.700000000000024, w1=1.8549321280793047\n",
      "SubSGD iter. 61/499: loss=23.59476348801033, w0=43.40000000000003, w1=1.300825054082724\n",
      "SubSGD iter. 62/499: loss=18.273844803720486, w0=44.10000000000003, w1=0.8611366489975396\n",
      "SubSGD iter. 63/499: loss=26.401838051442887, w0=44.80000000000003, w1=0.4308050605368036\n",
      "SubSGD iter. 64/499: loss=64.3272324259388, w0=45.500000000000036, w1=1.5559350206671108\n",
      "SubSGD iter. 65/499: loss=29.24085903857864, w0=46.20000000000004, w1=2.1043295244249216\n",
      "SubSGD iter. 66/499: loss=29.712108499922074, w0=46.90000000000004, w1=1.8633496097582736\n",
      "SubSGD iter. 67/499: loss=42.772654075925225, w0=47.600000000000044, w1=3.031680410275742\n",
      "SubSGD iter. 68/499: loss=33.48140008417258, w0=48.30000000000005, w1=3.6239983715439203\n",
      "SubSGD iter. 69/499: loss=26.939329051564762, w0=49.00000000000005, w1=3.9592264268471755\n",
      "SubSGD iter. 70/499: loss=38.73814344299382, w0=49.70000000000005, w1=4.602156125610971\n",
      "SubSGD iter. 71/499: loss=40.8218302786604, w0=50.400000000000055, w1=5.481404091507144\n",
      "SubSGD iter. 72/499: loss=17.831263612681056, w0=51.10000000000006, w1=5.742116312889007\n",
      "SubSGD iter. 73/499: loss=18.49598281468362, w0=51.80000000000006, w1=5.537282793013785\n",
      "SubSGD iter. 74/499: loss=31.334198549667136, w0=52.500000000000064, w1=6.298639743632854\n",
      "SubSGD iter. 75/499: loss=7.273758928649485, w0=53.20000000000007, w1=5.79013862589302\n",
      "SubSGD iter. 76/499: loss=25.299476196820166, w0=53.90000000000007, w1=5.816511276517841\n",
      "SubSGD iter. 77/499: loss=24.094961484438713, w0=54.60000000000007, w1=6.160223897701232\n",
      "SubSGD iter. 78/499: loss=10.317499924922103, w0=55.300000000000075, w1=6.0097672212501045\n",
      "SubSGD iter. 79/499: loss=29.685234896142212, w0=56.00000000000008, w1=6.287894190051793\n",
      "SubSGD iter. 80/499: loss=4.093830446353834, w0=56.70000000000008, w1=5.553779867050803\n",
      "SubSGD iter. 81/499: loss=6.544436033885461, w0=57.400000000000084, w1=4.93272829348021\n",
      "SubSGD iter. 82/499: loss=10.793350675645414, w0=58.10000000000009, w1=4.55714245519364\n",
      "SubSGD iter. 83/499: loss=18.280722738086006, w0=58.80000000000009, w1=4.722948714761137\n",
      "SubSGD iter. 84/499: loss=30.320569296477387, w0=59.50000000000009, w1=5.162873157645319\n",
      "SubSGD iter. 85/499: loss=13.935152404525695, w0=60.200000000000095, w1=5.21503350317503\n",
      "SubSGD iter. 86/499: loss=0.5429680970792816, w0=60.9000000000001, w1=4.935104510738567\n",
      "SubSGD iter. 87/499: loss=0.16213590148907286, w0=61.6000000000001, w1=4.365144386202581\n",
      "SubSGD iter. 88/499: loss=4.05961351446647, w0=60.9000000000001, w1=4.796936964486001\n",
      "SubSGD iter. 89/499: loss=15.870105710510884, w0=61.6000000000001, w1=4.999373854527\n",
      "SubSGD iter. 90/499: loss=29.829865664912376, w0=62.300000000000104, w1=6.117684075138481\n",
      "SubSGD iter. 91/499: loss=22.149160765052216, w0=63.00000000000011, w1=6.749958503620172\n",
      "SubSGD iter. 92/499: loss=5.794761158981018, w0=62.300000000000104, w1=7.8758909171213425\n",
      "SubSGD iter. 93/499: loss=11.59655367288314, w0=63.00000000000011, w1=7.9861909505821345\n",
      "SubSGD iter. 94/499: loss=8.33545724333377, w0=63.70000000000011, w1=7.992618831460481\n",
      "SubSGD iter. 95/499: loss=8.22171315301371, w0=64.4000000000001, w1=8.034945388063973\n",
      "SubSGD iter. 96/499: loss=16.736059813135256, w0=65.10000000000011, w1=8.876135556288615\n",
      "SubSGD iter. 97/499: loss=11.036006145873003, w0=65.80000000000011, w1=9.468453517556792\n",
      "SubSGD iter. 98/499: loss=17.888843165925636, w0=66.50000000000011, w1=10.021659433632905\n",
      "SubSGD iter. 99/499: loss=14.199802778735972, w0=67.20000000000012, w1=10.854997016192481\n",
      "SubSGD iter. 100/499: loss=3.9875789162422137, w0=66.50000000000011, w1=11.402159318426351\n",
      "SubSGD iter. 101/499: loss=10.255249253744978, w0=67.20000000000012, w1=12.16351626904542\n",
      "SubSGD iter. 102/499: loss=0.8593262202099083, w0=66.50000000000011, w1=12.982174638851049\n",
      "SubSGD iter. 103/499: loss=9.522861526076802, w0=67.20000000000012, w1=12.726742757749479\n",
      "SubSGD iter. 104/499: loss=2.4084073059309787, w0=67.90000000000012, w1=12.105691184178886\n",
      "SubSGD iter. 105/499: loss=9.514533786668459, w0=68.60000000000012, w1=11.675359595718149\n",
      "SubSGD iter. 106/499: loss=7.858102210849012, w0=69.30000000000013, w1=12.436716546337218\n",
      "SubSGD iter. 107/499: loss=10.64421132317922, w0=70.00000000000013, w1=13.449154884203638\n",
      "SubSGD iter. 108/499: loss=12.377364676297006, w0=70.70000000000013, w1=14.112299201246685\n",
      "SubSGD iter. 109/499: loss=10.149825790666874, w0=71.40000000000013, w1=13.712841428860106\n",
      "SubSGD iter. 110/499: loss=12.722925159711764, w0=72.10000000000014, w1=14.06080847525052\n",
      "SubSGD iter. 111/499: loss=3.2819238237811987, w0=72.80000000000014, w1=14.939980787481327\n",
      "SubSGD iter. 112/499: loss=4.154385941799141, w0=73.50000000000014, w1=14.366970704466704\n",
      "SubSGD iter. 113/499: loss=4.758167483676623, w0=72.80000000000014, w1=14.936930829002689\n",
      "SubSGD iter. 114/499: loss=6.37077646601378, w0=73.50000000000014, w1=14.409742190255463\n",
      "SubSGD iter. 115/499: loss=1.9758680980570773, w0=74.20000000000014, w1=15.24307977281504\n",
      "SubSGD iter. 116/499: loss=8.765013883898618, w0=73.50000000000014, w1=15.268377538203882\n",
      "SubSGD iter. 117/499: loss=4.344853579498427, w0=72.80000000000014, w1=16.109796047929827\n",
      "SubSGD iter. 118/499: loss=9.460937241144151, w0=72.10000000000014, w1=15.561401544172016\n",
      "SubSGD iter. 119/499: loss=3.1926013757943963, w0=72.80000000000014, w1=15.831539994461686\n",
      "SubSGD iter. 120/499: loss=5.632503778610086, w0=73.50000000000014, w1=16.844218335814453\n",
      "SubSGD iter. 121/499: loss=4.600822427223193, w0=72.80000000000014, w1=15.964488205981976\n",
      "SubSGD iter. 122/499: loss=4.519346479111107, w0=73.50000000000014, w1=16.129940997914602\n",
      "SubSGD iter. 123/499: loss=4.529155215227689, w0=72.80000000000014, w1=16.553180422687117\n",
      "SubSGD iter. 124/499: loss=3.3709619447216284, w0=73.50000000000014, w1=17.19611012145091\n",
      "SubSGD iter. 125/499: loss=0.31568007945460863, w0=72.80000000000014, w1=16.99367323140991\n",
      "SubSGD iter. 126/499: loss=0.7054422116146242, w0=73.50000000000014, w1=17.788842956527258\n",
      "SubSGD iter. 127/499: loss=7.7287836009708215, w0=72.80000000000014, w1=18.06877194896372\n",
      "SubSGD iter. 128/499: loss=0.5937668723320613, w0=72.10000000000014, w1=18.508460354048907\n",
      "SubSGD iter. 129/499: loss=2.679127848314991, w0=72.80000000000014, w1=18.651790944643423\n",
      "SubSGD iter. 130/499: loss=1.107392256066106, w0=72.10000000000014, w1=18.308078323460034\n",
      "SubSGD iter. 131/499: loss=1.087025654572777, w0=71.40000000000013, w1=18.93289442327098\n",
      "SubSGD iter. 132/499: loss=5.385654560592535, w0=72.10000000000014, w1=18.031262158858222\n",
      "SubSGD iter. 133/499: loss=6.110140716605983, w0=72.80000000000014, w1=17.35291516288368\n",
      "SubSGD iter. 134/499: loss=4.191176312805524, w0=73.50000000000014, w1=17.518367954816306\n",
      "SubSGD iter. 135/499: loss=9.706253299547072, w0=74.20000000000014, w1=16.779035155372362\n",
      "SubSGD iter. 136/499: loss=2.817114948676604, w0=73.50000000000014, w1=17.204821302663103\n",
      "SubSGD iter. 137/499: loss=3.3833822771515685, w0=72.80000000000014, w1=16.36363113443846\n",
      "SubSGD iter. 138/499: loss=2.7151710812563437, w0=73.50000000000014, w1=16.18016535538804\n",
      "SubSGD iter. 139/499: loss=0.20453579158151314, w0=72.80000000000014, w1=17.123671838925162\n",
      "SubSGD iter. 140/499: loss=1.0369700685206595, w0=73.50000000000014, w1=17.088980979583692\n",
      "SubSGD iter. 141/499: loss=0.3205158992866046, w0=74.20000000000014, w1=15.999185956510471\n",
      "SubSGD iter. 142/499: loss=7.3700842013719665, w0=73.50000000000014, w1=15.820833835130657\n",
      "SubSGD iter. 143/499: loss=1.8294408429636206, w0=74.20000000000014, w1=15.964164425725173\n",
      "SubSGD iter. 144/499: loss=2.351964964536137, w0=74.90000000000015, w1=16.696129019232806\n",
      "SubSGD iter. 145/499: loss=2.24712698959263, w0=74.20000000000014, w1=16.352416398049417\n",
      "SubSGD iter. 146/499: loss=3.5303571442481285, w0=73.50000000000014, w1=17.192443969509227\n",
      "SubSGD iter. 147/499: loss=3.886736434489407, w0=72.80000000000014, w1=17.615683394281742\n",
      "SubSGD iter. 148/499: loss=2.733540238813333, w0=72.10000000000014, w1=16.075621055894555\n",
      "SubSGD iter. 149/499: loss=2.9969835712749955, w0=72.80000000000014, w1=15.47185498592804\n",
      "SubSGD iter. 150/499: loss=3.6296995049960543, w0=72.10000000000014, w1=15.116827508398215\n",
      "SubSGD iter. 151/499: loss=12.314648873972288, w0=71.40000000000013, w1=15.236719321886547\n",
      "SubSGD iter. 152/499: loss=1.9499931546771663, w0=70.70000000000013, w1=15.806679446422532\n",
      "SubSGD iter. 153/499: loss=3.2057288484863875, w0=70.00000000000013, w1=16.36480315017157\n",
      "SubSGD iter. 154/499: loss=2.7915369644746875, w0=70.70000000000013, w1=16.25462903329083\n",
      "SubSGD iter. 155/499: loss=7.949557152849067, w0=70.00000000000013, w1=15.296493942389022\n",
      "SubSGD iter. 156/499: loss=0.193718156147213, w0=69.30000000000013, w1=15.691930734332027\n",
      "SubSGD iter. 157/499: loss=10.166534714149023, w0=70.00000000000013, w1=15.957414436556258\n",
      "SubSGD iter. 158/499: loss=4.676727805794478, w0=69.30000000000013, w1=16.43546263898238\n",
      "SubSGD iter. 159/499: loss=0.9672228513181551, w0=68.60000000000012, w1=15.734528641547627\n",
      "SubSGD iter. 160/499: loss=0.9599675148094065, w0=67.90000000000012, w1=16.017076460633852\n",
      "SubSGD iter. 161/499: loss=3.548939998028814, w0=68.60000000000012, w1=16.30557372519127\n",
      "SubSGD iter. 162/499: loss=0.225651926613736, w0=69.30000000000013, w1=17.38944912188257\n",
      "SubSGD iter. 163/499: loss=4.042196060173808, w0=70.00000000000013, w1=17.370869662983694\n",
      "SubSGD iter. 164/499: loss=14.558002302108825, w0=70.70000000000013, w1=16.34393746539116\n",
      "SubSGD iter. 165/499: loss=8.550077025222564, w0=71.40000000000013, w1=15.490091194965892\n",
      "SubSGD iter. 166/499: loss=6.395874999500521, w0=70.70000000000013, w1=15.968139397392015\n",
      "SubSGD iter. 167/499: loss=4.18487318414266, w0=71.40000000000013, w1=16.05666691088114\n",
      "SubSGD iter. 168/499: loss=2.1146413722524358, w0=70.70000000000013, w1=15.721438855577887\n",
      "SubSGD iter. 169/499: loss=2.825840410111553, w0=70.00000000000013, w1=15.84194340705693\n",
      "SubSGD iter. 170/499: loss=12.058778869462579, w0=70.70000000000013, w1=15.459817422506594\n",
      "SubSGD iter. 171/499: loss=1.0826586040394375, w0=71.40000000000013, w1=16.205803076093993\n",
      "SubSGD iter. 172/499: loss=8.535116162832132, w0=72.10000000000014, w1=15.775471487633256\n",
      "SubSGD iter. 173/499: loss=2.8575266600237796, w0=71.40000000000013, w1=15.279845227164424\n",
      "SubSGD iter. 174/499: loss=6.34094029423494, w0=72.10000000000014, w1=15.998008551195612\n",
      "SubSGD iter. 175/499: loss=0.5354340894538439, w0=71.40000000000013, w1=15.378875155250945\n",
      "SubSGD iter. 176/499: loss=10.172602375159649, w0=72.10000000000014, w1=14.979417382864366\n",
      "SubSGD iter. 177/499: loss=4.792121190731692, w0=72.80000000000014, w1=15.595804331701615\n",
      "SubSGD iter. 178/499: loss=5.215962159382606, w0=72.10000000000014, w1=15.87835215078784\n",
      "SubSGD iter. 179/499: loss=10.888036320703996, w0=71.40000000000013, w1=15.822042535742536\n",
      "SubSGD iter. 180/499: loss=0.780636006802311, w0=70.70000000000013, w1=16.640700905548165\n",
      "SubSGD iter. 181/499: loss=2.6449417982132104, w0=71.40000000000013, w1=16.843137795589165\n",
      "SubSGD iter. 182/499: loss=7.400369005519622, w0=70.70000000000013, w1=16.5824255742073\n",
      "SubSGD iter. 183/499: loss=0.19006321448485153, w0=71.40000000000013, w1=15.84831125120631\n",
      "SubSGD iter. 184/499: loss=7.154853762309045, w0=72.10000000000014, w1=15.88559856282418\n",
      "SubSGD iter. 185/499: loss=5.0276309818418525, w0=71.40000000000013, w1=16.50838496964447\n",
      "SubSGD iter. 186/499: loss=0.9145664247282497, w0=70.70000000000013, w1=17.078345094180456\n",
      "SubSGD iter. 187/499: loss=9.587911337921696, w0=71.40000000000013, w1=17.19216542099477\n",
      "SubSGD iter. 188/499: loss=6.864368850289864, w0=72.10000000000014, w1=17.345059178109352\n",
      "SubSGD iter. 189/499: loss=1.241017858551146, w0=72.80000000000014, w1=17.326479719210475\n",
      "SubSGD iter. 190/499: loss=1.727672377864252, w0=72.10000000000014, w1=15.961110266546864\n",
      "SubSGD iter. 191/499: loss=1.619614467344789, w0=72.80000000000014, w1=16.339650049903597\n",
      "SubSGD iter. 192/499: loss=0.9981152262297996, w0=73.50000000000014, w1=16.304959190562126\n",
      "SubSGD iter. 193/499: loss=3.1275263474196393, w0=72.80000000000014, w1=16.661684708917964\n",
      "SubSGD iter. 194/499: loss=7.674535157450343, w0=72.10000000000014, w1=17.093477287201384\n",
      "SubSGD iter. 195/499: loss=0.11112755778880512, w0=71.40000000000013, w1=16.563776313139268\n",
      "SubSGD iter. 196/499: loss=3.195707732107195, w0=70.70000000000013, w1=15.862842315704516\n",
      "SubSGD iter. 197/499: loss=3.0727549370916023, w0=71.40000000000013, w1=16.241382099061248\n",
      "SubSGD iter. 198/499: loss=1.9779762006423596, w0=72.10000000000014, w1=15.297875615524127\n",
      "SubSGD iter. 199/499: loss=4.298756550491106, w0=71.40000000000013, w1=15.41838016700317\n",
      "SubSGD iter. 200/499: loss=0.880570600983134, w0=70.70000000000013, w1=14.81384806114901\n",
      "SubSGD iter. 201/499: loss=0.5222066292317749, w0=70.00000000000013, w1=13.619109224044584\n",
      "SubSGD iter. 202/499: loss=1.0416064027639607, w0=69.30000000000013, w1=12.961926520849598\n",
      "SubSGD iter. 203/499: loss=9.409002766475538, w0=70.00000000000013, w1=14.08023674146108\n",
      "SubSGD iter. 204/499: loss=3.3801874894745865, w0=69.30000000000013, w1=14.230693417912208\n",
      "SubSGD iter. 205/499: loss=7.7696757563433465, w0=70.00000000000013, w1=14.42721350069071\n",
      "SubSGD iter. 206/499: loss=4.069046990733838, w0=70.70000000000013, w1=15.152233690771514\n",
      "SubSGD iter. 207/499: loss=9.703941117900271, w0=71.40000000000013, w1=14.911253776104866\n",
      "SubSGD iter. 208/499: loss=1.388455758679882, w0=70.70000000000013, w1=13.71651493900044\n",
      "SubSGD iter. 209/499: loss=3.723353217722135, w0=70.00000000000013, w1=12.76960267725801\n",
      "SubSGD iter. 210/499: loss=2.7414746042556573, w0=70.70000000000013, w1=12.403681946527612\n",
      "SubSGD iter. 211/499: loss=2.315647986173964, w0=70.00000000000013, w1=12.76040746488345\n",
      "SubSGD iter. 212/499: loss=6.313448124112199, w0=70.70000000000013, w1=13.010005927570628\n",
      "SubSGD iter. 213/499: loss=4.310220877036805, w0=70.00000000000013, w1=13.160462604021756\n",
      "SubSGD iter. 214/499: loss=0.10940079639027545, w0=69.30000000000013, w1=13.510799537390147\n",
      "SubSGD iter. 215/499: loss=2.5064952134946665, w0=68.60000000000012, w1=13.857883523010138\n",
      "SubSGD iter. 216/499: loss=4.5225386607314135, w0=69.30000000000013, w1=12.881086044068219\n",
      "SubSGD iter. 217/499: loss=14.69347783995154, w0=70.00000000000013, w1=13.321010486952401\n",
      "SubSGD iter. 218/499: loss=6.766886788846428, w0=70.70000000000013, w1=12.28349629313886\n",
      "SubSGD iter. 219/499: loss=1.5121856449684685, w0=71.40000000000013, w1=12.415819819492699\n",
      "SubSGD iter. 220/499: loss=2.8706754747397127, w0=72.10000000000014, w1=13.610558656597124\n",
      "SubSGD iter. 221/499: loss=3.8474197868483486, w0=72.80000000000014, w1=14.489730968827931\n",
      "SubSGD iter. 222/499: loss=8.520613856206182, w0=73.50000000000014, w1=14.107604984277597\n",
      "SubSGD iter. 223/499: loss=3.3804264832500763, w0=74.20000000000014, w1=14.753752217664818\n",
      "SubSGD iter. 224/499: loss=8.28419229469165, w0=73.50000000000014, w1=15.262253335404651\n",
      "SubSGD iter. 225/499: loss=5.205712068492055, w0=72.80000000000014, w1=15.887069435215597\n",
      "SubSGD iter. 226/499: loss=13.140166431003152, w0=73.50000000000014, w1=16.231537682086064\n",
      "SubSGD iter. 227/499: loss=4.5116361180785205, w0=74.20000000000014, w1=15.65852759907144\n",
      "SubSGD iter. 228/499: loss=5.79909085342598, w0=74.90000000000015, w1=16.19948465104258\n",
      "SubSGD iter. 229/499: loss=3.474527376486037, w0=74.20000000000014, w1=16.15715809443909\n",
      "SubSGD iter. 230/499: loss=2.6165384152585744, w0=73.50000000000014, w1=15.395801143820021\n",
      "SubSGD iter. 231/499: loss=0.21175079239321093, w0=72.80000000000014, w1=16.195631877882732\n",
      "SubSGD iter. 232/499: loss=1.3903552311639658, w0=72.10000000000014, w1=16.545968811251125\n",
      "SubSGD iter. 233/499: loss=3.607916282635955, w0=71.40000000000013, w1=15.339952521480678\n",
      "SubSGD iter. 234/499: loss=11.576433356182193, w0=70.70000000000013, w1=15.45984433496901\n",
      "SubSGD iter. 235/499: loss=5.287331696361321, w0=70.00000000000013, w1=15.67710133914876\n",
      "SubSGD iter. 236/499: loss=8.713781235732597, w0=70.70000000000013, w1=14.323144445115531\n",
      "SubSGD iter. 237/499: loss=2.8535790884193943, w0=71.40000000000013, w1=13.957223714385133\n",
      "SubSGD iter. 238/499: loss=6.393785785228147, w0=72.10000000000014, w1=14.12267650631776\n",
      "SubSGD iter. 239/499: loss=7.692753535565991, w0=72.80000000000014, w1=13.095744308725227\n",
      "SubSGD iter. 240/499: loss=3.0874246875959415, w0=73.50000000000014, w1=13.239074899319743\n",
      "SubSGD iter. 241/499: loss=5.857180466266712, w0=72.80000000000014, w1=14.365007312820914\n",
      "SubSGD iter. 242/499: loss=2.0174498417570277, w0=73.50000000000014, w1=14.874193156767166\n",
      "SubSGD iter. 243/499: loss=3.3890177249894435, w0=74.20000000000014, w1=15.070713239545668\n",
      "SubSGD iter. 244/499: loss=1.9981256171578252, w0=74.90000000000015, w1=15.722770136988123\n",
      "SubSGD iter. 245/499: loss=10.739204960131516, w0=75.60000000000015, w1=16.962086437966125\n",
      "SubSGD iter. 246/499: loss=0.4710166937196192, w0=74.90000000000015, w1=17.08910088319415\n",
      "SubSGD iter. 247/499: loss=9.244634541762743, w0=75.60000000000015, w1=16.062168685601616\n",
      "SubSGD iter. 248/499: loss=2.9841705384602335, w0=74.90000000000015, w1=16.26700220547684\n",
      "SubSGD iter. 249/499: loss=7.045040023081903, w0=74.20000000000014, w1=16.549550024563064\n",
      "SubSGD iter. 250/499: loss=2.730227166957718, w0=74.90000000000015, w1=15.365547824169173\n",
      "SubSGD iter. 251/499: loss=6.6837670131051965, w0=75.60000000000015, w1=15.576712218735377\n",
      "SubSGD iter. 252/499: loss=11.921648740366308, w0=74.90000000000015, w1=14.61857712783357\n",
      "SubSGD iter. 253/499: loss=108.97978193960157, w0=75.60000000000015, w1=11.245433450243892\n",
      "SubSGD iter. 254/499: loss=1.9094569767800635, w0=74.90000000000015, w1=12.282947644057433\n",
      "SubSGD iter. 255/499: loss=0.08017536356513233, w0=74.20000000000014, w1=11.939235022874042\n",
      "SubSGD iter. 256/499: loss=7.762972338466135, w0=73.50000000000014, w1=12.362474447646557\n",
      "SubSGD iter. 257/499: loss=1.846180008758374, w0=74.20000000000014, w1=11.324960253833016\n",
      "SubSGD iter. 258/499: loss=1.4977662844883497, w0=74.90000000000015, w1=11.944093649777683\n",
      "SubSGD iter. 259/499: loss=10.485444121160484, w0=74.20000000000014, w1=12.502217353526722\n",
      "SubSGD iter. 260/499: loss=10.137807876472806, w0=73.50000000000014, w1=13.125003760347015\n",
      "SubSGD iter. 261/499: loss=8.787057614384537, w0=72.80000000000014, w1=12.51960060890303\n",
      "SubSGD iter. 262/499: loss=8.722342083394722, w0=72.10000000000014, w1=13.142387015723322\n",
      "SubSGD iter. 263/499: loss=11.348900239486426, w0=72.80000000000014, w1=13.386699739432457\n",
      "SubSGD iter. 264/499: loss=0.5452172111940854, w0=73.50000000000014, w1=14.47057513612376\n",
      "SubSGD iter. 265/499: loss=2.9405777831470203, w0=72.80000000000014, w1=15.127456103876723\n",
      "SubSGD iter. 266/499: loss=8.093068372608123, w0=72.10000000000014, w1=15.407385096313186\n",
      "SubSGD iter. 267/499: loss=3.2520381822854034, w0=72.80000000000014, w1=15.677523546602856\n",
      "SubSGD iter. 268/499: loss=4.930378784162599, w0=73.50000000000014, w1=16.201704098054574\n",
      "SubSGD iter. 269/499: loss=4.274192591413019, w0=72.80000000000014, w1=16.706721829878894\n",
      "SubSGD iter. 270/499: loss=2.1063138401361954, w0=73.50000000000014, w1=16.956320292566073\n",
      "SubSGD iter. 271/499: loss=5.539224820734972, w0=72.80000000000014, w1=17.13818413614124\n",
      "SubSGD iter. 272/499: loss=0.1512979751799861, w0=72.10000000000014, w1=17.569194820889855\n",
      "SubSGD iter. 273/499: loss=5.670104789296488, w0=71.40000000000013, w1=17.390842699510042\n",
      "SubSGD iter. 274/499: loss=7.560626781326221, w0=72.10000000000014, w1=16.81783261649542\n",
      "SubSGD iter. 275/499: loss=3.1791615140475926, w0=71.40000000000013, w1=16.482604561192165\n",
      "SubSGD iter. 276/499: loss=1.1916651169363348, w0=72.10000000000014, w1=16.534764906721875\n",
      "SubSGD iter. 277/499: loss=4.1369603555541445, w0=71.40000000000013, w1=16.8173127258081\n",
      "SubSGD iter. 278/499: loss=8.427490883249128, w0=72.10000000000014, w1=15.963466455382832\n",
      "SubSGD iter. 279/499: loss=7.686137444356646, w0=72.80000000000014, w1=15.533134866922095\n",
      "SubSGD iter. 280/499: loss=5.638555754150772, w0=72.10000000000014, w1=14.87595216372711\n",
      "SubSGD iter. 281/499: loss=3.4229129115410615, w0=72.80000000000014, w1=15.019282754321626\n",
      "SubSGD iter. 282/499: loss=8.308116142191423, w0=73.50000000000014, w1=13.992350556729093\n",
      "SubSGD iter. 283/499: loss=13.829373649601294, w0=74.20000000000014, w1=15.1174805168594\n",
      "SubSGD iter. 284/499: loss=10.936550078348041, w0=73.50000000000014, w1=15.388960698841979\n",
      "SubSGD iter. 285/499: loss=9.596219893209692, w0=72.80000000000014, w1=14.840566195084168\n",
      "SubSGD iter. 286/499: loss=3.9446369362594353, w0=72.10000000000014, w1=15.680593766543979\n",
      "SubSGD iter. 287/499: loss=5.2297932897471355, w0=72.80000000000014, w1=16.398757090575167\n",
      "SubSGD iter. 288/499: loss=0.24640693299376437, w0=72.10000000000014, w1=16.288457057114375\n",
      "SubSGD iter. 289/499: loss=3.4710029000258587, w0=71.40000000000013, w1=16.840428489052055\n",
      "SubSGD iter. 290/499: loss=7.102006070010447, w0=72.10000000000014, w1=16.877715800669925\n",
      "SubSGD iter. 291/499: loss=2.2029660953937054, w0=72.80000000000014, w1=17.32789554687622\n",
      "SubSGD iter. 292/499: loss=8.994016140789022, w0=73.50000000000014, w1=16.803262622164482\n",
      "SubSGD iter. 293/499: loss=2.1023782061851293, w0=72.80000000000014, w1=17.229048769455222\n",
      "SubSGD iter. 294/499: loss=11.5499445280292, w0=73.50000000000014, w1=16.20211657186269\n",
      "SubSGD iter. 295/499: loss=4.782825790552387, w0=74.20000000000014, w1=15.617804034316798\n",
      "SubSGD iter. 296/499: loss=1.3640989735490336, w0=73.50000000000014, w1=14.892783844235995\n",
      "SubSGD iter. 297/499: loss=8.773967418179367, w0=72.80000000000014, w1=14.63207162285413\n",
      "SubSGD iter. 298/499: loss=0.8869433857703797, w0=73.50000000000014, w1=13.44806942246024\n",
      "SubSGD iter. 299/499: loss=6.82504252110018, w0=72.80000000000014, w1=14.072885522271186\n",
      "SubSGD iter. 300/499: loss=5.649040877821321, w0=73.50000000000014, w1=14.715815221034982\n",
      "SubSGD iter. 301/499: loss=0.28808660283816323, w0=74.20000000000014, w1=15.245516195097098\n",
      "SubSGD iter. 302/499: loss=9.605341182102578, w0=73.50000000000014, w1=14.984803973715234\n",
      "SubSGD iter. 303/499: loss=5.7526516330296715, w0=72.80000000000014, w1=15.105308525194276\n",
      "SubSGD iter. 304/499: loss=0.944452497289241, w0=72.10000000000014, w1=16.006940789607036\n",
      "SubSGD iter. 305/499: loss=7.712863700505174, w0=72.80000000000014, w1=15.576609201146299\n",
      "SubSGD iter. 306/499: loss=8.284168583611475, w0=73.50000000000014, w1=15.772953794663527\n",
      "SubSGD iter. 307/499: loss=0.22407741492563105, w0=74.20000000000014, w1=15.938760054231023\n",
      "SubSGD iter. 308/499: loss=2.378529393756196, w0=74.90000000000015, w1=16.67072464773866\n",
      "SubSGD iter. 309/499: loss=0.35678100347647046, w0=75.60000000000015, w1=16.317608896171343\n",
      "SubSGD iter. 310/499: loss=6.515685996218487, w0=74.90000000000015, w1=16.740848320943858\n",
      "SubSGD iter. 311/499: loss=1.0621977212496816, w0=74.20000000000014, w1=17.525081048983182\n",
      "SubSGD iter. 312/499: loss=9.32471957993448, w0=73.50000000000014, w1=17.101539795929167\n",
      "SubSGD iter. 313/499: loss=6.230794323579026, w0=72.80000000000014, w1=17.251996472380295\n",
      "SubSGD iter. 314/499: loss=2.090995346477328, w0=73.50000000000014, w1=17.868383421217544\n",
      "SubSGD iter. 315/499: loss=5.66094380066037, w0=74.20000000000014, w1=16.684381220823653\n",
      "SubSGD iter. 316/499: loss=2.099959954338132, w0=74.90000000000015, w1=17.40254454485484\n",
      "SubSGD iter. 317/499: loss=5.30244803305046, w0=74.20000000000014, w1=17.892228078251428\n",
      "SubSGD iter. 318/499: loss=9.546865169693518, w0=73.50000000000014, w1=17.468686825197413\n",
      "SubSGD iter. 319/499: loss=2.8891031340000666, w0=74.20000000000014, w1=17.99286737664913\n",
      "SubSGD iter. 320/499: loss=2.055696909625617, w0=74.90000000000015, w1=17.19303664258642\n",
      "SubSGD iter. 321/499: loss=8.949405741475132, w0=74.20000000000014, w1=17.41029364676617\n",
      "SubSGD iter. 322/499: loss=2.4702137794634638, w0=73.50000000000014, w1=18.251712156492115\n",
      "SubSGD iter. 323/499: loss=1.909661084578076, w0=72.80000000000014, w1=18.022590464205358\n",
      "SubSGD iter. 324/499: loss=5.103746814040434, w0=73.50000000000014, w1=17.25176739813717\n",
      "SubSGD iter. 325/499: loss=9.652561478952023, w0=72.80000000000014, w1=16.991055176755303\n",
      "SubSGD iter. 326/499: loss=4.276705948510724, w0=73.50000000000014, w1=17.15650796868793\n",
      "SubSGD iter. 327/499: loss=8.547899063319718, w0=74.20000000000014, w1=17.400820692397065\n",
      "SubSGD iter. 328/499: loss=7.650842409816363, w0=74.90000000000015, w1=18.52595065252737\n",
      "SubSGD iter. 329/499: loss=9.741579592964868, w0=75.60000000000015, w1=18.870418899397837\n",
      "SubSGD iter. 330/499: loss=11.15791000603592, w0=76.30000000000015, w1=17.843486701805304\n",
      "SubSGD iter. 331/499: loss=4.21050029889534, w0=75.60000000000015, w1=17.499774080621915\n",
      "SubSGD iter. 332/499: loss=5.437694936633889, w0=74.90000000000015, w1=18.004791812446236\n",
      "SubSGD iter. 333/499: loss=4.536405163006123, w0=74.20000000000014, w1=17.27977162236543\n",
      "SubSGD iter. 334/499: loss=4.533935712258561, w0=73.50000000000014, w1=17.703011047137945\n",
      "SubSGD iter. 335/499: loss=1.848056756407928, w0=74.20000000000014, w1=16.759504563600824\n",
      "SubSGD iter. 336/499: loss=7.279146412708997, w0=74.90000000000015, w1=16.40956663212051\n",
      "SubSGD iter. 337/499: loss=1.4333433535002698, w0=74.20000000000014, w1=17.193799360159833\n",
      "SubSGD iter. 338/499: loss=5.102139092445782, w0=73.50000000000014, w1=16.601481398891654\n",
      "SubSGD iter. 339/499: loss=5.770380128146371, w0=74.20000000000014, w1=17.142438450862794\n",
      "SubSGD iter. 340/499: loss=0.06829316909389149, w0=74.90000000000015, w1=17.79449534830525\n",
      "SubSGD iter. 341/499: loss=6.00557948162519, w0=75.60000000000015, w1=18.23441979118943\n",
      "SubSGD iter. 342/499: loss=0.6437024028475378, w0=74.90000000000015, w1=17.221741449836664\n",
      "SubSGD iter. 343/499: loss=0.8411513416976248, w0=74.20000000000014, w1=17.322018549076873\n",
      "SubSGD iter. 344/499: loss=4.704288306821752, w0=74.90000000000015, w1=16.74900846606225\n",
      "SubSGD iter. 345/499: loss=4.616134060895021, w0=74.20000000000014, w1=17.361806637063662\n",
      "SubSGD iter. 346/499: loss=2.844808929818612, w0=73.50000000000014, w1=17.31948008046017\n",
      "SubSGD iter. 347/499: loss=7.650030903684225, w0=74.20000000000014, w1=17.07850016579352\n",
      "SubSGD iter. 348/499: loss=2.5436744936779263, w0=73.50000000000014, w1=16.46458575241594\n",
      "SubSGD iter. 349/499: loss=2.3013984012174404, w0=72.80000000000014, w1=16.904274157501124\n",
      "SubSGD iter. 350/499: loss=3.9205581278886683, w0=72.10000000000014, w1=16.56904610219787\n",
      "SubSGD iter. 351/499: loss=3.900984552550753, w0=71.40000000000013, w1=15.868112104763117\n",
      "SubSGD iter. 352/499: loss=7.845433506796645, w0=70.70000000000013, w1=16.139592286745696\n",
      "SubSGD iter. 353/499: loss=5.588316569923826, w0=70.00000000000013, w1=16.41952127918216\n",
      "SubSGD iter. 354/499: loss=11.309184916584229, w0=70.70000000000013, w1=16.069583347701844\n",
      "SubSGD iter. 355/499: loss=3.193798554947847, w0=71.40000000000013, w1=15.969306248461635\n",
      "SubSGD iter. 356/499: loss=5.0068820455758924, w0=72.10000000000014, w1=16.583967229592023\n",
      "SubSGD iter. 357/499: loss=1.0537006415093089, w0=71.40000000000013, w1=15.964833833647356\n",
      "SubSGD iter. 358/499: loss=7.15479170315173, w0=72.10000000000014, w1=16.51803974972347\n",
      "SubSGD iter. 359/499: loss=8.27705780216369, w0=72.80000000000014, w1=16.12206350650164\n",
      "SubSGD iter. 360/499: loss=6.7744377986917215, w0=73.50000000000014, w1=14.768106612468411\n",
      "SubSGD iter. 361/499: loss=2.562505462795329, w0=74.20000000000014, w1=16.133476065132022\n",
      "SubSGD iter. 362/499: loss=1.5690825779472206, w0=73.50000000000014, w1=16.754527638702616\n",
      "SubSGD iter. 363/499: loss=3.00738670672456, w0=74.20000000000014, w1=15.983704572634428\n",
      "SubSGD iter. 364/499: loss=6.490180806699016, w0=74.90000000000015, w1=15.74272465796778\n",
      "SubSGD iter. 365/499: loss=0.8526930447780288, w0=75.60000000000015, w1=14.765927179025862\n",
      "SubSGD iter. 366/499: loss=1.637392133040379, w0=74.90000000000015, w1=14.600120919458366\n",
      "SubSGD iter. 367/499: loss=2.2011681706559614, w0=74.20000000000014, w1=13.854135265870969\n",
      "SubSGD iter. 368/499: loss=3.1313799871737586, w0=74.90000000000015, w1=14.506192163313424\n",
      "SubSGD iter. 369/499: loss=1.156684878104457, w0=74.20000000000014, w1=15.109958233279936\n",
      "SubSGD iter. 370/499: loss=1.2772642348183751, w0=73.50000000000014, w1=14.581902417769287\n",
      "SubSGD iter. 371/499: loss=3.311189457973967, w0=72.80000000000014, w1=13.880968420334534\n",
      "SubSGD iter. 372/499: loss=0.22010516392710144, w0=72.10000000000014, w1=12.797093023643232\n",
      "SubSGD iter. 373/499: loss=0.12399862471129097, w0=71.40000000000013, w1=12.096159026208479\n",
      "SubSGD iter. 374/499: loss=1.6649027900105153, w0=72.10000000000014, w1=12.384656290765895\n",
      "SubSGD iter. 375/499: loss=2.4266501798158515, w0=72.80000000000014, w1=12.55046255033339\n",
      "SubSGD iter. 376/499: loss=2.8275077298175546, w0=73.50000000000014, w1=13.164376963710973\n",
      "SubSGD iter. 377/499: loss=0.5708458357110544, w0=74.20000000000014, w1=13.78351035965564\n",
      "SubSGD iter. 378/499: loss=7.271275843332383, w0=73.50000000000014, w1=14.13059434527563\n",
      "SubSGD iter. 379/499: loss=7.876671981770741, w0=74.20000000000014, w1=14.743586842459191\n",
      "SubSGD iter. 380/499: loss=1.626660192124092, w0=74.90000000000015, w1=14.71115783469183\n",
      "SubSGD iter. 381/499: loss=1.5566746543400711, w0=75.60000000000015, w1=16.251220173079016\n",
      "SubSGD iter. 382/499: loss=8.290442022960057, w0=74.90000000000015, w1=16.49524699778817\n",
      "SubSGD iter. 383/499: loss=112.18727848385458, w0=75.60000000000015, w1=13.72494730089918\n",
      "SubSGD iter. 384/499: loss=8.425414709311127, w0=74.90000000000015, w1=13.301406047845164\n",
      "SubSGD iter. 385/499: loss=2.465755478371449, w0=74.20000000000014, w1=12.670240707850311\n",
      "SubSGD iter. 386/499: loss=0.7827522506308924, w0=73.50000000000014, w1=12.441119015563553\n",
      "SubSGD iter. 387/499: loss=7.518680433373142, w0=72.80000000000014, w1=12.685145840272703\n",
      "SubSGD iter. 388/499: loss=12.010078356739207, w0=72.10000000000014, w1=13.365745017928786\n",
      "SubSGD iter. 389/499: loss=2.1431000561853537, w0=72.80000000000014, w1=12.011788123895558\n",
      "SubSGD iter. 390/499: loss=1.5152033137987644, w0=72.10000000000014, w1=12.782611189963745\n",
      "SubSGD iter. 391/499: loss=0.3052147443984694, w0=72.80000000000014, w1=12.407025351677175\n",
      "SubSGD iter. 392/499: loss=2.822988338342668, w0=73.50000000000014, w1=13.613041641447623\n",
      "SubSGD iter. 393/499: loss=5.371394146073129, w0=74.20000000000014, w1=14.255971340211419\n",
      "SubSGD iter. 394/499: loss=4.38868718457509, w0=73.50000000000014, w1=14.695659745296604\n",
      "SubSGD iter. 395/499: loss=5.489068788602239, w0=74.20000000000014, w1=14.168471106549378\n",
      "SubSGD iter. 396/499: loss=2.514749635937889, w0=74.90000000000015, w1=14.963640831666725\n",
      "SubSGD iter. 397/499: loss=5.407406937484112, w0=75.60000000000015, w1=14.276946196160589\n",
      "SubSGD iter. 398/499: loss=4.616226572955107, w0=74.90000000000015, w1=14.897997769731182\n",
      "SubSGD iter. 399/499: loss=3.7054746693353593, w0=75.60000000000015, w1=14.935285081349052\n",
      "SubSGD iter. 400/499: loss=5.132465630869575, w0=74.90000000000015, w1=16.025080104422273\n",
      "SubSGD iter. 401/499: loss=1.0846486046844888, w0=74.20000000000014, w1=14.90676988381079\n",
      "SubSGD iter. 402/499: loss=4.948061455100856, w0=74.90000000000015, w1=14.379581245063564\n",
      "SubSGD iter. 403/499: loss=0.8574798763263516, w0=74.20000000000014, w1=13.849880271001448\n",
      "SubSGD iter. 404/499: loss=3.857817381172339, w0=73.50000000000014, w1=13.514652215698193\n",
      "SubSGD iter. 405/499: loss=8.496972712313536, w0=74.20000000000014, w1=12.89551106028381\n",
      "SubSGD iter. 406/499: loss=6.851716003681986, w0=73.50000000000014, w1=11.937375969382003\n",
      "SubSGD iter. 407/499: loss=2.0196858302483633, w0=72.80000000000014, w1=13.291332863415231\n",
      "SubSGD iter. 408/499: loss=3.0531009120807155, w0=73.50000000000014, w1=12.70702032586934\n",
      "SubSGD iter. 409/499: loss=14.005060633372437, w0=74.20000000000014, w1=13.051488572739808\n",
      "SubSGD iter. 410/499: loss=4.389764271943264, w0=74.90000000000015, w1=13.667875521577058\n",
      "SubSGD iter. 411/499: loss=1.814716441455289, w0=74.20000000000014, w1=14.364460272817821\n",
      "SubSGD iter. 412/499: loss=2.548874911649385, w0=74.90000000000015, w1=13.78014773527193\n",
      "SubSGD iter. 413/499: loss=1.3737888056032403, w0=74.20000000000014, w1=13.161014339327263\n",
      "SubSGD iter. 414/499: loss=5.600071303996472, w0=73.50000000000014, w1=13.556451131270268\n",
      "SubSGD iter. 415/499: loss=1.914712365229505, w0=72.80000000000014, w1=13.514124574666777\n",
      "SubSGD iter. 416/499: loss=2.6367736627196052, w0=72.10000000000014, w1=13.159097097136952\n",
      "SubSGD iter. 417/499: loss=7.453385340421292, w0=71.40000000000013, w1=13.781883503957244\n",
      "SubSGD iter. 418/499: loss=11.84329009911201, w0=70.70000000000013, w1=13.901775317445576\n",
      "SubSGD iter. 419/499: loss=4.169933516435762, w0=71.40000000000013, w1=13.718309538395157\n",
      "SubSGD iter. 420/499: loss=0.5409577188663093, w0=72.10000000000014, w1=13.850633064748996\n",
      "SubSGD iter. 421/499: loss=0.24181570307345623, w0=72.80000000000014, w1=14.139130329306411\n",
      "SubSGD iter. 422/499: loss=0.13645564101129537, w0=73.50000000000014, w1=14.36825202159317\n",
      "SubSGD iter. 423/499: loss=2.212693313897887, w0=72.80000000000014, w1=14.103466912402402\n",
      "SubSGD iter. 424/499: loss=3.190477445276805, w0=73.50000000000014, w1=13.54935983840582\n",
      "SubSGD iter. 425/499: loss=3.605337779247897, w0=74.20000000000014, w1=12.51184564459228\n",
      "SubSGD iter. 426/499: loss=4.046234438525005, w0=74.90000000000015, w1=12.5382182952171\n",
      "SubSGD iter. 427/499: loss=3.0462704574309356, w0=75.60000000000015, w1=11.798885495773156\n",
      "SubSGD iter. 428/499: loss=1.893888222130819, w0=74.90000000000015, w1=12.652731766198425\n",
      "SubSGD iter. 429/499: loss=0.057897707590470304, w0=74.20000000000014, w1=13.206838840195006\n",
      "SubSGD iter. 430/499: loss=3.8914746723016265, w0=74.90000000000015, w1=12.77650725173427\n",
      "SubSGD iter. 431/499: loss=1.6076611430329564, w0=74.20000000000014, w1=13.129623003301583\n",
      "SubSGD iter. 432/499: loss=0.9708686030926827, w0=73.50000000000014, w1=13.14820246220046\n",
      "SubSGD iter. 433/499: loss=1.4512728829603248, w0=72.80000000000014, w1=13.015878935846622\n",
      "SubSGD iter. 434/499: loss=1.510728806987018, w0=72.10000000000014, w1=13.009451054968276\n",
      "SubSGD iter. 435/499: loss=2.4072858095092613, w0=72.80000000000014, w1=13.537506870478925\n",
      "SubSGD iter. 436/499: loss=4.8524277832130664, w0=72.10000000000014, w1=14.356165240284554\n",
      "SubSGD iter. 437/499: loss=2.225752761543056, w0=71.40000000000013, w1=14.79585364536974\n",
      "SubSGD iter. 438/499: loss=5.132095832688506, w0=72.10000000000014, w1=15.80829198323616\n",
      "SubSGD iter. 439/499: loss=12.196218974185044, w0=71.40000000000013, w1=15.928183796724491\n",
      "SubSGD iter. 440/499: loss=0.12420800343287652, w0=70.70000000000013, w1=16.278520730092882\n",
      "SubSGD iter. 441/499: loss=2.4164007526863145, w0=71.40000000000013, w1=17.643890182756493\n",
      "SubSGD iter. 442/499: loss=5.788767897166032, w0=70.70000000000013, w1=18.324489360412578\n",
      "SubSGD iter. 443/499: loss=3.76019078814096, w0=70.00000000000013, w1=18.8025375628387\n",
      "SubSGD iter. 444/499: loss=2.6671306026265853, w0=70.70000000000013, w1=18.362849157753516\n",
      "SubSGD iter. 445/499: loss=9.604546625715344, w0=71.40000000000013, w1=19.487979117883825\n",
      "SubSGD iter. 446/499: loss=12.768167794535131, w0=72.10000000000014, w1=19.83244736475429\n",
      "SubSGD iter. 447/499: loss=2.1984739952075216, w0=72.80000000000014, w1=19.098333041753303\n",
      "SubSGD iter. 448/499: loss=14.29227053613809, w0=73.50000000000014, w1=18.07140084416077\n",
      "SubSGD iter. 449/499: loss=8.457451067265723, w0=72.80000000000014, w1=16.708649448769023\n",
      "SubSGD iter. 450/499: loss=8.65372336458124, w0=73.50000000000014, w1=16.358711517288707\n",
      "SubSGD iter. 451/499: loss=0.7094218073442278, w0=72.80000000000014, w1=16.46888563416945\n",
      "SubSGD iter. 452/499: loss=3.993788858787049, w0=73.50000000000014, w1=15.284883433775558\n",
      "SubSGD iter. 453/499: loss=1.6219679908391385, w0=72.80000000000014, w1=15.905935007346152\n",
      "SubSGD iter. 454/499: loss=4.29860498326741, w0=73.50000000000014, w1=16.62409833137734\n",
      "SubSGD iter. 455/499: loss=2.5662331170616284, w0=72.80000000000014, w1=17.358212654378327\n",
      "SubSGD iter. 456/499: loss=4.75791056016871, w0=73.50000000000014, w1=16.679865658403784\n",
      "SubSGD iter. 457/499: loss=2.2473606277885665, w0=74.20000000000014, w1=17.30335378068996\n",
      "SubSGD iter. 458/499: loss=7.659317956679828, w0=74.90000000000015, w1=15.949396886656732\n",
      "SubSGD iter. 459/499: loss=1.2556597113519246, w0=75.60000000000015, w1=16.37691397187415\n",
      "SubSGD iter. 460/499: loss=0.4914321074183903, w0=74.90000000000015, w1=16.730029723441465\n",
      "SubSGD iter. 461/499: loss=3.0012342416190307, w0=75.60000000000015, w1=16.39366676576539\n",
      "SubSGD iter. 462/499: loss=4.667883088179622, w0=76.30000000000015, w1=15.866478127018162\n",
      "SubSGD iter. 463/499: loss=0.5016780877142395, w0=77.00000000000016, w1=16.509407825781956\n",
      "SubSGD iter. 464/499: loss=4.738454129582372, w0=76.30000000000015, w1=16.94041851053057\n",
      "SubSGD iter. 465/499: loss=6.839118020612496, w0=75.60000000000015, w1=17.363657935303085\n",
      "SubSGD iter. 466/499: loss=7.158749800629153, w0=76.30000000000015, w1=16.326143741489545\n",
      "SubSGD iter. 467/499: loss=4.294712563889604, w0=75.60000000000015, w1=15.798087925978896\n",
      "SubSGD iter. 468/499: loss=3.8410723044277546, w0=74.90000000000015, w1=16.924020339480066\n",
      "SubSGD iter. 469/499: loss=3.646616046127434, w0=74.20000000000014, w1=16.91759245860172\n",
      "SubSGD iter. 470/499: loss=120.75822340769061, w0=74.90000000000015, w1=13.544448781012044\n",
      "SubSGD iter. 471/499: loss=5.305637162563713, w0=74.20000000000014, w1=13.728609559330364\n",
      "SubSGD iter. 472/499: loss=0.5926212226713545, w0=74.90000000000015, w1=14.453629749411167\n",
      "SubSGD iter. 473/499: loss=0.9135135404417412, w0=74.20000000000014, w1=13.92392877534905\n",
      "SubSGD iter. 474/499: loss=5.442161819574466, w0=73.50000000000014, w1=14.4508858111179\n",
      "SubSGD iter. 475/499: loss=1.40907690538576, w0=72.80000000000014, w1=13.088134415726152\n",
      "SubSGD iter. 476/499: loss=7.072520315633042, w0=72.10000000000014, w1=13.47341855605428\n",
      "SubSGD iter. 477/499: loss=0.17914690273259737, w0=72.80000000000014, w1=14.104583896049133\n",
      "SubSGD iter. 478/499: loss=5.847737912692395, w0=73.50000000000014, w1=14.141871207667004\n",
      "SubSGD iter. 479/499: loss=10.614919496638684, w0=72.80000000000014, w1=14.413351389649582\n",
      "SubSGD iter. 480/499: loss=2.9356477895051114, w0=73.50000000000014, w1=14.683489839939252\n",
      "SubSGD iter. 481/499: loss=2.162724052537584, w0=72.80000000000014, w1=13.599614443247951\n",
      "SubSGD iter. 482/499: loss=4.549336029926053, w0=73.50000000000014, w1=14.245761676635173\n",
      "SubSGD iter. 483/499: loss=9.145656704971792, w0=72.80000000000014, w1=14.525690669071636\n",
      "SubSGD iter. 484/499: loss=3.696735015178433, w0=72.10000000000014, w1=15.344349038877265\n",
      "SubSGD iter. 485/499: loss=0.2523476893502874, w0=72.80000000000014, w1=16.185539207101908\n",
      "SubSGD iter. 486/499: loss=4.838607426798646, w0=73.50000000000014, w1=15.631432133105326\n",
      "SubSGD iter. 487/499: loss=0.8254984065877267, w0=72.80000000000014, w1=15.521132099644534\n",
      "SubSGD iter. 488/499: loss=8.296793333898549, w0=73.50000000000014, w1=14.78179930020059\n",
      "SubSGD iter. 489/499: loss=0.8597258555283247, w0=72.80000000000014, w1=14.162665904255922\n",
      "SubSGD iter. 490/499: loss=0.3650249228167439, w0=73.50000000000014, w1=14.908651557843319\n",
      "SubSGD iter. 491/499: loss=1.27167935610683, w0=74.20000000000014, w1=14.884685718040227\n",
      "SubSGD iter. 492/499: loss=2.628624841854105, w0=73.50000000000014, w1=15.786317982452985\n",
      "SubSGD iter. 493/499: loss=3.721325783564552, w0=74.20000000000014, w1=16.504481306484173\n",
      "SubSGD iter. 494/499: loss=5.400661165371709, w0=73.50000000000014, w1=17.056452738421854\n",
      "SubSGD iter. 495/499: loss=5.418780193055483, w0=74.20000000000014, w1=17.597409790392994\n",
      "SubSGD iter. 496/499: loss=6.406446636868949, w0=73.50000000000014, w1=16.692234687205932\n",
      "SubSGD iter. 497/499: loss=2.498515962425145, w0=72.80000000000014, w1=15.930877736586863\n",
      "SubSGD iter. 498/499: loss=9.645467911408524, w0=73.50000000000014, w1=14.90394553899433\n",
      "SubSGD iter. 499/499: loss=6.7254962655059245, w0=72.80000000000014, w1=15.46206924274337\n",
      "SubSGD: execution time=0.038 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601ef20afc564b3fac9612d255cc37ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
