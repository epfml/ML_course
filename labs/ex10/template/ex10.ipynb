{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EPFL ML – Lab 10: Adversarial Robustness",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoE7_FDHHkat"
      },
      "source": [
        "# Lab 10 – Adversarial Robustness\n",
        "\n",
        "Security and robustness of machine learning models have traditionally been often overlooked. It is usually quite easy for an adversary to create inputs (e.g. images) that look much like the originals to a human, but fool an ML model into thinking they are something else.\n",
        "\n",
        "![Illustration from Goodfellow et al. Adding imperceptible pixel-wise perturbations to an image can drastically alter a model's predictions.](https://miro.medium.com/max/573/1*Nj_toOwx_Hc5NLn97Jv-ww.png)\n",
        "\n",
        "In this lab, we will see that tiny perturbations to an image can cause drastically different model predictions. You will train two different image classifiers for hand-written digits: a logistic regression model and a neural net. For both, you will run the [Fast Gradient Sign Attack (FGSM)](https://arxiv.org/pdf/1412.6572.pdf), to try to fool them. Which one do you think will be more robust?\n",
        "\n",
        "Making machine learning models provably robust against any attack is still an open area of research, but in the second part of this lab, we will explore if we can make the models robust against the attacks conducted in the first part.\n",
        "\n",
        "A few notes:\n",
        "- If you're interested to read more, note that research on adversarial examples is not limited to the image domain, check out [this attack](https://arxiv.org/pdf/1801.01944.pdf) on speech-to-text models.\n",
        "- This lab is based on an official PyTorch tutorial on [Aversarial Example Generation](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WONGBZ44qRiU"
      },
      "source": [
        "## 1. Training a digit classifier\n",
        "-------------------------\n",
        "\n",
        "We will start by implementing a simple classifier for 28x28 black-and-white images of handwritten digits (the MNIST dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB8fpDkDHkav"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaPQjvcxEzy2"
      },
      "source": [
        "### Implementing the `accuracy` function\n",
        "\n",
        "We will evaluate the quality of a classifier by its accuracy on the test set.\n",
        "\n",
        "__Exercise__ As a warm-up exercise, please complete the accuracy-function below and make sure that the tests pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILOZIfcvwspy"
      },
      "source": [
        "def accuracy(predicted_logits, reference):\n",
        "    \"\"\"\n",
        "    Compute the ratio of correctly predicted labels\n",
        "    \n",
        "    @param predicted_logits: float32 tensor of shape (batch size, num classes)\n",
        "    @param reference: int64 tensor of shape (batch_size) with the class number\n",
        "    \"\"\"\n",
        "    \n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO\n",
        "    # ***************************************************\n",
        "    \n",
        "def test_accuracy():\n",
        "    predictions = torch.tensor([[0.5, 1.0, 0.7], [-0.6, -0.3, 0]])\n",
        "    correct_labels = torch.tensor([0, 2])  # first is wrong, second is correct\n",
        "    assert accuracy(predictions, correct_labels).allclose(torch.tensor([0.5]))\n",
        "\n",
        "\n",
        "    predictions = torch.tensor([[0.5, 1.0, 0.7], [-0.6, -0.3, 0], [-1, 0, 1]])\n",
        "    correct_labels = torch.tensor([1, 1, 2])  # correct, wrong, correct\n",
        "    assert accuracy(predictions, correct_labels).allclose(torch.tensor([2/3]))\n",
        "\n",
        "    print(\"Tests passed\")\n",
        "  \n",
        "test_accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM4xDA30GAri"
      },
      "source": [
        "### Logistic regression in PyTorch\n",
        "\n",
        "__Exercise__ Complete the PyTorch model below for logistic regression.\n",
        "Note that the model has 10 target classes, and therefore 10 outputs.\n",
        "Do not include the non-linear transformation like the softmax function. In PyTorch, this transformation is usually included in the loss function (`CrossEntropyLoss`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7uo0P7dtofd"
      },
      "source": [
        "class LogisticRegressionModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.image_area = 28 * 28 # pixels\n",
        "        self.num_classes = 10\n",
        "        \n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO\n",
        "        # ***************************************************\n",
        "        self.linear_transform = # Fill HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        flattened_images = x.view(batch_size, self.image_area)\n",
        "        return self.linear_transform(flattened_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijepok2GG7Q8"
      },
      "source": [
        "__Exercise__ Complete the training function below and train the logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T06qNribrErv"
      },
      "source": [
        "def train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs):\n",
        "  \"\"\"\n",
        "  @param model: torch.nn.Module\n",
        "  @param criterion: torch.nn.modules.loss._Loss\n",
        "  @param dataset_train: torch.utils.data.DataLoader\n",
        "  @param dataset_test: torch.utils.data.DataLoader\n",
        "  @param optimizer: torch.optim.Optimizer\n",
        "  @param num_epochs: int\n",
        "  \"\"\"\n",
        "  print(\"Starting training\")\n",
        "  for epoch in range(num_epochs):\n",
        "    # Train an epoch\n",
        "    model.train()\n",
        "    for batch_x, batch_y in dataset_train:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "      # Evaluate the network (forward pass)\n",
        "      # TODO: insert your code here\n",
        "      \n",
        "      # Compute the gradient\n",
        "      # TODO: insert your code here\n",
        "\n",
        "      # Update the parameters of the model with a gradient step\n",
        "      # TODO: insert your code here\n",
        "\n",
        "    # Test the quality on the test set\n",
        "    model.eval()\n",
        "    accuracies_test = []\n",
        "    for batch_x, batch_y in dataset_test:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "      # Evaluate the network (forward pass)\n",
        "      prediction = model(batch_x)\n",
        "      accuracies_test.append(accuracy(prediction, batch_y))\n",
        "\n",
        "    print(\"Epoch {} | Test accuracy: {:.5f}\".format(epoch, sum(accuracies_test).item()/len(accuracies_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycGiA_y2iafW"
      },
      "source": [
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "batch_size = 1000\n",
        "\n",
        "dataset_test = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('../data', train=False, download=True, transform=torchvision.transforms.ToTensor()), \n",
        "  batch_size=100,\n",
        "  shuffle=True\n",
        ")\n",
        "dataset_train = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('../data', train=True, download=True, transform=torchvision.transforms.ToTensor()),\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True\n",
        ")\n",
        "\n",
        "# If a GPU is available (should be on Colab, we will use it)\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"Things will go much quicker if you enable a GPU in Colab under 'Runtime / Change Runtime Type'\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the logistic regression model with the Adam optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss() # this includes LogSoftmax which executes a logistic transformation\n",
        "model_logreg = LogisticRegressionModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_logreg.parameters(), lr=learning_rate)\n",
        "train(model_logreg, criterion, dataset_train, dataset_test, optimizer, num_epochs)\n",
        "\n",
        "# You should expect a test accuracy of around 91%.\n",
        "# Training should take around a minute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehy22aHoKDzf"
      },
      "source": [
        "### A small convolutional network\n",
        "\n",
        "__Exercise__ Now, use the tools you built before to train this simple Convolutional Network. The following architecture can give reasonable results on MNIST:\n",
        "* first convolutional layer: 5x5 convolutions and 10 feature maps, \n",
        "* second convolutional layer: 5x5 convolutions and 20 feature maps, \n",
        "* dropout probability: 50%,\n",
        "* first fully-connected layer: 50 hidden units. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Moh7XMmtJc7n"
      },
      "source": [
        "class LeNetModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"This architecture is inspired by LeCun et al., 1998. (Gradient-Based Learning Applied to Document Recognition)\"\"\"\n",
        "    super().__init__()\n",
        "    \n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO\n",
        "    # ***************************************************\n",
        "    \n",
        "    self.conv1 =  # XXX\n",
        "    self.conv2 = # XXX\n",
        "    self.conv2_drop = # XXX\n",
        "    self.fc1 = # XXX\n",
        "    self.fc2 = # XXX\n",
        "\n",
        "  def forward(self, x):\n",
        "    relu = torch.nn.functional.relu\n",
        "    max_pool2d = torch.nn.functional.max_pool2d\n",
        "\n",
        "    x = relu(max_pool2d(self.conv1(x), 2))\n",
        "    x = relu(max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = relu(self.fc1(x))\n",
        "    x = torch.nn.functional.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return x  # Previously there was torch.nn.functional.log_softmax(x, dim=1) here which was incorrect (although the network could still train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjLWKuW_nshD"
      },
      "source": [
        "model_lenet = LeNetModel().to(device)\n",
        "optimizer = torch.optim.Adam(model_lenet.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model_lenet, criterion, dataset_train, dataset_test, optimizer, num_epochs)\n",
        "\n",
        "# Expect at least 97% accuracy on the test set.\n",
        "# Training should take around two minutes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5f-YxQVzyru"
      },
      "source": [
        "\n",
        "## 2. The Fast Gradient Sign Attack\n",
        "-------------------------\n",
        "\n",
        "One of the first and most popular adversarial attacks to date is\n",
        "referred to as the [Fast Gradient Sign Attack](https://arxiv.org/abs/1412.6572) and is described by Goodfellow et. al. in their paper 'Explaining and Harnessing Adversarial Examples'. The attack is remarkably simple, and yet often sufficient to generate successful adversarial examples for many standardly trained models. The main idea is that rather than working to __minimize__ the loss by adjusting the __model parameters__ based on the backpropagated gradients, the attack optimizes the __input data__ to __maximize__ the loss. In other words, the attack computes the gradient of the loss w.r.t the input data, then adjusts the input data to maximize this loss.\n",
        "\n",
        "![Illustration from Goodfellow et al. Adding imperceptible pixel-wise perturbations to an image can drastically alter a model's predictions.](https://miro.medium.com/max/573/1*Nj_toOwx_Hc5NLn97Jv-ww.png)\n",
        "\n",
        "In the figure above, $\\mathbf{x}$ is the original input image\n",
        "correctly classified as a “panda”, $y$ is the ground truth label\n",
        "for $\\mathbf{x}$, $\\mathbf{\\theta}$ represents the model\n",
        "parameters, and $J(\\mathbf{\\theta}, \\mathbf{x}, y)$ is the loss\n",
        "that is used to train the network. The attack backpropagates the\n",
        "gradient back to the input data to calculate\n",
        "$\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)$. Then, it adjusts\n",
        "the input data by a small step ($\\epsilon$ or $0.007$ in the\n",
        "picture) in the direction (i.e. $sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))$) that will\n",
        "maximize the loss with respect to the $L_\\infty$ norm. The resulting perturbed image, $x'$, is then *misclassified* by the target network as a “gibbon” when it is still\n",
        "clearly a “panda”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95T_alKBHka2"
      },
      "source": [
        "### Update rule\n",
        "\n",
        "__Exercise__ \n",
        "Let $\\mathbf{x}$ represent a network input (image) and let $J(\\theta, \\mathbf{x}, y)$ be the loss function of applying the network with parameters $\\theta$ to datapoint ($\\mathbf{x}$, $y$).\n",
        "Assuming that $J$ is **linear** in $\\mathbf{x}$ for some fixed weights $\\theta$, the perturbed image that achieves the highest loss while being not further than $\\epsilon$ away from $\\mathbf{x}$ in $L_\\infty$ norm is\n",
        "\n",
        "$$ \\hat{\\mathbf{x}} = \\mathbf{x} + \\epsilon \\text{ sign}(\\nabla_xJ(\\theta,\\mathbf{x},y).$$\n",
        "\n",
        "Implement the update below (for a batch of images all at once). Make sure that the pixel values stay in the image range $[0, 1]$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xC8tUdYHka2"
      },
      "source": [
        "def fgsm_update(image, data_grad, update_max_norm):\n",
        "    \"\"\"\n",
        "    Compute the FGSM update on an image (or a batch of images)\n",
        "\n",
        "    @param image: float32 tensor of shape (batch_size, rgb, height, width)\n",
        "    @param data_grad: float32 tensor of the same shape as `image`. Gradient of the loss with respect to `image`.\n",
        "    @param update_max_norm: float, the maximum permitted difference between `image` and the output of this function measured in L_inf norm.\n",
        "\n",
        "    @returns a perturbed version of `image` with the same shape\n",
        "    \"\"\"\n",
        "    \n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO\n",
        "    # ***************************************************\n",
        "    \n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    \n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "\n",
        "    # Return the perturbed image\n",
        "    \n",
        "    \n",
        "    return perturbed_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuwR9SpqHka4"
      },
      "source": [
        "### Executing the attack\n",
        "\n",
        "__Exercise__ Complete the function below and perturb every digit in the test set such that the original models give as many incorrect predictions as possible. Use the `fgsm_update` function you created before.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q91gBNcVHka5"
      },
      "source": [
        "def evaluate_attack(model, criterion, test_loader, update_max_norm):\n",
        "  \"\"\"\n",
        "  @param model: torch.nn.Module\n",
        "  @param criterion: torch.nn.modules.loss._Loss\n",
        "  @param test_loader: torch.util.data.DataLoader\n",
        "  @param update_max_norm: float indicating the maximum L_infinity norm allowed for the perturbations\n",
        "\n",
        "  @return (\n",
        "    accuracy of the model in the perturbed test set,\n",
        "    adversarial example images: list of 5 samples of a tuple (original prediction - float, prediction - float, example image - torch.tensor)\n",
        "  )\n",
        "  \"\"\"\n",
        "  accuracy_per_batch = []\n",
        "  adversarial_examples = []  # a small sample of 5 adversarial images\n",
        "\n",
        "  # Loop over all examples in test set in batches\n",
        "  for data, target in test_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Indicate that we want PyTorch to compute a gradient with respect to the\n",
        "    # input batch.\n",
        "    data.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    original_predictions = output.argmax(1) # get the index of the max logit\n",
        "    original_accuracy = accuracy(output, target)\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Zero all existing gradients\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Perturb the batch with a gradient step (using the `fgsm_update`)\n",
        "    perturbed_data = # TODO: insert your code here\n",
        "\n",
        "    # Re-classify the perturbed batch\n",
        "    output = model(perturbed_data)\n",
        "    adversarial_predictions = output.argmax(1)\n",
        "    adversarial_accuracy = accuracy(output, target)\n",
        "\n",
        "    accuracy_per_batch.append(adversarial_accuracy)\n",
        "\n",
        "    # Save some adversarial examples for visualization\n",
        "    if len(adversarial_examples) < 5:\n",
        "      adv_ex = perturbed_data[0, 0, :, :].detach().cpu().numpy()\n",
        "      adversarial_examples.append( (original_predictions[0].item(), adversarial_predictions[0].item(), adv_ex) )\n",
        "\n",
        "  average_accuracy = sum(accuracy_per_batch) / len(accuracy_per_batch)  # assuming all batches are the same size\n",
        "\n",
        "  print(\"Epsilon: {:.2f}\\tTest Accuracy = {:.3f}\".format(update_max_norm, average_accuracy))\n",
        "\n",
        "  return average_accuracy, adversarial_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvCW-Na-Hka7"
      },
      "source": [
        "### Accuracy vs perturbation magnitude\n",
        "\n",
        "Let's explore how the maximum allowed perturbation influences the model's test error.\n",
        "\n",
        "__Exercise__ At what maximum allowed perturbation ($\\epsilon$) does the model start to perform worse than random chance for 10-class classification with balanced classes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBYC7mgEHka7"
      },
      "source": [
        "accuracies_logreg = []\n",
        "examples_logreg = []\n",
        "\n",
        "epsilons_logreg = [0, .05, .1, .15, .2, .25, .3]\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons_logreg:\n",
        "    acc, ex = evaluate_attack(model_logreg, criterion, dataset_test, eps)\n",
        "    accuracies_logreg.append(acc)\n",
        "    examples_logreg.append(ex)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(epsilons_logreg, accuracies_logreg, \"*-\")\n",
        "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.xticks(np.arange(0, .35, step=0.05))\n",
        "plt.title(\"Accuracy vs Epsilon\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibgZ1snR43nr"
      },
      "source": [
        "### Image quality\n",
        "\n",
        "The code below explores the visual relationship between perturbation and image quality. \n",
        "\n",
        "__Exercise__ How many perturbed images would you get right yourself?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyewFQpc333n"
      },
      "source": [
        "# Plot several examples of adversarial samples at each epsilon\n",
        "counter = 0\n",
        "plt.figure(figsize=(8,10))\n",
        "for epsilon, examples in zip(epsilons_logreg, examples_logreg):\n",
        "    for column_number, example in enumerate(examples):\n",
        "        counter += 1\n",
        "        original_prediction, adversarial_prediction, img = example\n",
        "        img = img.squeeze()\n",
        "\n",
        "        plt.subplot(len(epsilons_logreg), len(examples), counter)\n",
        "\n",
        "        plt.title(\"{} -> {}\".format(original_prediction, adversarial_prediction))\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "\n",
        "        # Clear the axes\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "\n",
        "        # Print y-labels in the first column\n",
        "        if column_number == 0:\n",
        "            plt.ylabel(\"Eps: {:.2f}\".format(epsilon), fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG5RTfNr4KwA"
      },
      "source": [
        "### Logistic regression vs a ConvNet\n",
        "Now, let's run the same attack on the convolutional neural network you trained before. \n",
        "\n",
        "__Exercise__ Find out which model is more robust to this type of adversarial perturbations. Does the result match your expectations? Can you intuitively explain the result? Do the adversarial examples look similar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDnQJIdT5nCA"
      },
      "source": [
        "accuracies_lenet = []\n",
        "examples_lenet = []\n",
        "\n",
        "epsilons_lenet = [0, .05, .1, .15, .2, .25, .3]\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons_lenet:\n",
        "    acc, ex = evaluate_attack(model_lenet, criterion, dataset_test, eps)\n",
        "    accuracies_lenet.append(acc)\n",
        "    examples_lenet.append(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq2fdeeh6bao"
      },
      "source": [
        "# Comparing the models\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(epsilons_logreg, accuracies_logreg, \"*-\", c='red', label='Logistic regression')\n",
        "plt.plot(epsilons_lenet, accuracies_lenet, \"*-\", c='blue', label='Convolutional network')\n",
        "\n",
        "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.xticks(np.arange(0, .35, step=0.05))\n",
        "\n",
        "plt.title(\"Accuracy vs Epsilon\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAoJXiXrziCO"
      },
      "source": [
        "## 3. Training a neural network model robust to FGSM\n",
        "\n",
        "[Goodfellow et al.](https://arxiv.org/pdf/1412.6572.pdf) present an adversarial training algorithm using the Fast Gradient Sign Method to solve the inner maximization problem. The idea of FGSM training is simple: while training, adapt the training data to be a worst-case adversarial example by using the method developed before (details can be found in Section 5 of the [paper](https://arxiv.org/pdf/1412.6572.pdf)).\n",
        "\n",
        "Training models that are robust against *any* adversarial input inside of some fixed $L_\\infty$-ball can be sometimes tricky, and FGSM training is not always sufficient. Sometimes FGSM training leads to models which are robust only to FGSM, but not to other attacks. In case you are interested, see [this recent paper](https://arxiv.org/pdf/2007.02617.pdf) that discusses this issue in detail. More generally, accurate evaluation of adversarial robustness of neural networks is a complicated topic and many papers have been written on this (a good summary can be found [here](https://arxiv.org/abs/1902.06705)), and we won't go into more detail here, and we'll assume that FGSM attack is a good proxy for measuring the robustness of our model.\n",
        "\n",
        "\n",
        "__Exercise__: Complete the code for FGSM adversarial training below. Use your `fgsm_update` function using perturbations which have $L_\\infty$ distance of 0.25 to the original training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJge1R3n2vKw"
      },
      "source": [
        "robust_neural_net_model = LeNetModel().to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "optimizer = torch.optim.Adam(robust_neural_net_model.parameters(), lr=1e-3)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Train an epoch\n",
        "    robust_neural_net_model.train()\n",
        "    for batch_x, batch_y in dataset_train:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        # Forward pass for adversarial perturbations\n",
        "        batch_x.requires_grad = True\n",
        "        output = robust_neural_net_model(batch_x)\n",
        "        original_predictions = output.argmax(1) # get the index of the max logit\n",
        "        original_accuracy = accuracy(output, batch_y)\n",
        "        loss = criterion(output, batch_y)\n",
        "        robust_neural_net_model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Generate adversarial examples for training using fgsm_update()\n",
        "        perturbed_data = # TODO: insert your code here\n",
        "        \n",
        "        # Evaluate the network on perturbed data (forward pass)\n",
        "        prediction = robust_neural_net_model(perturbed_data)\n",
        "        loss = criterion(prediction, batch_y)\n",
        "        \n",
        "        # Compute the gradient with respect to the parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters of the model with a gradient step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test the quality on the test set\n",
        "    robust_neural_net_model.eval()\n",
        "    accuracies = []\n",
        "    for batch_x, batch_y in dataset_test:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        # Evaluate the network (forward pass)\n",
        "        prediction = robust_neural_net_model(batch_x)\n",
        "        accuracies.append(accuracy(prediction, batch_y))\n",
        "      \n",
        "    print(\"Epoch {:.2f} | Test accuracy: {:.5f}\".format(epoch, sum(accuracies).item()/len(accuracies)))\n",
        "\n",
        "\n",
        "  # training takes around two minutes\n",
        "  # you should expect an accuracy around 96%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C119HVTIzScI"
      },
      "source": [
        "accuracies_lenet_robust = []\n",
        "examples_lenet_robust = []\n",
        "\n",
        "epsilons_lenet_robust = [0, .05, .1, .15, .2, .25, .3]\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons_lenet_robust:\n",
        "    acc, ex = evaluate_attack(robust_neural_net_model, criterion, dataset_test, eps)\n",
        "    accuracies_lenet_robust.append(acc)\n",
        "    examples_lenet_robust.append(ex)\n",
        "\n",
        "# Comparing the models\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(epsilons_logreg, accuracies_logreg, \"*-\", c='red', label='Logistic regression')\n",
        "plt.plot(epsilons_lenet, accuracies_lenet, \"*-\", c='blue', label='Convolutional network')\n",
        "plt.plot(epsilons_lenet_robust, accuracies_lenet_robust, \"*-\", c='orange', label='Convolutional network (robust)')\n",
        "\n",
        "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.xticks(np.arange(0, .35, step=0.05))\n",
        "\n",
        "plt.title(\"Accuracy vs Epsilon\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYy5aeo37Hw8"
      },
      "source": [
        "__Discussion__ Does adversarial training help to improve robustness against FGSM perturbations? Is there a trade-off between robustness and standard accuracy? Can you tune it to perform even better than with our default supplied parameters? How do you think the same defense would perform on the linear classification model (see Problem 2 in the current exercise sheet)?"
      ]
    }
  ]
}